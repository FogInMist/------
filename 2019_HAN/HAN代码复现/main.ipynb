{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAN模型简单代码复现\n",
    "\n",
    "*参考资料：*\n",
    "* [源代码链接](https://github.com/taishan1994/pytorch_HAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from model import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2125 300 600\n",
      "y_train:(3025, 3), y_val:(3025, 3), y_test:(3025, 3), train_idx:(1, 2125), val_idx:(1, 300), test_idx:(1, 600)\n"
     ]
    }
   ],
   "source": [
    "adj_list, fea_list, y_train, y_val, y_test, train_mask, val_mask, test_mask, my_data = load_data_dblp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3025, 3025)\n",
      "(3025, 3025)\n"
     ]
    }
   ],
   "source": [
    "# rownetworks: ['PAP','PLP']\n",
    "for i in adj_list:\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "节点数目,特征维度： (3025, 1870)\n",
      "(3025, 3)\n",
      "[0. 0. 1.]\n",
      "(3025,)\n"
     ]
    }
   ],
   "source": [
    "print('节点数目,特征维度：',fea_list[0].shape)\n",
    "print(y_train.shape)\n",
    "print(y_train[3000])\n",
    "print(train_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3025 1870 3\n"
     ]
    }
   ],
   "source": [
    "nb_nodes = fea_list[0].shape[0] #节点数目 3025\n",
    "ft_size = fea_list[0].shape[1] #特征的维度 1870\n",
    "nb_classes = y_train.shape[1]  #标签的维度 3\n",
    "print(nb_nodes,ft_size,nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_list = [torch.transpose(torch.from_numpy(fea[np.newaxis]),2,1).to(device) for fea in fea_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1870, 3025])\n"
     ]
    }
   ],
   "source": [
    "print(fea_list[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_list = [adj[np.newaxis] for adj in adj_list]\n",
    "y_train = y_train[np.newaxis]\n",
    "y_val = y_val[np.newaxis]\n",
    "y_test = y_test[np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3025, 3025)\n",
      "(1, 3025, 3)\n"
     ]
    }
   ],
   "source": [
    "print(adj_list[0].shape)\n",
    "print(y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_labels = my_data['my_labels']\n",
    "train_my_labels = my_data['train_my_labels']\n",
    "val_my_labels = my_data['val_my_labels']\n",
    "test_my_labels = my_data['test_my_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3025,)\n",
      "(2125,)\n",
      "(300,)\n",
      "(600,)\n"
     ]
    }
   ],
   "source": [
    "print(my_labels.shape)\n",
    "print(train_my_labels.shape)\n",
    "print(val_my_labels.shape)\n",
    "print(test_my_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "biases_list = [torch.transpose(torch.from_numpy(adj_to_bias(adj, [nb_nodes], nhood=1)),2,1).to(device) for adj in adj_list]\n",
    "print(len(biases_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0000e+00, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "          -1.0000e+09, -1.0000e+09],\n",
       "         [-1.0000e+09, -0.0000e+00, -1.0000e+09,  ..., -1.0000e+09,\n",
       "          -1.0000e+09, -1.0000e+09],\n",
       "         [-1.0000e+09, -1.0000e+09, -0.0000e+00,  ..., -1.0000e+09,\n",
       "          -1.0000e+09, -1.0000e+09],\n",
       "         ...,\n",
       "         [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -0.0000e+00,\n",
       "          -1.0000e+09, -1.0000e+09],\n",
       "         [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "          -0.0000e+00, -1.0000e+09],\n",
       "         [-1.0000e+09, -1.0000e+09, -1.0000e+09,  ..., -1.0000e+09,\n",
       "          -1.0000e+09, -0.0000e+00]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3025, 3025])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases_list[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5, 5)\n",
      "[[[0. 1. 0. 0. 1.]\n",
      "  [1. 0. 1. 1. 0.]\n",
      "  [0. 1. 0. 0. 0.]\n",
      "  [0. 1. 0. 0. 1.]\n",
      "  [1. 0. 0. 1. 0.]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[-0.e+00, -0.e+00, -0.e+00, -0.e+00, -0.e+00],\n",
       "        [-0.e+00, -0.e+00, -0.e+00, -0.e+00, -0.e+00],\n",
       "        [-0.e+00, -0.e+00, -0.e+00, -0.e+00, -1.e+09],\n",
       "        [-0.e+00, -0.e+00, -0.e+00, -0.e+00, -0.e+00],\n",
       "        [-0.e+00, -0.e+00, -1.e+09, -0.e+00, -0.e+00]]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h1 = np.array([[0.0,1.0,0.0,0,1],[1.0,0.0,1.0,1,0],[0.0,1.0,0.0,0,0],[0,1,0,0,1],[1,0,0,1,0]])\n",
    "h1 = h1[np.newaxis]\n",
    "print(h1.shape)\n",
    "print(h1)\n",
    "adj_to_bias(h1, [5], nhood=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: pre_trained/acm/acm_allMP_multi_fea_.ckpt\n"
     ]
    }
   ],
   "source": [
    "dataset = 'acm'\n",
    "featype = 'fea'\n",
    "checkpt_file = 'pre_trained/{}/{}_allMP_multi_{}_.ckpt'.format(dataset, dataset, featype)\n",
    "print('model: {}'.format(checkpt_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "batch_size = 1 # 批大小\n",
    "nb_epochs = 200 # 迭代次数\n",
    "patience = 100  # \n",
    "lr = 0.005  # learning rate学习率\n",
    "l2_coef = 0.0005  # weight decay\n",
    "\n",
    "# numbers of hidden units per each attention head in each layer\n",
    "hid_units = [8] # 隐藏单元\n",
    "n_heads = [8, 1]  # additional entry for the output layer 注意力数\n",
    "residual = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 3025])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.randn(1,1870,3025)\n",
    "ret = Attn_head(1870, 8, biases_list[0], activation=nn.ELU())\n",
    "result = ret(inputs)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fea_list[0].shape: torch.Size([1, 1870, 3025])\n",
      "biases_list[0].shape: torch.Size([1, 3025, 3025])\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(\"fea_list[0].shape:\",fea_list[0].shape)\n",
    "print(\"biases_list[0].shape:\",biases_list[0].shape)\n",
    "print(len(fea_list))\n",
    "print(len(biases_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1870, 3025]) torch.Size([1, 3025, 3025])\n",
      "torch.Size([1, 1870, 3025]) torch.Size([1, 3025, 3025])\n"
     ]
    }
   ],
   "source": [
    "for inputs,biases in zip(fea_list,biases_list):\n",
    "    print(inputs.shape, biases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteGAT_multi(\n",
       "  (activation): ELU(alpha=1.0)\n",
       "  (layers): Sequential(\n",
       "    (0): Attn_head(\n",
       "      (conv1): Conv1d(1870, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (conv2_1): Conv1d(8, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (conv2_2): Conv1d(8, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (softmax): Softmax(dim=1)\n",
       "      (in_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (coef_dropout): Dropout(p=0.5, inplace=False)\n",
       "      (activation): ELU(alpha=1.0)\n",
       "    )\n",
       "    (1): Attn_head(\n",
       "      (conv1): Conv1d(1870, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (conv2_1): Conv1d(8, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (conv2_2): Conv1d(8, 1, kernel_size=(1,), stride=(1,), bias=False)\n",
       "      (leakyrelu): LeakyReLU(negative_slope=0.01)\n",
       "      (softmax): Softmax(dim=1)\n",
       "      (in_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (coef_dropout): Dropout(p=0.5, inplace=False)\n",
       "      (activation): ELU(alpha=1.0)\n",
       "    )\n",
       "  )\n",
       "  (simpleAttLayer): SimpleAttLayer(\n",
       "    (tanh): Tanh()\n",
       "    (softmax): Softmax(dim=1)\n",
       "  )\n",
       "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = HeteGAT_multi(inputs_list=fea_list,nb_classes=nb_classes,nb_nodes=nb_nodes,attn_drop=0.5,\n",
    "                      ffd_drop=0.0,bias_mat_list=biases_list,hid_units=hid_units,n_heads=n_heads,\n",
    "                      activation=nn.ELU(),residual=False)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3025, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result211 = model.forward(fea_list)\n",
    "result211.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(params=model.parameters(),lr=lr,betas=(0.9,0.99),weight_decay=0.0)\n",
    "\n",
    "train_my_labels = torch.from_numpy(train_my_labels).long().to(device)\n",
    "val_my_labels = torch.from_numpy(val_my_labels).long().to(device)\n",
    "test_my_labels = torch.from_numpy(test_my_labels).long().to(device)\n",
    "\n",
    "train_mask = np.where(train_mask == 1)[0]\n",
    "val_mask = np.where(val_mask == 1)[0]\n",
    "test_mask = np.where(test_mask == 1)[0]\n",
    "train_mask = torch.from_numpy(train_mask).to(device)\n",
    "val_mask = torch.from_numpy(val_mask).to(device)\n",
    "test_mask = torch.from_numpy(test_mask).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    outputs = model(fea_list)\n",
    "    train_mask_outputs = torch.index_select(outputs,0,train_mask)\n",
    "    _, preds = torch.max(train_mask_outputs.data,1)\n",
    "    #print(preds)\n",
    "    #print(train_my_labels)\n",
    "    loss = criterion(train_mask_outputs,train_my_labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    correct += torch.sum(preds == train_my_labels).to(torch.float32)\n",
    "    acc = correct / len(train_my_labels)\n",
    "    #val_loss,val_acc = test(\"val\",val_mask,val_my_labels,epoch)\n",
    "    #test_acc = test(\"test\",test_mask,test_my_labels,epoch)\n",
    "    #test_acc_history.append(test_acc)\n",
    "    #print(\"epoch:{:03d}, loss:{:.4f}, TrainAcc:{:.4F}, ValLoss:{:.4f}, ValAcc:{:.4f}\".format(epoch,loss,acc,val_loss,val_acc))\n",
    "    return loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(mode,mask,label):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0.0\n",
    "        outputs = model(fea_list)\n",
    "        mask_outputs = torch.index_select(outputs,0,mask)\n",
    "        _, preds = torch.max(mask_outputs,1)\n",
    "        loss = criterion(mask_outputs,label)\n",
    "        correct += torch.sum(preds == label).to(torch.float32)\n",
    "        if mode == \"val\":\n",
    "            acc = correct / len(label)\n",
    "        elif mode == \"test\":\n",
    "            acc = correct / len(label)\n",
    "        else:\n",
    "            print(\"请输入合法的mode: val/test\")\n",
    "            return\n",
    "        #print(\"[{}]>>>>>  [epoch]:{:03d}, [loss]:{:.4f}, [acc]:{:.4F}\".format(mode,epoch,loss,acc))\n",
    "    return loss,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "    print(\"训练节点个数：\",len(train_my_labels))\n",
    "    print(\"验证节点个数：\",len(val_my_labels))\n",
    "    print(\"测试节点个数：\",len(test_my_labels))\n",
    "    for epoch in range(1,20):\n",
    "        train_loss,train_acc = train()\n",
    "        val_loss,val_acc = test(\"val\",val_mask,val_my_labels)\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc)\n",
    "        print(\"epoch:{:03d}, loss:{:.4f}, TrainAcc:{:.4F}, ValLoss:{:.4f}, ValAcc:{:.4f}\".format(epoch,train_loss,train_acc,val_loss,val_acc))\n",
    "    test_loss,test_acc = test(\"test\",test_mask,test_my_labels)\n",
    "    print(\"TestAcc:{:.4f}\".format(test_acc))\n",
    "    \n",
    "    return train_loss_history, train_acc_history, val_loss_history, val_acc_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练节点个数： 2125\n",
      "验证节点个数： 300\n",
      "测试节点个数： 600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:001, loss:1.0963, TrainAcc:0.3586, ValLoss:1.1904, ValAcc:0.2167\n",
      "epoch:002, loss:1.1018, TrainAcc:0.3642, ValLoss:1.1412, ValAcc:0.2367\n",
      "epoch:003, loss:1.0792, TrainAcc:0.4475, ValLoss:1.1102, ValAcc:0.3367\n",
      "epoch:004, loss:1.0702, TrainAcc:0.4080, ValLoss:1.0841, ValAcc:0.3700\n",
      "epoch:005, loss:1.0505, TrainAcc:0.5327, ValLoss:1.0961, ValAcc:0.3567\n",
      "epoch:006, loss:1.0220, TrainAcc:0.6136, ValLoss:1.1178, ValAcc:0.3167\n",
      "epoch:007, loss:1.0022, TrainAcc:0.6160, ValLoss:1.1279, ValAcc:0.3133\n",
      "epoch:008, loss:0.9787, TrainAcc:0.6452, ValLoss:1.1242, ValAcc:0.3200\n",
      "epoch:009, loss:0.9567, TrainAcc:0.6725, ValLoss:1.1078, ValAcc:0.3500\n",
      "epoch:010, loss:0.9305, TrainAcc:0.7096, ValLoss:1.0825, ValAcc:0.4133\n",
      "epoch:011, loss:0.9039, TrainAcc:0.7384, ValLoss:1.0550, ValAcc:0.4633\n",
      "epoch:012, loss:0.8676, TrainAcc:0.7760, ValLoss:1.0294, ValAcc:0.5267\n",
      "epoch:013, loss:0.8364, TrainAcc:0.7962, ValLoss:1.0070, ValAcc:0.5500\n",
      "epoch:014, loss:0.7989, TrainAcc:0.8113, ValLoss:0.9844, ValAcc:0.5667\n",
      "epoch:015, loss:0.7717, TrainAcc:0.8264, ValLoss:0.9622, ValAcc:0.6033\n",
      "epoch:016, loss:0.7309, TrainAcc:0.8296, ValLoss:0.9399, ValAcc:0.6233\n",
      "epoch:017, loss:0.6890, TrainAcc:0.8452, ValLoss:0.9158, ValAcc:0.6433\n",
      "epoch:018, loss:0.6448, TrainAcc:0.8612, ValLoss:0.8912, ValAcc:0.6733\n",
      "epoch:019, loss:0.6201, TrainAcc:0.8696, ValLoss:0.8660, ValAcc:0.6800\n",
      "TestAcc:0.3533\n"
     ]
    }
   ],
   "source": [
    "train_loss_history, train_acc_history, val_loss_history, val_acc_history = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
