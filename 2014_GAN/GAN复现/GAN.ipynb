{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 创建文件夹\n",
    "os.makedirs(\"./images/gan/\", exist_ok=True)         ## 记录训练过程的图片效果\n",
    "os.makedirs(\"./save/gan/\", exist_ok=True)           ## 训练完成时模型保存的位置\n",
    "os.makedirs(\"./datasets/mnist\", exist_ok=True)      ## 下载数据集存放的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(n_epochs=50, batch_size=64, lr=0.0002, b1=0.5, b2=0.999, n_cpu=2, latent_dim=100, img_size=28, channels=1, sample_interval=500)\n",
      "n_epochs 50\n"
     ]
    }
   ],
   "source": [
    "## 超参数配置\n",
    "parser = argparse.ArgumentParser() # 创建参数解析器的实例\n",
    "\n",
    "# 添加一个选项参数\n",
    "parser.add_argument(\"--n_epochs\", type=int, default=50, help=\"number of epochs of training\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"size of the batches\")\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--n_cpu\", type=int, default=2, help=\"number of cpu threads to use during batch generation\")\n",
    "parser.add_argument(\"--latent_dim\", type=int, default=100, help=\"dimensionality of the latent space\")\n",
    "parser.add_argument(\"--img_size\", type=int, default=28, help=\"size of each image dimension\")\n",
    "parser.add_argument(\"--channels\", type=int, default=1, help=\"number of image channels\")\n",
    "parser.add_argument(\"--sample_interval\", type=int, default=500, help=\"interval betwen image samples\")\n",
    "\n",
    "# 访问参数值\n",
    "# opt = parser.parse_args() # 解析命令行中的参数\n",
    "opt = parser.parse_args(args=[])                 ## 在jupyter notebook中运行时，换为此行\n",
    "print(opt)\n",
    "print('n_epochs:',opt.n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_shape: (1, 28, 28)\n",
      "img_area: 784\n",
      "cuda: False\n"
     ]
    }
   ],
   "source": [
    "## 图像的尺寸:(1， 28， 28),  和图像的像素面积:(784)\n",
    "img_shape = (opt.channels, opt.img_size, opt.img_size)\n",
    "img_area = np.prod(img_shape)\n",
    "\n",
    "print('img_shape:',img_shape,)\n",
    "print('img_area:', img_area)\n",
    "## 设置cuda:(cuda:0)\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "print('cuda:',cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./datasets/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/MNIST\\raw\\train-images-idx3-ubyte.gz to ./datasets/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./datasets/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./datasets/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./datasets/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./datasets/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./datasets/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./datasets/MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## mnist数据集下载\n",
    "mnist = datasets.MNIST(\n",
    "    root='./datasets/', train=True, download=True, transform=transforms.Compose(\n",
    "            [transforms.Resize(opt.img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "        ), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 配置数据到加载器\n",
    "dataloader = DataLoader(\n",
    "    mnist,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "    print(i, data[0].shape, data[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ##### 定义判别器 Discriminator ######\n",
    "## 将图片28x28展开成784，然后通过多层感知器，中间经过斜率设置为0.2的LeakyReLU激活函数，\n",
    "## 最后接sigmoid激活函数得到一个0到1之间的概率进行二分类\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(img_area, 512),                   ## 输入特征数为784，输出为64x512\n",
    "            nn.LeakyReLU(0.2, inplace=True),            ## 进行非线性映射\n",
    "            nn.Linear(512, 256),                        ## 输入特征数为512，输出为64x256\n",
    "            nn.LeakyReLU(0.2, inplace=True),            ## 进行非线性映射\n",
    "            nn.Linear(256, 64),                         ## 输入特征数为256，输出为64x64\n",
    "            nn.LeakyReLU(0.2, inplace=True),            ## 进行非线性映射\n",
    "            nn.Linear(64, 1),                           ## 输入特征数为64，输出为64x1\n",
    "            nn.Sigmoid(),                               ## sigmoid是一个激活函数，二分类问题中可将实数映射到[0, 1],作为概率值, 多分类用softmax函数64x1\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.view(img.size(0), -1)            ## 鉴别器输入是一个被view展开的(784)的一维图像:(64, 784)\n",
    "        validity = self.model(img_flat)                 ## 通过鉴别器网络; 64x784 ——> 64x1\n",
    "        return validity                                 ## 鉴别器返回的是一个[0, 1]间的概率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ###### 定义生成器 Generator #####\n",
    "## 输入一个100维的0～1之间的高斯分布，然后通过第一层线性变换将其映射到256维,\n",
    "## 然后通过LeakyReLU激活函数，接着进行一个线性变换，再经过一个LeakyReLU激活函数，\n",
    "## 然后经过线性变换将其变成784维，最后经过Tanh激活函数是希望生成的假的图片数据分布, 能够在-1～1之间。\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        ## 模型中间块儿\n",
    "        def block(in_feat, out_feat, normalize=True):           ## block(in， out)\n",
    "            layers = [nn.Linear(in_feat, out_feat)]             ## 线性变换将输入映射到out维\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))    ## 正则化\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))      ## 非线性激活函数\n",
    "            return layers\n",
    "        \n",
    "        ## prod():返回给定轴上的数组元素的乘积:1*28*28=784\n",
    "        self.model = nn.Sequential(\n",
    "            *block(opt.latent_dim, 128, normalize=False),       ## 线性变化将输入映射 100 to 128, 正则化, LeakyReLU\n",
    "            *block(128, 256),                                   ## 线性变化将输入映射 128 to 256, 正则化, LeakyReLU\n",
    "            *block(256, 512),                                   ## 线性变化将输入映射 256 to 512, 正则化, LeakyReLU\n",
    "            *block(512, 1024),                                  ## 线性变化将输入映射 512 to 1024, 正则化, LeakyReLU\n",
    "            nn.Linear(1024, img_area),                          ## 线性变化将输入映射 1024 to 784\n",
    "            nn.Tanh()                                           ## 将(784)的数据每一个都映射到[-1, 1]之间\n",
    "        )\n",
    "\n",
    "    ## view():相当于numpy中的reshape，重新定义矩阵的形状:这里是reshape(64, 1, 28, 28)\n",
    "    def forward(self, z):                                       ## 输入的是(64， 100)的噪声数据\n",
    "        imgs = self.model(z)                                     ## 噪声数据通过生成器模型\n",
    "        imgs = imgs.view(imgs.size(0), *img_shape)                 ## reshape成(64, 1, 28, 28)\n",
    "        return imgs                                              ## 输出为64张大小为(1, 28, 28)的图像\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 创建生成器，判别器对象\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "## 首先需要定义loss的度量方式  （二分类的交叉熵）\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "## 其次定义 优化函数,优化函数的学习率为0.0003\n",
    "## betas:用于计算梯度以及梯度平方的运行平均值的系数\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "## 如果有显卡，都在cuda模式中运行\n",
    "if torch.cuda.is_available():\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50] [Batch 99/938] [D loss: 1.350256] [G loss: 0.657420] [D real: 0.455796] [D fake: 0.413444]\n",
      "[Epoch 0/50] [Batch 199/938] [D loss: 1.265804] [G loss: 0.817958] [D real: 0.637671] [D fake: 0.550980]\n",
      "[Epoch 0/50] [Batch 299/938] [D loss: 1.059999] [G loss: 1.347359] [D real: 0.691148] [D fake: 0.478960]\n",
      "[Epoch 0/50] [Batch 399/938] [D loss: 1.124800] [G loss: 1.330101] [D real: 0.641281] [D fake: 0.482164]\n",
      "[Epoch 0/50] [Batch 499/938] [D loss: 0.902301] [G loss: 0.911080] [D real: 0.612378] [D fake: 0.309790]\n",
      "[Epoch 0/50] [Batch 599/938] [D loss: 0.927903] [G loss: 1.037134] [D real: 0.593640] [D fake: 0.300587]\n",
      "[Epoch 0/50] [Batch 699/938] [D loss: 0.927989] [G loss: 2.056396] [D real: 0.740449] [D fake: 0.440471]\n",
      "[Epoch 0/50] [Batch 799/938] [D loss: 0.880294] [G loss: 1.861353] [D real: 0.735229] [D fake: 0.387673]\n",
      "[Epoch 0/50] [Batch 899/938] [D loss: 0.914018] [G loss: 2.026015] [D real: 0.691760] [D fake: 0.352555]\n",
      "[Epoch 1/50] [Batch 99/938] [D loss: 0.996430] [G loss: 3.412513] [D real: 0.884839] [D fake: 0.574168]\n",
      "[Epoch 1/50] [Batch 199/938] [D loss: 1.058793] [G loss: 3.404269] [D real: 0.897013] [D fake: 0.575819]\n",
      "[Epoch 1/50] [Batch 299/938] [D loss: 0.768487] [G loss: 1.308537] [D real: 0.637801] [D fake: 0.187953]\n",
      "[Epoch 1/50] [Batch 399/938] [D loss: 0.659335] [G loss: 2.236079] [D real: 0.784239] [D fake: 0.300836]\n",
      "[Epoch 1/50] [Batch 499/938] [D loss: 0.855679] [G loss: 2.091174] [D real: 0.682069] [D fake: 0.290656]\n",
      "[Epoch 1/50] [Batch 599/938] [D loss: 0.709000] [G loss: 1.905862] [D real: 0.735691] [D fake: 0.295651]\n",
      "[Epoch 1/50] [Batch 699/938] [D loss: 1.259703] [G loss: 4.108972] [D real: 0.880696] [D fake: 0.636119]\n",
      "[Epoch 1/50] [Batch 799/938] [D loss: 1.025888] [G loss: 3.258736] [D real: 0.902504] [D fake: 0.586488]\n",
      "[Epoch 1/50] [Batch 899/938] [D loss: 0.597560] [G loss: 2.500994] [D real: 0.840006] [D fake: 0.299397]\n",
      "[Epoch 2/50] [Batch 99/938] [D loss: 0.951796] [G loss: 1.892924] [D real: 0.525618] [D fake: 0.055818]\n",
      "[Epoch 2/50] [Batch 199/938] [D loss: 1.056145] [G loss: 1.316379] [D real: 0.511669] [D fake: 0.075709]\n",
      "[Epoch 2/50] [Batch 299/938] [D loss: 0.717229] [G loss: 2.076296] [D real: 0.832706] [D fake: 0.379054]\n",
      "[Epoch 2/50] [Batch 399/938] [D loss: 0.438275] [G loss: 2.628175] [D real: 0.848136] [D fake: 0.218125]\n",
      "[Epoch 2/50] [Batch 499/938] [D loss: 0.376841] [G loss: 3.184366] [D real: 0.816608] [D fake: 0.058674]\n",
      "[Epoch 2/50] [Batch 599/938] [D loss: 0.704863] [G loss: 2.673269] [D real: 0.848792] [D fake: 0.379281]\n",
      "[Epoch 2/50] [Batch 699/938] [D loss: 0.873834] [G loss: 1.600286] [D real: 0.640909] [D fake: 0.137374]\n",
      "[Epoch 2/50] [Batch 799/938] [D loss: 0.511428] [G loss: 2.208465] [D real: 0.781224] [D fake: 0.195380]\n",
      "[Epoch 2/50] [Batch 899/938] [D loss: 0.543728] [G loss: 1.880843] [D real: 0.705490] [D fake: 0.090738]\n",
      "[Epoch 3/50] [Batch 99/938] [D loss: 0.687822] [G loss: 1.666718] [D real: 0.678256] [D fake: 0.134285]\n",
      "[Epoch 3/50] [Batch 199/938] [D loss: 0.456012] [G loss: 2.114002] [D real: 0.807401] [D fake: 0.147278]\n",
      "[Epoch 3/50] [Batch 299/938] [D loss: 0.611953] [G loss: 1.629672] [D real: 0.829698] [D fake: 0.264825]\n",
      "[Epoch 3/50] [Batch 399/938] [D loss: 0.470030] [G loss: 1.832421] [D real: 0.779267] [D fake: 0.124019]\n",
      "[Epoch 3/50] [Batch 499/938] [D loss: 0.644942] [G loss: 2.629407] [D real: 0.803860] [D fake: 0.264420]\n",
      "[Epoch 3/50] [Batch 599/938] [D loss: 1.061952] [G loss: 1.083032] [D real: 0.519721] [D fake: 0.055819]\n",
      "[Epoch 3/50] [Batch 699/938] [D loss: 0.747999] [G loss: 1.664471] [D real: 0.747432] [D fake: 0.275915]\n",
      "[Epoch 3/50] [Batch 799/938] [D loss: 0.602230] [G loss: 2.063261] [D real: 0.782622] [D fake: 0.228836]\n",
      "[Epoch 3/50] [Batch 899/938] [D loss: 0.713470] [G loss: 2.802439] [D real: 0.910275] [D fake: 0.433642]\n",
      "[Epoch 4/50] [Batch 99/938] [D loss: 0.758369] [G loss: 1.958862] [D real: 0.733107] [D fake: 0.265763]\n",
      "[Epoch 4/50] [Batch 199/938] [D loss: 0.518274] [G loss: 2.251640] [D real: 0.785604] [D fake: 0.181483]\n",
      "[Epoch 4/50] [Batch 299/938] [D loss: 0.961246] [G loss: 3.494993] [D real: 0.883743] [D fake: 0.516187]\n",
      "[Epoch 4/50] [Batch 399/938] [D loss: 0.654476] [G loss: 2.426998] [D real: 0.881916] [D fake: 0.328135]\n",
      "[Epoch 4/50] [Batch 499/938] [D loss: 0.776855] [G loss: 2.902700] [D real: 0.883033] [D fake: 0.426049]\n",
      "[Epoch 4/50] [Batch 599/938] [D loss: 0.757033] [G loss: 1.703925] [D real: 0.666320] [D fake: 0.133965]\n",
      "[Epoch 4/50] [Batch 699/938] [D loss: 0.973558] [G loss: 3.410165] [D real: 0.896379] [D fake: 0.538752]\n",
      "[Epoch 4/50] [Batch 799/938] [D loss: 0.579597] [G loss: 1.692054] [D real: 0.750553] [D fake: 0.153007]\n",
      "[Epoch 4/50] [Batch 899/938] [D loss: 0.895868] [G loss: 1.214793] [D real: 0.555651] [D fake: 0.079145]\n",
      "[Epoch 5/50] [Batch 99/938] [D loss: 0.672835] [G loss: 2.271495] [D real: 0.756007] [D fake: 0.221591]\n",
      "[Epoch 5/50] [Batch 199/938] [D loss: 0.891263] [G loss: 1.320002] [D real: 0.603029] [D fake: 0.097474]\n",
      "[Epoch 5/50] [Batch 299/938] [D loss: 0.888556] [G loss: 3.304013] [D real: 0.892127] [D fake: 0.472455]\n",
      "[Epoch 5/50] [Batch 399/938] [D loss: 0.721988] [G loss: 1.700896] [D real: 0.807591] [D fake: 0.298339]\n",
      "[Epoch 5/50] [Batch 499/938] [D loss: 0.568759] [G loss: 1.414347] [D real: 0.803386] [D fake: 0.198847]\n",
      "[Epoch 5/50] [Batch 599/938] [D loss: 1.449545] [G loss: 1.250590] [D real: 0.398080] [D fake: 0.044207]\n",
      "[Epoch 5/50] [Batch 699/938] [D loss: 0.712022] [G loss: 3.022157] [D real: 0.895293] [D fake: 0.386765]\n",
      "[Epoch 5/50] [Batch 799/938] [D loss: 0.688021] [G loss: 1.716207] [D real: 0.677747] [D fake: 0.128389]\n",
      "[Epoch 5/50] [Batch 899/938] [D loss: 0.868353] [G loss: 3.141034] [D real: 0.891348] [D fake: 0.469073]\n",
      "[Epoch 6/50] [Batch 99/938] [D loss: 0.757135] [G loss: 1.923608] [D real: 0.756920] [D fake: 0.273130]\n",
      "[Epoch 6/50] [Batch 199/938] [D loss: 0.656865] [G loss: 2.880515] [D real: 0.873631] [D fake: 0.348319]\n",
      "[Epoch 6/50] [Batch 299/938] [D loss: 0.582347] [G loss: 2.479143] [D real: 0.848604] [D fake: 0.284846]\n",
      "[Epoch 6/50] [Batch 399/938] [D loss: 0.588154] [G loss: 2.185070] [D real: 0.761326] [D fake: 0.144283]\n",
      "[Epoch 6/50] [Batch 499/938] [D loss: 0.606861] [G loss: 2.585125] [D real: 0.845141] [D fake: 0.280816]\n",
      "[Epoch 6/50] [Batch 599/938] [D loss: 0.634655] [G loss: 1.120588] [D real: 0.757753] [D fake: 0.201430]\n",
      "[Epoch 6/50] [Batch 699/938] [D loss: 0.562478] [G loss: 1.733955] [D real: 0.778244] [D fake: 0.189770]\n",
      "[Epoch 6/50] [Batch 799/938] [D loss: 1.117353] [G loss: 1.158865] [D real: 0.510962] [D fake: 0.071348]\n",
      "[Epoch 6/50] [Batch 899/938] [D loss: 0.901965] [G loss: 1.318145] [D real: 0.588752] [D fake: 0.128173]\n",
      "[Epoch 7/50] [Batch 99/938] [D loss: 0.860796] [G loss: 1.378806] [D real: 0.602816] [D fake: 0.123174]\n",
      "[Epoch 7/50] [Batch 199/938] [D loss: 0.743606] [G loss: 1.987261] [D real: 0.755502] [D fake: 0.291245]\n",
      "[Epoch 7/50] [Batch 299/938] [D loss: 0.790326] [G loss: 1.011701] [D real: 0.619002] [D fake: 0.103180]\n",
      "[Epoch 7/50] [Batch 399/938] [D loss: 0.711487] [G loss: 1.222054] [D real: 0.717312] [D fake: 0.218173]\n",
      "[Epoch 7/50] [Batch 499/938] [D loss: 0.742417] [G loss: 2.264832] [D real: 0.823280] [D fake: 0.352019]\n",
      "[Epoch 7/50] [Batch 599/938] [D loss: 0.605158] [G loss: 2.037129] [D real: 0.862393] [D fake: 0.312079]\n",
      "[Epoch 7/50] [Batch 699/938] [D loss: 0.764280] [G loss: 1.106724] [D real: 0.656304] [D fake: 0.143607]\n",
      "[Epoch 7/50] [Batch 799/938] [D loss: 0.844944] [G loss: 1.071977] [D real: 0.615597] [D fake: 0.148330]\n",
      "[Epoch 7/50] [Batch 899/938] [D loss: 0.802900] [G loss: 1.431965] [D real: 0.693363] [D fake: 0.194361]\n",
      "[Epoch 8/50] [Batch 99/938] [D loss: 0.625825] [G loss: 1.760566] [D real: 0.721809] [D fake: 0.147333]\n",
      "[Epoch 8/50] [Batch 199/938] [D loss: 0.886996] [G loss: 2.405303] [D real: 0.840529] [D fake: 0.420633]\n",
      "[Epoch 8/50] [Batch 299/938] [D loss: 0.814056] [G loss: 2.478310] [D real: 0.817943] [D fake: 0.386296]\n",
      "[Epoch 8/50] [Batch 399/938] [D loss: 0.674590] [G loss: 1.304559] [D real: 0.719212] [D fake: 0.175368]\n",
      "[Epoch 8/50] [Batch 499/938] [D loss: 0.670566] [G loss: 2.062091] [D real: 0.815878] [D fake: 0.295004]\n",
      "[Epoch 8/50] [Batch 599/938] [D loss: 0.751113] [G loss: 1.290096] [D real: 0.683266] [D fake: 0.168199]\n",
      "[Epoch 8/50] [Batch 699/938] [D loss: 0.815384] [G loss: 3.629774] [D real: 0.858980] [D fake: 0.415780]\n",
      "[Epoch 8/50] [Batch 799/938] [D loss: 0.686809] [G loss: 2.254912] [D real: 0.668771] [D fake: 0.087783]\n",
      "[Epoch 8/50] [Batch 899/938] [D loss: 0.645387] [G loss: 0.895882] [D real: 0.792874] [D fake: 0.246705]\n",
      "[Epoch 9/50] [Batch 99/938] [D loss: 0.620464] [G loss: 2.283550] [D real: 0.817051] [D fake: 0.254389]\n",
      "[Epoch 9/50] [Batch 199/938] [D loss: 1.290304] [G loss: 1.377569] [D real: 0.472947] [D fake: 0.046109]\n",
      "[Epoch 9/50] [Batch 299/938] [D loss: 0.805495] [G loss: 2.790646] [D real: 0.916277] [D fake: 0.405817]\n",
      "[Epoch 9/50] [Batch 399/938] [D loss: 0.796752] [G loss: 1.854381] [D real: 0.748746] [D fake: 0.280693]\n",
      "[Epoch 9/50] [Batch 499/938] [D loss: 0.755448] [G loss: 1.285504] [D real: 0.672611] [D fake: 0.141793]\n",
      "[Epoch 9/50] [Batch 599/938] [D loss: 0.835312] [G loss: 1.509580] [D real: 0.683783] [D fake: 0.229699]\n",
      "[Epoch 9/50] [Batch 699/938] [D loss: 0.785284] [G loss: 1.234055] [D real: 0.671937] [D fake: 0.198707]\n",
      "[Epoch 9/50] [Batch 799/938] [D loss: 0.663486] [G loss: 1.542892] [D real: 0.712252] [D fake: 0.181834]\n",
      "[Epoch 9/50] [Batch 899/938] [D loss: 1.004190] [G loss: 2.155226] [D real: 0.756356] [D fake: 0.383879]\n",
      "[Epoch 10/50] [Batch 99/938] [D loss: 0.585029] [G loss: 1.836838] [D real: 0.801155] [D fake: 0.252336]\n",
      "[Epoch 10/50] [Batch 199/938] [D loss: 0.863986] [G loss: 2.414002] [D real: 0.893381] [D fake: 0.433685]\n",
      "[Epoch 10/50] [Batch 299/938] [D loss: 1.000792] [G loss: 2.912137] [D real: 0.854744] [D fake: 0.472465]\n",
      "[Epoch 10/50] [Batch 399/938] [D loss: 0.994122] [G loss: 0.973822] [D real: 0.575960] [D fake: 0.139583]\n",
      "[Epoch 10/50] [Batch 499/938] [D loss: 0.751934] [G loss: 1.463637] [D real: 0.693840] [D fake: 0.209239]\n",
      "[Epoch 10/50] [Batch 599/938] [D loss: 0.693445] [G loss: 1.231287] [D real: 0.671254] [D fake: 0.137694]\n",
      "[Epoch 10/50] [Batch 699/938] [D loss: 0.715950] [G loss: 2.240090] [D real: 0.844480] [D fake: 0.324402]\n",
      "[Epoch 10/50] [Batch 799/938] [D loss: 0.603816] [G loss: 2.159047] [D real: 0.834177] [D fake: 0.272391]\n",
      "[Epoch 10/50] [Batch 899/938] [D loss: 0.745143] [G loss: 1.800512] [D real: 0.715909] [D fake: 0.184650]\n",
      "[Epoch 11/50] [Batch 99/938] [D loss: 0.726924] [G loss: 2.241805] [D real: 0.804554] [D fake: 0.297164]\n",
      "[Epoch 11/50] [Batch 199/938] [D loss: 0.929126] [G loss: 1.425640] [D real: 0.665191] [D fake: 0.252244]\n",
      "[Epoch 11/50] [Batch 299/938] [D loss: 0.793223] [G loss: 0.686925] [D real: 0.648961] [D fake: 0.186810]\n",
      "[Epoch 11/50] [Batch 399/938] [D loss: 1.043082] [G loss: 1.220341] [D real: 0.477326] [D fake: 0.081119]\n",
      "[Epoch 11/50] [Batch 499/938] [D loss: 0.793500] [G loss: 1.275099] [D real: 0.670806] [D fake: 0.216293]\n",
      "[Epoch 11/50] [Batch 599/938] [D loss: 0.824118] [G loss: 2.429905] [D real: 0.796411] [D fake: 0.345136]\n",
      "[Epoch 11/50] [Batch 699/938] [D loss: 0.850871] [G loss: 1.413696] [D real: 0.736187] [D fake: 0.307128]\n",
      "[Epoch 11/50] [Batch 799/938] [D loss: 0.619428] [G loss: 1.549505] [D real: 0.800626] [D fake: 0.240584]\n",
      "[Epoch 11/50] [Batch 899/938] [D loss: 1.265412] [G loss: 1.531039] [D real: 0.445531] [D fake: 0.108344]\n",
      "[Epoch 12/50] [Batch 99/938] [D loss: 0.998144] [G loss: 2.806797] [D real: 0.888902] [D fake: 0.514320]\n",
      "[Epoch 12/50] [Batch 199/938] [D loss: 0.881288] [G loss: 1.658691] [D real: 0.812074] [D fake: 0.413271]\n",
      "[Epoch 12/50] [Batch 299/938] [D loss: 1.007504] [G loss: 1.111418] [D real: 0.509712] [D fake: 0.108930]\n",
      "[Epoch 12/50] [Batch 399/938] [D loss: 0.829432] [G loss: 1.067688] [D real: 0.621749] [D fake: 0.180063]\n",
      "[Epoch 12/50] [Batch 499/938] [D loss: 0.843944] [G loss: 1.249476] [D real: 0.657764] [D fake: 0.197059]\n",
      "[Epoch 12/50] [Batch 599/938] [D loss: 0.807335] [G loss: 1.463161] [D real: 0.795120] [D fake: 0.328090]\n",
      "[Epoch 12/50] [Batch 699/938] [D loss: 0.929292] [G loss: 1.452618] [D real: 0.758593] [D fake: 0.384911]\n",
      "[Epoch 12/50] [Batch 799/938] [D loss: 0.785004] [G loss: 2.481990] [D real: 0.861286] [D fake: 0.409602]\n",
      "[Epoch 12/50] [Batch 899/938] [D loss: 0.875038] [G loss: 1.568852] [D real: 0.601556] [D fake: 0.161346]\n",
      "[Epoch 13/50] [Batch 99/938] [D loss: 0.890669] [G loss: 1.829857] [D real: 0.759508] [D fake: 0.376932]\n",
      "[Epoch 13/50] [Batch 199/938] [D loss: 0.860990] [G loss: 1.186264] [D real: 0.575776] [D fake: 0.152915]\n",
      "[Epoch 13/50] [Batch 299/938] [D loss: 0.948667] [G loss: 1.440624] [D real: 0.699347] [D fake: 0.292740]\n",
      "[Epoch 13/50] [Batch 399/938] [D loss: 0.886814] [G loss: 1.674564] [D real: 0.645847] [D fake: 0.217646]\n",
      "[Epoch 13/50] [Batch 499/938] [D loss: 0.749623] [G loss: 1.600031] [D real: 0.739938] [D fake: 0.248107]\n",
      "[Epoch 13/50] [Batch 599/938] [D loss: 1.017955] [G loss: 1.940038] [D real: 0.738537] [D fake: 0.409108]\n",
      "[Epoch 13/50] [Batch 699/938] [D loss: 1.091102] [G loss: 1.724399] [D real: 0.741268] [D fake: 0.421787]\n",
      "[Epoch 13/50] [Batch 799/938] [D loss: 0.806030] [G loss: 1.471736] [D real: 0.768111] [D fake: 0.320754]\n",
      "[Epoch 13/50] [Batch 899/938] [D loss: 0.810923] [G loss: 1.882832] [D real: 0.706538] [D fake: 0.277590]\n",
      "[Epoch 14/50] [Batch 99/938] [D loss: 0.741766] [G loss: 1.321297] [D real: 0.718296] [D fake: 0.251337]\n",
      "[Epoch 14/50] [Batch 199/938] [D loss: 0.775740] [G loss: 1.399667] [D real: 0.667681] [D fake: 0.157438]\n",
      "[Epoch 14/50] [Batch 299/938] [D loss: 1.028938] [G loss: 1.341957] [D real: 0.585608] [D fake: 0.192032]\n",
      "[Epoch 14/50] [Batch 399/938] [D loss: 0.876100] [G loss: 1.882665] [D real: 0.760742] [D fake: 0.360474]\n",
      "[Epoch 14/50] [Batch 499/938] [D loss: 0.785117] [G loss: 1.035407] [D real: 0.703428] [D fake: 0.283751]\n",
      "[Epoch 14/50] [Batch 599/938] [D loss: 1.058960] [G loss: 2.251801] [D real: 0.862505] [D fake: 0.506384]\n",
      "[Epoch 14/50] [Batch 699/938] [D loss: 0.794344] [G loss: 1.460968] [D real: 0.775646] [D fake: 0.324252]\n",
      "[Epoch 14/50] [Batch 799/938] [D loss: 0.805419] [G loss: 1.597527] [D real: 0.726249] [D fake: 0.277921]\n",
      "[Epoch 14/50] [Batch 899/938] [D loss: 0.805034] [G loss: 1.781719] [D real: 0.715406] [D fake: 0.230925]\n",
      "[Epoch 15/50] [Batch 99/938] [D loss: 0.931117] [G loss: 1.215303] [D real: 0.589948] [D fake: 0.190606]\n",
      "[Epoch 15/50] [Batch 199/938] [D loss: 0.929345] [G loss: 1.731461] [D real: 0.663696] [D fake: 0.258276]\n",
      "[Epoch 15/50] [Batch 299/938] [D loss: 0.862733] [G loss: 1.510107] [D real: 0.752771] [D fake: 0.323325]\n",
      "[Epoch 15/50] [Batch 399/938] [D loss: 0.946502] [G loss: 1.476890] [D real: 0.655212] [D fake: 0.261949]\n",
      "[Epoch 15/50] [Batch 499/938] [D loss: 0.905438] [G loss: 1.722960] [D real: 0.750549] [D fake: 0.364073]\n",
      "[Epoch 15/50] [Batch 599/938] [D loss: 1.086753] [G loss: 0.824157] [D real: 0.616569] [D fake: 0.221909]\n",
      "[Epoch 15/50] [Batch 699/938] [D loss: 0.912737] [G loss: 1.648334] [D real: 0.715995] [D fake: 0.311955]\n",
      "[Epoch 15/50] [Batch 799/938] [D loss: 0.715081] [G loss: 1.794764] [D real: 0.757676] [D fake: 0.260082]\n",
      "[Epoch 15/50] [Batch 899/938] [D loss: 0.903911] [G loss: 1.409741] [D real: 0.606748] [D fake: 0.171242]\n",
      "[Epoch 16/50] [Batch 99/938] [D loss: 0.988758] [G loss: 1.727291] [D real: 0.696539] [D fake: 0.294383]\n",
      "[Epoch 16/50] [Batch 199/938] [D loss: 0.815053] [G loss: 1.910006] [D real: 0.769084] [D fake: 0.323063]\n",
      "[Epoch 16/50] [Batch 299/938] [D loss: 0.726673] [G loss: 1.637437] [D real: 0.826212] [D fake: 0.304425]\n",
      "[Epoch 16/50] [Batch 399/938] [D loss: 0.826466] [G loss: 1.647387] [D real: 0.743335] [D fake: 0.265015]\n",
      "[Epoch 16/50] [Batch 499/938] [D loss: 0.780845] [G loss: 1.766940] [D real: 0.677583] [D fake: 0.210161]\n",
      "[Epoch 16/50] [Batch 599/938] [D loss: 0.774304] [G loss: 2.201499] [D real: 0.868158] [D fake: 0.395535]\n",
      "[Epoch 16/50] [Batch 699/938] [D loss: 0.697526] [G loss: 1.409759] [D real: 0.719130] [D fake: 0.205394]\n",
      "[Epoch 16/50] [Batch 799/938] [D loss: 0.784976] [G loss: 1.569075] [D real: 0.724371] [D fake: 0.238178]\n",
      "[Epoch 16/50] [Batch 899/938] [D loss: 0.886353] [G loss: 1.290240] [D real: 0.629754] [D fake: 0.223837]\n",
      "[Epoch 17/50] [Batch 99/938] [D loss: 0.816326] [G loss: 1.465803] [D real: 0.718224] [D fake: 0.256976]\n",
      "[Epoch 17/50] [Batch 199/938] [D loss: 1.037290] [G loss: 1.700892] [D real: 0.758871] [D fake: 0.404196]\n",
      "[Epoch 17/50] [Batch 299/938] [D loss: 0.802546] [G loss: 1.758170] [D real: 0.700754] [D fake: 0.220636]\n",
      "[Epoch 17/50] [Batch 399/938] [D loss: 0.840869] [G loss: 1.586584] [D real: 0.733809] [D fake: 0.320603]\n",
      "[Epoch 17/50] [Batch 499/938] [D loss: 0.878985] [G loss: 2.090951] [D real: 0.840315] [D fake: 0.395506]\n",
      "[Epoch 17/50] [Batch 599/938] [D loss: 0.909690] [G loss: 1.949527] [D real: 0.847175] [D fake: 0.461707]\n",
      "[Epoch 17/50] [Batch 699/938] [D loss: 0.870744] [G loss: 1.246687] [D real: 0.694530] [D fake: 0.283298]\n",
      "[Epoch 17/50] [Batch 799/938] [D loss: 1.035714] [G loss: 1.334465] [D real: 0.539033] [D fake: 0.127271]\n",
      "[Epoch 17/50] [Batch 899/938] [D loss: 0.893718] [G loss: 1.245830] [D real: 0.650294] [D fake: 0.239522]\n",
      "[Epoch 18/50] [Batch 99/938] [D loss: 1.008365] [G loss: 1.577319] [D real: 0.791282] [D fake: 0.380474]\n",
      "[Epoch 18/50] [Batch 199/938] [D loss: 1.058190] [G loss: 1.608120] [D real: 0.525697] [D fake: 0.145492]\n",
      "[Epoch 18/50] [Batch 299/938] [D loss: 0.906303] [G loss: 1.909658] [D real: 0.778218] [D fake: 0.338115]\n",
      "[Epoch 18/50] [Batch 399/938] [D loss: 0.767225] [G loss: 1.523520] [D real: 0.740309] [D fake: 0.253979]\n",
      "[Epoch 18/50] [Batch 499/938] [D loss: 0.884480] [G loss: 1.730944] [D real: 0.704555] [D fake: 0.293725]\n",
      "[Epoch 18/50] [Batch 599/938] [D loss: 0.940701] [G loss: 2.107994] [D real: 0.815629] [D fake: 0.421152]\n",
      "[Epoch 18/50] [Batch 699/938] [D loss: 0.817045] [G loss: 1.854147] [D real: 0.770904] [D fake: 0.323141]\n",
      "[Epoch 18/50] [Batch 799/938] [D loss: 0.914229] [G loss: 1.438585] [D real: 0.632071] [D fake: 0.217782]\n",
      "[Epoch 18/50] [Batch 899/938] [D loss: 0.966478] [G loss: 1.137481] [D real: 0.590551] [D fake: 0.183617]\n",
      "[Epoch 19/50] [Batch 99/938] [D loss: 0.990244] [G loss: 1.437009] [D real: 0.638784] [D fake: 0.267472]\n",
      "[Epoch 19/50] [Batch 199/938] [D loss: 0.735247] [G loss: 1.665559] [D real: 0.757004] [D fake: 0.273228]\n",
      "[Epoch 19/50] [Batch 299/938] [D loss: 0.994519] [G loss: 1.570989] [D real: 0.548715] [D fake: 0.136511]\n",
      "[Epoch 19/50] [Batch 399/938] [D loss: 0.625920] [G loss: 0.928873] [D real: 0.723271] [D fake: 0.177102]\n",
      "[Epoch 19/50] [Batch 499/938] [D loss: 0.879331] [G loss: 1.645470] [D real: 0.755603] [D fake: 0.305675]\n",
      "[Epoch 19/50] [Batch 599/938] [D loss: 0.724514] [G loss: 1.524391] [D real: 0.782509] [D fake: 0.309468]\n",
      "[Epoch 19/50] [Batch 699/938] [D loss: 0.861086] [G loss: 1.867394] [D real: 0.733434] [D fake: 0.261763]\n",
      "[Epoch 19/50] [Batch 799/938] [D loss: 0.964777] [G loss: 0.917126] [D real: 0.563274] [D fake: 0.158899]\n",
      "[Epoch 19/50] [Batch 899/938] [D loss: 0.860638] [G loss: 1.325583] [D real: 0.672406] [D fake: 0.252266]\n",
      "[Epoch 20/50] [Batch 99/938] [D loss: 0.853776] [G loss: 1.649491] [D real: 0.714997] [D fake: 0.239911]\n",
      "[Epoch 20/50] [Batch 199/938] [D loss: 0.750875] [G loss: 1.699427] [D real: 0.814686] [D fake: 0.326720]\n",
      "[Epoch 20/50] [Batch 299/938] [D loss: 0.777387] [G loss: 1.467146] [D real: 0.699233] [D fake: 0.217628]\n",
      "[Epoch 20/50] [Batch 399/938] [D loss: 0.690799] [G loss: 2.190600] [D real: 0.832078] [D fake: 0.319005]\n",
      "[Epoch 20/50] [Batch 499/938] [D loss: 0.769570] [G loss: 2.187182] [D real: 0.820387] [D fake: 0.369779]\n",
      "[Epoch 20/50] [Batch 599/938] [D loss: 1.001945] [G loss: 1.324010] [D real: 0.560157] [D fake: 0.167346]\n",
      "[Epoch 20/50] [Batch 699/938] [D loss: 0.774782] [G loss: 1.507886] [D real: 0.676868] [D fake: 0.198794]\n",
      "[Epoch 20/50] [Batch 799/938] [D loss: 0.873467] [G loss: 1.267569] [D real: 0.748242] [D fake: 0.347193]\n",
      "[Epoch 20/50] [Batch 899/938] [D loss: 0.688200] [G loss: 1.371630] [D real: 0.741006] [D fake: 0.224625]\n",
      "[Epoch 21/50] [Batch 99/938] [D loss: 0.761442] [G loss: 1.643064] [D real: 0.735169] [D fake: 0.275298]\n",
      "[Epoch 21/50] [Batch 199/938] [D loss: 0.806695] [G loss: 1.286208] [D real: 0.693394] [D fake: 0.248172]\n",
      "[Epoch 21/50] [Batch 299/938] [D loss: 0.956383] [G loss: 1.570774] [D real: 0.581721] [D fake: 0.129249]\n",
      "[Epoch 21/50] [Batch 399/938] [D loss: 0.779549] [G loss: 1.791317] [D real: 0.797054] [D fake: 0.333075]\n",
      "[Epoch 21/50] [Batch 499/938] [D loss: 0.867702] [G loss: 1.779055] [D real: 0.742206] [D fake: 0.284440]\n",
      "[Epoch 21/50] [Batch 599/938] [D loss: 0.715295] [G loss: 1.786792] [D real: 0.758394] [D fake: 0.230289]\n",
      "[Epoch 21/50] [Batch 699/938] [D loss: 0.723344] [G loss: 1.280100] [D real: 0.758633] [D fake: 0.246446]\n",
      "[Epoch 21/50] [Batch 799/938] [D loss: 0.621549] [G loss: 1.706459] [D real: 0.827906] [D fake: 0.283988]\n",
      "[Epoch 21/50] [Batch 899/938] [D loss: 0.742331] [G loss: 1.680556] [D real: 0.732230] [D fake: 0.228127]\n",
      "[Epoch 22/50] [Batch 99/938] [D loss: 0.735183] [G loss: 1.684805] [D real: 0.784727] [D fake: 0.312988]\n",
      "[Epoch 22/50] [Batch 199/938] [D loss: 1.011997] [G loss: 1.315785] [D real: 0.646628] [D fake: 0.302420]\n",
      "[Epoch 22/50] [Batch 299/938] [D loss: 0.884784] [G loss: 1.193358] [D real: 0.675973] [D fake: 0.246542]\n",
      "[Epoch 22/50] [Batch 399/938] [D loss: 0.820376] [G loss: 1.284221] [D real: 0.700129] [D fake: 0.246472]\n",
      "[Epoch 22/50] [Batch 499/938] [D loss: 1.154920] [G loss: 2.790078] [D real: 0.888848] [D fake: 0.498538]\n",
      "[Epoch 22/50] [Batch 599/938] [D loss: 0.914536] [G loss: 1.117588] [D real: 0.686973] [D fake: 0.262961]\n",
      "[Epoch 22/50] [Batch 699/938] [D loss: 0.809173] [G loss: 1.691692] [D real: 0.732508] [D fake: 0.288701]\n",
      "[Epoch 22/50] [Batch 799/938] [D loss: 0.768270] [G loss: 1.594890] [D real: 0.697782] [D fake: 0.193123]\n",
      "[Epoch 22/50] [Batch 899/938] [D loss: 0.932400] [G loss: 1.980029] [D real: 0.771534] [D fake: 0.356462]\n",
      "[Epoch 23/50] [Batch 99/938] [D loss: 0.982486] [G loss: 1.383295] [D real: 0.734775] [D fake: 0.392908]\n",
      "[Epoch 23/50] [Batch 199/938] [D loss: 0.786612] [G loss: 1.564827] [D real: 0.761428] [D fake: 0.318414]\n",
      "[Epoch 23/50] [Batch 299/938] [D loss: 0.990098] [G loss: 2.205540] [D real: 0.763568] [D fake: 0.373968]\n",
      "[Epoch 23/50] [Batch 399/938] [D loss: 0.955176] [G loss: 1.645146] [D real: 0.640100] [D fake: 0.212718]\n",
      "[Epoch 23/50] [Batch 499/938] [D loss: 0.887109] [G loss: 1.106529] [D real: 0.665661] [D fake: 0.240386]\n",
      "[Epoch 23/50] [Batch 599/938] [D loss: 0.811911] [G loss: 1.584595] [D real: 0.747688] [D fake: 0.276645]\n",
      "[Epoch 23/50] [Batch 699/938] [D loss: 0.852122] [G loss: 1.350853] [D real: 0.631443] [D fake: 0.162903]\n",
      "[Epoch 23/50] [Batch 799/938] [D loss: 0.925636] [G loss: 1.629809] [D real: 0.715926] [D fake: 0.324953]\n",
      "[Epoch 23/50] [Batch 899/938] [D loss: 0.810630] [G loss: 1.381878] [D real: 0.665414] [D fake: 0.169442]\n",
      "[Epoch 24/50] [Batch 99/938] [D loss: 0.661623] [G loss: 1.629698] [D real: 0.789470] [D fake: 0.245848]\n",
      "[Epoch 24/50] [Batch 199/938] [D loss: 0.763562] [G loss: 1.712402] [D real: 0.773379] [D fake: 0.303093]\n",
      "[Epoch 24/50] [Batch 299/938] [D loss: 0.854607] [G loss: 1.466053] [D real: 0.685213] [D fake: 0.240947]\n",
      "[Epoch 24/50] [Batch 399/938] [D loss: 0.862375] [G loss: 1.212896] [D real: 0.693910] [D fake: 0.234000]\n",
      "[Epoch 24/50] [Batch 499/938] [D loss: 0.954628] [G loss: 1.159954] [D real: 0.583451] [D fake: 0.186850]\n",
      "[Epoch 24/50] [Batch 599/938] [D loss: 0.965539] [G loss: 1.329458] [D real: 0.605756] [D fake: 0.210973]\n",
      "[Epoch 24/50] [Batch 699/938] [D loss: 0.769910] [G loss: 1.981256] [D real: 0.753240] [D fake: 0.255194]\n",
      "[Epoch 24/50] [Batch 799/938] [D loss: 0.927635] [G loss: 1.477203] [D real: 0.690576] [D fake: 0.279271]\n",
      "[Epoch 24/50] [Batch 899/938] [D loss: 0.707829] [G loss: 1.901513] [D real: 0.819066] [D fake: 0.327878]\n",
      "[Epoch 25/50] [Batch 99/938] [D loss: 0.787720] [G loss: 1.542477] [D real: 0.762537] [D fake: 0.290978]\n",
      "[Epoch 25/50] [Batch 199/938] [D loss: 1.008632] [G loss: 1.779666] [D real: 0.684200] [D fake: 0.292110]\n",
      "[Epoch 25/50] [Batch 299/938] [D loss: 0.835749] [G loss: 1.682732] [D real: 0.719988] [D fake: 0.288915]\n",
      "[Epoch 25/50] [Batch 399/938] [D loss: 0.833590] [G loss: 1.428038] [D real: 0.689358] [D fake: 0.210755]\n",
      "[Epoch 25/50] [Batch 499/938] [D loss: 0.836400] [G loss: 1.607939] [D real: 0.685720] [D fake: 0.256404]\n",
      "[Epoch 25/50] [Batch 599/938] [D loss: 0.851491] [G loss: 1.016749] [D real: 0.719237] [D fake: 0.282849]\n",
      "[Epoch 25/50] [Batch 699/938] [D loss: 0.904009] [G loss: 1.372659] [D real: 0.685600] [D fake: 0.261220]\n",
      "[Epoch 25/50] [Batch 799/938] [D loss: 0.867306] [G loss: 1.728831] [D real: 0.718950] [D fake: 0.282435]\n",
      "[Epoch 25/50] [Batch 899/938] [D loss: 0.855693] [G loss: 1.690933] [D real: 0.700046] [D fake: 0.283169]\n",
      "[Epoch 26/50] [Batch 99/938] [D loss: 0.784472] [G loss: 1.897958] [D real: 0.822204] [D fake: 0.347784]\n",
      "[Epoch 26/50] [Batch 199/938] [D loss: 0.757146] [G loss: 1.701087] [D real: 0.739163] [D fake: 0.232393]\n",
      "[Epoch 26/50] [Batch 299/938] [D loss: 0.879055] [G loss: 2.056966] [D real: 0.797734] [D fake: 0.367657]\n",
      "[Epoch 26/50] [Batch 399/938] [D loss: 0.894463] [G loss: 1.486220] [D real: 0.730926] [D fake: 0.323301]\n",
      "[Epoch 26/50] [Batch 499/938] [D loss: 0.936277] [G loss: 1.650205] [D real: 0.585926] [D fake: 0.145488]\n",
      "[Epoch 26/50] [Batch 599/938] [D loss: 0.851704] [G loss: 1.323658] [D real: 0.718686] [D fake: 0.296458]\n",
      "[Epoch 26/50] [Batch 699/938] [D loss: 1.052634] [G loss: 1.220706] [D real: 0.556292] [D fake: 0.149052]\n",
      "[Epoch 26/50] [Batch 799/938] [D loss: 0.766958] [G loss: 1.528662] [D real: 0.707635] [D fake: 0.246563]\n",
      "[Epoch 26/50] [Batch 899/938] [D loss: 0.926933] [G loss: 1.925752] [D real: 0.798699] [D fake: 0.396554]\n",
      "[Epoch 27/50] [Batch 99/938] [D loss: 0.817871] [G loss: 1.150488] [D real: 0.668418] [D fake: 0.217467]\n",
      "[Epoch 27/50] [Batch 199/938] [D loss: 0.850079] [G loss: 1.249545] [D real: 0.727456] [D fake: 0.280815]\n",
      "[Epoch 27/50] [Batch 299/938] [D loss: 1.190607] [G loss: 1.261492] [D real: 0.496010] [D fake: 0.170082]\n",
      "[Epoch 27/50] [Batch 399/938] [D loss: 0.867250] [G loss: 1.349928] [D real: 0.651054] [D fake: 0.183281]\n",
      "[Epoch 27/50] [Batch 499/938] [D loss: 0.832399] [G loss: 1.427197] [D real: 0.720698] [D fake: 0.295464]\n",
      "[Epoch 27/50] [Batch 599/938] [D loss: 0.711844] [G loss: 1.760242] [D real: 0.784962] [D fake: 0.300592]\n",
      "[Epoch 27/50] [Batch 699/938] [D loss: 0.775481] [G loss: 1.629638] [D real: 0.781866] [D fake: 0.322813]\n",
      "[Epoch 27/50] [Batch 799/938] [D loss: 0.750958] [G loss: 1.481531] [D real: 0.735942] [D fake: 0.273708]\n",
      "[Epoch 27/50] [Batch 899/938] [D loss: 0.863555] [G loss: 1.894938] [D real: 0.768874] [D fake: 0.306522]\n",
      "[Epoch 28/50] [Batch 99/938] [D loss: 0.784560] [G loss: 1.695754] [D real: 0.809025] [D fake: 0.327841]\n",
      "[Epoch 28/50] [Batch 199/938] [D loss: 0.839190] [G loss: 1.642447] [D real: 0.686091] [D fake: 0.228318]\n",
      "[Epoch 28/50] [Batch 299/938] [D loss: 0.776007] [G loss: 1.424868] [D real: 0.800454] [D fake: 0.338141]\n",
      "[Epoch 28/50] [Batch 399/938] [D loss: 0.790451] [G loss: 1.341826] [D real: 0.658225] [D fake: 0.215396]\n",
      "[Epoch 28/50] [Batch 499/938] [D loss: 0.752559] [G loss: 1.687359] [D real: 0.763827] [D fake: 0.308234]\n",
      "[Epoch 28/50] [Batch 599/938] [D loss: 0.924791] [G loss: 1.280584] [D real: 0.669527] [D fake: 0.252055]\n",
      "[Epoch 28/50] [Batch 699/938] [D loss: 0.865909] [G loss: 1.949349] [D real: 0.808298] [D fake: 0.370105]\n",
      "[Epoch 28/50] [Batch 799/938] [D loss: 0.823606] [G loss: 1.501570] [D real: 0.703845] [D fake: 0.250927]\n",
      "[Epoch 28/50] [Batch 899/938] [D loss: 0.882900] [G loss: 1.594027] [D real: 0.641977] [D fake: 0.195890]\n",
      "[Epoch 29/50] [Batch 99/938] [D loss: 0.788984] [G loss: 1.593629] [D real: 0.785864] [D fake: 0.325123]\n",
      "[Epoch 29/50] [Batch 199/938] [D loss: 0.824965] [G loss: 1.817970] [D real: 0.699383] [D fake: 0.282742]\n",
      "[Epoch 29/50] [Batch 299/938] [D loss: 0.950061] [G loss: 1.591984] [D real: 0.688685] [D fake: 0.241040]\n",
      "[Epoch 29/50] [Batch 399/938] [D loss: 0.915272] [G loss: 1.548579] [D real: 0.639019] [D fake: 0.203944]\n",
      "[Epoch 29/50] [Batch 499/938] [D loss: 0.880699] [G loss: 1.827883] [D real: 0.720847] [D fake: 0.242963]\n",
      "[Epoch 29/50] [Batch 599/938] [D loss: 0.860818] [G loss: 1.776566] [D real: 0.710485] [D fake: 0.255030]\n",
      "[Epoch 29/50] [Batch 699/938] [D loss: 0.806936] [G loss: 1.992599] [D real: 0.852896] [D fake: 0.364966]\n",
      "[Epoch 29/50] [Batch 799/938] [D loss: 0.963224] [G loss: 1.368741] [D real: 0.640048] [D fake: 0.269807]\n",
      "[Epoch 29/50] [Batch 899/938] [D loss: 0.798023] [G loss: 1.791596] [D real: 0.789375] [D fake: 0.316291]\n",
      "[Epoch 30/50] [Batch 99/938] [D loss: 0.957970] [G loss: 1.855254] [D real: 0.834219] [D fake: 0.463352]\n",
      "[Epoch 30/50] [Batch 199/938] [D loss: 0.859572] [G loss: 1.741667] [D real: 0.763854] [D fake: 0.317134]\n",
      "[Epoch 30/50] [Batch 299/938] [D loss: 0.762081] [G loss: 1.815994] [D real: 0.754460] [D fake: 0.268347]\n",
      "[Epoch 30/50] [Batch 399/938] [D loss: 0.848291] [G loss: 1.924066] [D real: 0.783100] [D fake: 0.335655]\n",
      "[Epoch 30/50] [Batch 499/938] [D loss: 0.854734] [G loss: 1.534568] [D real: 0.733346] [D fake: 0.274487]\n",
      "[Epoch 30/50] [Batch 599/938] [D loss: 0.816115] [G loss: 1.731795] [D real: 0.716222] [D fake: 0.259078]\n",
      "[Epoch 30/50] [Batch 699/938] [D loss: 0.675000] [G loss: 1.552533] [D real: 0.780408] [D fake: 0.262338]\n",
      "[Epoch 30/50] [Batch 799/938] [D loss: 0.811331] [G loss: 1.598769] [D real: 0.710183] [D fake: 0.217262]\n",
      "[Epoch 30/50] [Batch 899/938] [D loss: 0.804571] [G loss: 1.455755] [D real: 0.759621] [D fake: 0.318152]\n",
      "[Epoch 31/50] [Batch 99/938] [D loss: 0.715784] [G loss: 1.869167] [D real: 0.766700] [D fake: 0.250919]\n",
      "[Epoch 31/50] [Batch 199/938] [D loss: 0.713681] [G loss: 1.851304] [D real: 0.828433] [D fake: 0.295817]\n",
      "[Epoch 31/50] [Batch 299/938] [D loss: 0.822820] [G loss: 1.290855] [D real: 0.679021] [D fake: 0.240949]\n",
      "[Epoch 31/50] [Batch 399/938] [D loss: 0.765815] [G loss: 1.439387] [D real: 0.705647] [D fake: 0.220066]\n",
      "[Epoch 31/50] [Batch 499/938] [D loss: 0.786911] [G loss: 1.193815] [D real: 0.670573] [D fake: 0.206844]\n",
      "[Epoch 31/50] [Batch 599/938] [D loss: 0.843428] [G loss: 1.423475] [D real: 0.738891] [D fake: 0.315588]\n",
      "[Epoch 31/50] [Batch 699/938] [D loss: 0.993113] [G loss: 1.978936] [D real: 0.693670] [D fake: 0.255766]\n",
      "[Epoch 31/50] [Batch 799/938] [D loss: 0.823825] [G loss: 1.472962] [D real: 0.736328] [D fake: 0.280650]\n",
      "[Epoch 31/50] [Batch 899/938] [D loss: 0.933101] [G loss: 1.926161] [D real: 0.662069] [D fake: 0.231653]\n",
      "[Epoch 32/50] [Batch 99/938] [D loss: 0.897092] [G loss: 1.705246] [D real: 0.691399] [D fake: 0.258618]\n",
      "[Epoch 32/50] [Batch 199/938] [D loss: 0.843061] [G loss: 1.543315] [D real: 0.793063] [D fake: 0.349318]\n",
      "[Epoch 32/50] [Batch 299/938] [D loss: 0.998563] [G loss: 1.986050] [D real: 0.751114] [D fake: 0.359334]\n",
      "[Epoch 32/50] [Batch 399/938] [D loss: 0.758947] [G loss: 1.660197] [D real: 0.693238] [D fake: 0.238168]\n",
      "[Epoch 32/50] [Batch 499/938] [D loss: 0.894073] [G loss: 1.897537] [D real: 0.653584] [D fake: 0.203102]\n",
      "[Epoch 32/50] [Batch 599/938] [D loss: 0.902515] [G loss: 1.205346] [D real: 0.633639] [D fake: 0.203707]\n",
      "[Epoch 32/50] [Batch 699/938] [D loss: 0.785396] [G loss: 2.042504] [D real: 0.761334] [D fake: 0.306460]\n",
      "[Epoch 32/50] [Batch 799/938] [D loss: 0.737707] [G loss: 1.532671] [D real: 0.729227] [D fake: 0.211801]\n",
      "[Epoch 32/50] [Batch 899/938] [D loss: 0.731381] [G loss: 1.443850] [D real: 0.687680] [D fake: 0.214005]\n",
      "[Epoch 33/50] [Batch 99/938] [D loss: 0.774346] [G loss: 1.542342] [D real: 0.726914] [D fake: 0.260132]\n",
      "[Epoch 33/50] [Batch 199/938] [D loss: 0.741614] [G loss: 1.264198] [D real: 0.803821] [D fake: 0.334699]\n",
      "[Epoch 33/50] [Batch 299/938] [D loss: 0.899967] [G loss: 1.842294] [D real: 0.741861] [D fake: 0.299852]\n",
      "[Epoch 33/50] [Batch 399/938] [D loss: 0.872049] [G loss: 1.134347] [D real: 0.692810] [D fake: 0.270007]\n",
      "[Epoch 33/50] [Batch 499/938] [D loss: 0.890824] [G loss: 1.660801] [D real: 0.663515] [D fake: 0.222103]\n",
      "[Epoch 33/50] [Batch 599/938] [D loss: 0.960400] [G loss: 1.401709] [D real: 0.640671] [D fake: 0.281140]\n",
      "[Epoch 33/50] [Batch 699/938] [D loss: 0.899814] [G loss: 1.497678] [D real: 0.720873] [D fake: 0.344666]\n",
      "[Epoch 33/50] [Batch 799/938] [D loss: 0.887981] [G loss: 1.545532] [D real: 0.645766] [D fake: 0.211014]\n",
      "[Epoch 33/50] [Batch 899/938] [D loss: 0.938686] [G loss: 1.587149] [D real: 0.698514] [D fake: 0.279279]\n",
      "[Epoch 34/50] [Batch 99/938] [D loss: 0.789855] [G loss: 1.607312] [D real: 0.732673] [D fake: 0.260035]\n",
      "[Epoch 34/50] [Batch 199/938] [D loss: 0.780256] [G loss: 1.782979] [D real: 0.697492] [D fake: 0.210222]\n",
      "[Epoch 34/50] [Batch 299/938] [D loss: 0.962134] [G loss: 1.472205] [D real: 0.759347] [D fake: 0.362430]\n",
      "[Epoch 34/50] [Batch 399/938] [D loss: 0.907358] [G loss: 1.986102] [D real: 0.804466] [D fake: 0.377525]\n",
      "[Epoch 34/50] [Batch 499/938] [D loss: 0.886169] [G loss: 1.339623] [D real: 0.655195] [D fake: 0.253244]\n",
      "[Epoch 34/50] [Batch 599/938] [D loss: 0.838728] [G loss: 1.641423] [D real: 0.774292] [D fake: 0.308094]\n",
      "[Epoch 34/50] [Batch 699/938] [D loss: 0.855466] [G loss: 1.947909] [D real: 0.702657] [D fake: 0.292171]\n",
      "[Epoch 34/50] [Batch 799/938] [D loss: 0.824729] [G loss: 1.692450] [D real: 0.716927] [D fake: 0.292250]\n",
      "[Epoch 34/50] [Batch 899/938] [D loss: 0.833417] [G loss: 1.878363] [D real: 0.724174] [D fake: 0.311675]\n",
      "[Epoch 35/50] [Batch 99/938] [D loss: 0.949932] [G loss: 1.729441] [D real: 0.720613] [D fake: 0.317565]\n",
      "[Epoch 35/50] [Batch 199/938] [D loss: 0.894117] [G loss: 1.475115] [D real: 0.698477] [D fake: 0.292641]\n",
      "[Epoch 35/50] [Batch 299/938] [D loss: 0.780288] [G loss: 1.753658] [D real: 0.738136] [D fake: 0.249825]\n",
      "[Epoch 35/50] [Batch 399/938] [D loss: 1.130448] [G loss: 1.892385] [D real: 0.571294] [D fake: 0.236367]\n",
      "[Epoch 35/50] [Batch 499/938] [D loss: 0.817966] [G loss: 1.683167] [D real: 0.770522] [D fake: 0.309175]\n",
      "[Epoch 35/50] [Batch 599/938] [D loss: 0.831018] [G loss: 1.152942] [D real: 0.701968] [D fake: 0.278428]\n",
      "[Epoch 35/50] [Batch 699/938] [D loss: 0.949227] [G loss: 1.226694] [D real: 0.624616] [D fake: 0.223397]\n",
      "[Epoch 35/50] [Batch 799/938] [D loss: 0.935883] [G loss: 1.637138] [D real: 0.739337] [D fake: 0.313138]\n",
      "[Epoch 35/50] [Batch 899/938] [D loss: 0.785635] [G loss: 1.362802] [D real: 0.691356] [D fake: 0.216833]\n",
      "[Epoch 36/50] [Batch 99/938] [D loss: 0.786064] [G loss: 1.851323] [D real: 0.710165] [D fake: 0.240431]\n",
      "[Epoch 36/50] [Batch 199/938] [D loss: 0.815193] [G loss: 1.458982] [D real: 0.731343] [D fake: 0.294531]\n",
      "[Epoch 36/50] [Batch 299/938] [D loss: 0.917066] [G loss: 1.522777] [D real: 0.677867] [D fake: 0.277151]\n",
      "[Epoch 36/50] [Batch 399/938] [D loss: 0.803880] [G loss: 1.858932] [D real: 0.716344] [D fake: 0.227754]\n",
      "[Epoch 36/50] [Batch 499/938] [D loss: 0.785596] [G loss: 1.430561] [D real: 0.843396] [D fake: 0.386033]\n",
      "[Epoch 36/50] [Batch 599/938] [D loss: 0.771108] [G loss: 1.478197] [D real: 0.769307] [D fake: 0.299376]\n",
      "[Epoch 36/50] [Batch 699/938] [D loss: 0.993595] [G loss: 2.167043] [D real: 0.783149] [D fake: 0.352609]\n",
      "[Epoch 36/50] [Batch 799/938] [D loss: 0.921980] [G loss: 1.596812] [D real: 0.654219] [D fake: 0.251842]\n",
      "[Epoch 36/50] [Batch 899/938] [D loss: 0.798194] [G loss: 1.764334] [D real: 0.754259] [D fake: 0.291613]\n",
      "[Epoch 37/50] [Batch 99/938] [D loss: 0.989790] [G loss: 1.212510] [D real: 0.575678] [D fake: 0.212425]\n",
      "[Epoch 37/50] [Batch 199/938] [D loss: 0.972556] [G loss: 1.445073] [D real: 0.560200] [D fake: 0.176024]\n",
      "[Epoch 37/50] [Batch 299/938] [D loss: 0.683281] [G loss: 1.854135] [D real: 0.729414] [D fake: 0.197653]\n",
      "[Epoch 37/50] [Batch 399/938] [D loss: 0.715631] [G loss: 1.675784] [D real: 0.753247] [D fake: 0.249440]\n",
      "[Epoch 37/50] [Batch 499/938] [D loss: 0.822131] [G loss: 1.437031] [D real: 0.667866] [D fake: 0.217173]\n",
      "[Epoch 37/50] [Batch 599/938] [D loss: 0.846459] [G loss: 1.556744] [D real: 0.675861] [D fake: 0.254311]\n",
      "[Epoch 37/50] [Batch 699/938] [D loss: 0.958051] [G loss: 1.691528] [D real: 0.717823] [D fake: 0.330651]\n",
      "[Epoch 37/50] [Batch 799/938] [D loss: 0.732254] [G loss: 1.917275] [D real: 0.834580] [D fake: 0.348372]\n",
      "[Epoch 37/50] [Batch 899/938] [D loss: 0.826903] [G loss: 1.602573] [D real: 0.705007] [D fake: 0.251098]\n",
      "[Epoch 38/50] [Batch 99/938] [D loss: 0.999645] [G loss: 1.394849] [D real: 0.583770] [D fake: 0.190192]\n",
      "[Epoch 38/50] [Batch 199/938] [D loss: 0.655779] [G loss: 1.725864] [D real: 0.853894] [D fake: 0.328703]\n",
      "[Epoch 38/50] [Batch 299/938] [D loss: 0.770499] [G loss: 1.698682] [D real: 0.811263] [D fake: 0.336565]\n",
      "[Epoch 38/50] [Batch 399/938] [D loss: 0.729043] [G loss: 1.412403] [D real: 0.779240] [D fake: 0.308571]\n",
      "[Epoch 38/50] [Batch 499/938] [D loss: 0.929655] [G loss: 1.560534] [D real: 0.703148] [D fake: 0.305985]\n",
      "[Epoch 38/50] [Batch 599/938] [D loss: 0.893789] [G loss: 1.288077] [D real: 0.728303] [D fake: 0.303542]\n",
      "[Epoch 38/50] [Batch 699/938] [D loss: 0.915985] [G loss: 1.750198] [D real: 0.739416] [D fake: 0.272207]\n",
      "[Epoch 38/50] [Batch 799/938] [D loss: 0.976280] [G loss: 1.244353] [D real: 0.628839] [D fake: 0.229577]\n",
      "[Epoch 38/50] [Batch 899/938] [D loss: 0.900436] [G loss: 1.248965] [D real: 0.689461] [D fake: 0.301257]\n",
      "[Epoch 39/50] [Batch 99/938] [D loss: 1.019094] [G loss: 1.326650] [D real: 0.684947] [D fake: 0.299861]\n",
      "[Epoch 39/50] [Batch 199/938] [D loss: 0.941296] [G loss: 1.767843] [D real: 0.755878] [D fake: 0.329038]\n",
      "[Epoch 39/50] [Batch 299/938] [D loss: 1.064491] [G loss: 1.567751] [D real: 0.663878] [D fake: 0.293270]\n",
      "[Epoch 39/50] [Batch 399/938] [D loss: 1.044402] [G loss: 2.536306] [D real: 0.790035] [D fake: 0.372878]\n",
      "[Epoch 39/50] [Batch 499/938] [D loss: 0.882361] [G loss: 1.825432] [D real: 0.741555] [D fake: 0.313430]\n",
      "[Epoch 39/50] [Batch 599/938] [D loss: 0.732957] [G loss: 1.758522] [D real: 0.786306] [D fake: 0.325249]\n",
      "[Epoch 39/50] [Batch 699/938] [D loss: 0.940877] [G loss: 1.754206] [D real: 0.754512] [D fake: 0.371297]\n",
      "[Epoch 39/50] [Batch 799/938] [D loss: 0.957934] [G loss: 1.612426] [D real: 0.709744] [D fake: 0.298742]\n",
      "[Epoch 39/50] [Batch 899/938] [D loss: 0.831728] [G loss: 1.294400] [D real: 0.736376] [D fake: 0.311199]\n",
      "[Epoch 40/50] [Batch 99/938] [D loss: 0.883551] [G loss: 1.572057] [D real: 0.742895] [D fake: 0.324401]\n",
      "[Epoch 40/50] [Batch 199/938] [D loss: 0.875794] [G loss: 1.625771] [D real: 0.748344] [D fake: 0.330363]\n",
      "[Epoch 40/50] [Batch 299/938] [D loss: 0.929081] [G loss: 1.369922] [D real: 0.603901] [D fake: 0.209829]\n",
      "[Epoch 40/50] [Batch 399/938] [D loss: 0.895689] [G loss: 1.771492] [D real: 0.701759] [D fake: 0.292084]\n",
      "[Epoch 40/50] [Batch 499/938] [D loss: 0.862609] [G loss: 1.216804] [D real: 0.679223] [D fake: 0.224113]\n",
      "[Epoch 40/50] [Batch 599/938] [D loss: 0.907654] [G loss: 1.564955] [D real: 0.752616] [D fake: 0.360582]\n",
      "[Epoch 40/50] [Batch 699/938] [D loss: 1.094503] [G loss: 1.800835] [D real: 0.681758] [D fake: 0.347525]\n",
      "[Epoch 40/50] [Batch 799/938] [D loss: 0.728355] [G loss: 1.628655] [D real: 0.719561] [D fake: 0.240405]\n",
      "[Epoch 40/50] [Batch 899/938] [D loss: 0.807734] [G loss: 1.309336] [D real: 0.765640] [D fake: 0.318520]\n",
      "[Epoch 41/50] [Batch 99/938] [D loss: 0.921712] [G loss: 1.437195] [D real: 0.730842] [D fake: 0.324057]\n",
      "[Epoch 41/50] [Batch 199/938] [D loss: 1.138052] [G loss: 1.313327] [D real: 0.665542] [D fake: 0.391346]\n",
      "[Epoch 41/50] [Batch 299/938] [D loss: 0.972073] [G loss: 1.352618] [D real: 0.664563] [D fake: 0.306398]\n",
      "[Epoch 41/50] [Batch 399/938] [D loss: 0.919151] [G loss: 1.263065] [D real: 0.660987] [D fake: 0.301672]\n",
      "[Epoch 41/50] [Batch 499/938] [D loss: 0.865719] [G loss: 1.550449] [D real: 0.771746] [D fake: 0.325663]\n",
      "[Epoch 41/50] [Batch 599/938] [D loss: 0.947543] [G loss: 1.365778] [D real: 0.661502] [D fake: 0.254253]\n",
      "[Epoch 41/50] [Batch 699/938] [D loss: 0.805539] [G loss: 1.517276] [D real: 0.759419] [D fake: 0.280411]\n",
      "[Epoch 41/50] [Batch 799/938] [D loss: 1.020065] [G loss: 1.445172] [D real: 0.664998] [D fake: 0.323297]\n",
      "[Epoch 41/50] [Batch 899/938] [D loss: 0.953198] [G loss: 1.572674] [D real: 0.710405] [D fake: 0.328717]\n",
      "[Epoch 42/50] [Batch 99/938] [D loss: 0.816439] [G loss: 1.353943] [D real: 0.690581] [D fake: 0.245716]\n",
      "[Epoch 42/50] [Batch 199/938] [D loss: 0.850532] [G loss: 1.646129] [D real: 0.722722] [D fake: 0.296973]\n",
      "[Epoch 42/50] [Batch 299/938] [D loss: 1.003945] [G loss: 1.286542] [D real: 0.590679] [D fake: 0.217264]\n",
      "[Epoch 42/50] [Batch 399/938] [D loss: 1.037060] [G loss: 1.455520] [D real: 0.590611] [D fake: 0.203297]\n",
      "[Epoch 42/50] [Batch 499/938] [D loss: 1.095125] [G loss: 1.854748] [D real: 0.772128] [D fake: 0.431257]\n",
      "[Epoch 42/50] [Batch 599/938] [D loss: 0.862304] [G loss: 1.373016] [D real: 0.755454] [D fake: 0.357130]\n",
      "[Epoch 42/50] [Batch 699/938] [D loss: 0.984968] [G loss: 1.649074] [D real: 0.608021] [D fake: 0.255831]\n",
      "[Epoch 42/50] [Batch 799/938] [D loss: 0.955502] [G loss: 1.512798] [D real: 0.712142] [D fake: 0.340402]\n",
      "[Epoch 42/50] [Batch 899/938] [D loss: 0.969804] [G loss: 1.320155] [D real: 0.593187] [D fake: 0.271617]\n",
      "[Epoch 43/50] [Batch 99/938] [D loss: 0.842690] [G loss: 1.919009] [D real: 0.710114] [D fake: 0.251312]\n",
      "[Epoch 43/50] [Batch 199/938] [D loss: 0.941588] [G loss: 1.687538] [D real: 0.686138] [D fake: 0.308082]\n",
      "[Epoch 43/50] [Batch 299/938] [D loss: 0.792523] [G loss: 1.615565] [D real: 0.759080] [D fake: 0.300018]\n",
      "[Epoch 43/50] [Batch 399/938] [D loss: 0.868569] [G loss: 1.278169] [D real: 0.674485] [D fake: 0.243508]\n",
      "[Epoch 43/50] [Batch 499/938] [D loss: 0.805660] [G loss: 1.329596] [D real: 0.689287] [D fake: 0.263541]\n",
      "[Epoch 43/50] [Batch 599/938] [D loss: 0.854435] [G loss: 1.663443] [D real: 0.698794] [D fake: 0.279898]\n",
      "[Epoch 43/50] [Batch 699/938] [D loss: 1.019938] [G loss: 1.110236] [D real: 0.595762] [D fake: 0.251176]\n",
      "[Epoch 43/50] [Batch 799/938] [D loss: 1.040914] [G loss: 1.235213] [D real: 0.674492] [D fake: 0.339836]\n",
      "[Epoch 43/50] [Batch 899/938] [D loss: 1.041195] [G loss: 1.346768] [D real: 0.637134] [D fake: 0.299691]\n",
      "[Epoch 44/50] [Batch 99/938] [D loss: 0.878913] [G loss: 1.714763] [D real: 0.661998] [D fake: 0.255969]\n",
      "[Epoch 44/50] [Batch 199/938] [D loss: 0.869188] [G loss: 1.882116] [D real: 0.774783] [D fake: 0.320992]\n",
      "[Epoch 44/50] [Batch 299/938] [D loss: 0.819642] [G loss: 1.823882] [D real: 0.784727] [D fake: 0.339147]\n",
      "[Epoch 44/50] [Batch 399/938] [D loss: 0.916612] [G loss: 1.619301] [D real: 0.654775] [D fake: 0.227619]\n",
      "[Epoch 44/50] [Batch 499/938] [D loss: 0.954200] [G loss: 1.115374] [D real: 0.615605] [D fake: 0.243305]\n",
      "[Epoch 44/50] [Batch 599/938] [D loss: 0.766504] [G loss: 1.392268] [D real: 0.812765] [D fake: 0.358969]\n",
      "[Epoch 44/50] [Batch 699/938] [D loss: 0.870314] [G loss: 1.174707] [D real: 0.662164] [D fake: 0.246091]\n",
      "[Epoch 44/50] [Batch 799/938] [D loss: 0.978166] [G loss: 1.600576] [D real: 0.607146] [D fake: 0.207022]\n",
      "[Epoch 44/50] [Batch 899/938] [D loss: 1.032278] [G loss: 1.232499] [D real: 0.589884] [D fake: 0.219318]\n",
      "[Epoch 45/50] [Batch 99/938] [D loss: 0.966122] [G loss: 1.381552] [D real: 0.581504] [D fake: 0.200344]\n",
      "[Epoch 45/50] [Batch 199/938] [D loss: 0.935450] [G loss: 1.320991] [D real: 0.608699] [D fake: 0.231798]\n",
      "[Epoch 45/50] [Batch 299/938] [D loss: 0.813659] [G loss: 1.571732] [D real: 0.727353] [D fake: 0.266502]\n",
      "[Epoch 45/50] [Batch 399/938] [D loss: 0.718597] [G loss: 1.080637] [D real: 0.779487] [D fake: 0.297093]\n",
      "[Epoch 45/50] [Batch 499/938] [D loss: 0.921518] [G loss: 1.760507] [D real: 0.703755] [D fake: 0.322974]\n",
      "[Epoch 45/50] [Batch 599/938] [D loss: 0.978990] [G loss: 1.511472] [D real: 0.735779] [D fake: 0.349592]\n",
      "[Epoch 45/50] [Batch 699/938] [D loss: 0.854372] [G loss: 1.416005] [D real: 0.710032] [D fake: 0.261426]\n",
      "[Epoch 45/50] [Batch 799/938] [D loss: 1.049704] [G loss: 1.348678] [D real: 0.697963] [D fake: 0.353661]\n",
      "[Epoch 45/50] [Batch 899/938] [D loss: 0.967959] [G loss: 1.674886] [D real: 0.735577] [D fake: 0.325175]\n",
      "[Epoch 46/50] [Batch 99/938] [D loss: 1.044619] [G loss: 1.436891] [D real: 0.699202] [D fake: 0.362652]\n",
      "[Epoch 46/50] [Batch 199/938] [D loss: 0.759256] [G loss: 1.710752] [D real: 0.707501] [D fake: 0.218872]\n",
      "[Epoch 46/50] [Batch 299/938] [D loss: 0.978217] [G loss: 1.099588] [D real: 0.635711] [D fake: 0.283857]\n",
      "[Epoch 46/50] [Batch 399/938] [D loss: 0.931288] [G loss: 1.604593] [D real: 0.656958] [D fake: 0.277687]\n",
      "[Epoch 46/50] [Batch 499/938] [D loss: 0.858759] [G loss: 1.981801] [D real: 0.742960] [D fake: 0.310338]\n",
      "[Epoch 46/50] [Batch 599/938] [D loss: 0.881470] [G loss: 1.900164] [D real: 0.684890] [D fake: 0.270968]\n",
      "[Epoch 46/50] [Batch 699/938] [D loss: 0.901061] [G loss: 1.628340] [D real: 0.672664] [D fake: 0.263961]\n",
      "[Epoch 46/50] [Batch 799/938] [D loss: 0.907539] [G loss: 1.414420] [D real: 0.727074] [D fake: 0.339850]\n",
      "[Epoch 46/50] [Batch 899/938] [D loss: 0.983534] [G loss: 1.407087] [D real: 0.693752] [D fake: 0.319408]\n",
      "[Epoch 47/50] [Batch 99/938] [D loss: 0.870979] [G loss: 1.311628] [D real: 0.684726] [D fake: 0.250850]\n",
      "[Epoch 47/50] [Batch 199/938] [D loss: 0.828290] [G loss: 1.929888] [D real: 0.756215] [D fake: 0.317969]\n",
      "[Epoch 47/50] [Batch 299/938] [D loss: 0.966333] [G loss: 1.226602] [D real: 0.621255] [D fake: 0.258782]\n",
      "[Epoch 47/50] [Batch 399/938] [D loss: 0.920277] [G loss: 1.482791] [D real: 0.730925] [D fake: 0.375820]\n",
      "[Epoch 47/50] [Batch 499/938] [D loss: 0.872329] [G loss: 1.257464] [D real: 0.714026] [D fake: 0.286421]\n",
      "[Epoch 47/50] [Batch 599/938] [D loss: 0.839950] [G loss: 1.530711] [D real: 0.625458] [D fake: 0.175307]\n",
      "[Epoch 47/50] [Batch 699/938] [D loss: 0.942193] [G loss: 1.528454] [D real: 0.693336] [D fake: 0.306834]\n",
      "[Epoch 47/50] [Batch 799/938] [D loss: 0.893556] [G loss: 1.830420] [D real: 0.829584] [D fake: 0.391800]\n",
      "[Epoch 47/50] [Batch 899/938] [D loss: 0.957484] [G loss: 1.495402] [D real: 0.728787] [D fake: 0.346187]\n",
      "[Epoch 48/50] [Batch 99/938] [D loss: 0.824575] [G loss: 1.535991] [D real: 0.665763] [D fake: 0.215725]\n",
      "[Epoch 48/50] [Batch 199/938] [D loss: 0.973375] [G loss: 1.656307] [D real: 0.750145] [D fake: 0.386151]\n",
      "[Epoch 48/50] [Batch 299/938] [D loss: 1.005258] [G loss: 1.342110] [D real: 0.700277] [D fake: 0.342951]\n",
      "[Epoch 48/50] [Batch 399/938] [D loss: 0.774730] [G loss: 1.656559] [D real: 0.771512] [D fake: 0.320740]\n",
      "[Epoch 48/50] [Batch 499/938] [D loss: 0.902045] [G loss: 1.208247] [D real: 0.638077] [D fake: 0.236171]\n",
      "[Epoch 48/50] [Batch 599/938] [D loss: 0.768668] [G loss: 1.604373] [D real: 0.738467] [D fake: 0.269237]\n",
      "[Epoch 48/50] [Batch 699/938] [D loss: 0.870512] [G loss: 1.583801] [D real: 0.775391] [D fake: 0.366473]\n",
      "[Epoch 48/50] [Batch 799/938] [D loss: 0.856824] [G loss: 1.526640] [D real: 0.737505] [D fake: 0.309468]\n",
      "[Epoch 48/50] [Batch 899/938] [D loss: 0.905863] [G loss: 1.430739] [D real: 0.670134] [D fake: 0.264759]\n",
      "[Epoch 49/50] [Batch 99/938] [D loss: 0.975186] [G loss: 0.912557] [D real: 0.630838] [D fake: 0.256818]\n",
      "[Epoch 49/50] [Batch 199/938] [D loss: 0.739453] [G loss: 1.520738] [D real: 0.733938] [D fake: 0.262462]\n",
      "[Epoch 49/50] [Batch 299/938] [D loss: 0.869960] [G loss: 1.404409] [D real: 0.616770] [D fake: 0.191670]\n",
      "[Epoch 49/50] [Batch 399/938] [D loss: 0.790598] [G loss: 1.360468] [D real: 0.745866] [D fake: 0.294737]\n",
      "[Epoch 49/50] [Batch 499/938] [D loss: 0.783177] [G loss: 1.363942] [D real: 0.828924] [D fake: 0.383657]\n",
      "[Epoch 49/50] [Batch 599/938] [D loss: 1.065200] [G loss: 1.312142] [D real: 0.539492] [D fake: 0.200302]\n",
      "[Epoch 49/50] [Batch 699/938] [D loss: 0.884822] [G loss: 1.655327] [D real: 0.769176] [D fake: 0.363562]\n",
      "[Epoch 49/50] [Batch 799/938] [D loss: 0.868480] [G loss: 1.644797] [D real: 0.733364] [D fake: 0.323893]\n",
      "[Epoch 49/50] [Batch 899/938] [D loss: 0.945736] [G loss: 1.481939] [D real: 0.721226] [D fake: 0.364675]\n"
     ]
    }
   ],
   "source": [
    "## ----------\n",
    "##  Training\n",
    "## ----------\n",
    "## 进行多个epoch的训练\n",
    "for epoch in range(opt.n_epochs):                               ## epoch:50\n",
    "    for i, (imgs, _) in enumerate(dataloader):                  ## imgs:(64, 1, 28, 28)     _:label(64)\n",
    "        \n",
    "        ## =============================训练判别器==================\n",
    "        ## view(): 相当于numpy中的reshape，重新定义矩阵的形状, 相当于reshape(128，784)  原来是(128, 1, 28, 28)\n",
    "        imgs = imgs.view(imgs.size(0), -1)                             ## 将图片展开为28*28=784  imgs:(64, 784)\n",
    "        real_img = Variable(imgs)                                      ## 将tensor变成Variable放入计算图中，tensor变成variable之后才能进行反向传播求梯度\n",
    "        real_label = Variable(torch.ones(imgs.size(0), 1))             ## 定义真实的图片label为1\n",
    "        fake_label = Variable(torch.zeros(imgs.size(0), 1))            ## 定义假的图片的label为0\n",
    "\n",
    "\n",
    "        ## ---------------------\n",
    "        ##  Train Discriminator\n",
    "        ## 分为两部分：1、真的图像判别为真；2、假的图像判别为假\n",
    "        ## ---------------------\n",
    "        ## 计算真实图片的损失\n",
    "        real_out = discriminator(real_img)                          ## 将真实图片放入判别器中\n",
    "        loss_real_D = criterion(real_out, real_label)               ## 得到真实图片的loss\n",
    "        real_scores = real_out                                      ## 得到真实图片的判别值，输出的值越接近1越好\n",
    "        \n",
    "        \n",
    "        ## 计算假的图片的损失\n",
    "        ## detach(): 从当前计算图中分离下来避免梯度传到G，因为G不用更新\n",
    "        z = Variable(torch.randn(imgs.size(0), opt.latent_dim))             ## 随机生成一些噪声, 大小为(64, 100)\n",
    "        fake_img = generator(z).detach()                                    ## 随机噪声放入生成网络中，生成一张假的图片。 \n",
    "        fake_out = discriminator(fake_img)                                  ## 判别器判断假的图片\n",
    "        loss_fake_D = criterion(fake_out, fake_label)                       ## 得到假的图片的loss\n",
    "        fake_scores = fake_out                                              ## 得到假图片的判别值，对于判别器来说，假图片的损失越接近0越好\n",
    "        \n",
    "        ## 损失函数和优化\n",
    "        loss_D = loss_real_D + loss_fake_D                  ## 损失包括判真损失和判假损失\n",
    "        optimizer_D.zero_grad()                             ## 在反向传播之前，先将梯度归0\n",
    "        loss_D.backward()                                   ## 将误差反向传播\n",
    "        optimizer_D.step()                                  ## 更新参数\n",
    "\n",
    "\n",
    "        ## -----------------\n",
    "        ##  Train Generator\n",
    "        ## 原理：目的是希望生成的假的图片被判别器判断为真的图片，\n",
    "        ## 在此过程中，将判别器固定，将假的图片传入判别器的结果与真实的label对应，\n",
    "        ## 反向传播更新的参数是生成网络里面的参数，\n",
    "        ## 这样可以通过更新生成网络里面的参数，来训练网络，使得生成的图片让判别器以为是真的, 这样就达到了对抗的目的\n",
    "        ## -----------------\n",
    "        z = Variable(torch.randn(imgs.size(0), opt.latent_dim))             ## 得到随机噪声\n",
    "        fake_img = generator(z)                                             ## 随机噪声输入到生成器中，得到一副假的图片\n",
    "        output = discriminator(fake_img)                                    ## 经过判别器得到的结果\n",
    "        ## 损失函数和优化\n",
    "        loss_G = criterion(output, real_label)                              ## 得到的假的图片与真实的图片的label的loss\n",
    "        optimizer_G.zero_grad()                                             ## 梯度归0\n",
    "        loss_G.backward()                                                   ## 进行反向传播\n",
    "        optimizer_G.step()                                                  ## step()一般用在反向传播后面,用于更新生成网络的参数\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ## 打印训练过程中的日志\n",
    "        ## item():取出单元素张量的元素值并返回该值，保持原元素类型不变\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [D real: %f] [D fake: %f]\"\n",
    "                % (epoch, opt.n_epochs, i, len(dataloader), loss_D.item(), loss_G.item(), real_scores.data.mean(), fake_scores.data.mean())\n",
    "            )\n",
    "        \n",
    "        \n",
    "        ## 保存训练过程中的图像\n",
    "        batches_done = epoch * len(dataloader) + i\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "            save_image(fake_img.data[:25], \"./images/gan/%d.png\" % batches_done, nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 保存模型\n",
    "torch.save(generator.state_dict(), './save/gan/generator.pth')\n",
    "torch.save(discriminator.state_dict(), './save/gan/discriminator.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
