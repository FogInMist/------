{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  GIN模型实现\n",
    "*参考资料：*\n",
    "* [源码地址](https://github.com/weihua916/powerful-gnns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入所需python文件和库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from util import load_data, separate_data\n",
    "from models.graphcnn import GraphCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(dataset='MUTAG', device=0, batch_size=32, iters_per_epoch=50, epochs=50, lr=0.01, seed=0, fold_idx=0, num_layers=5, num_mlp_layers=2, hidden_dim=64, final_dropout=0.5, graph_pooling_type='sum', neighbor_pooling_type='sum', learn_eps=False, degree_as_tag=False, filename='output file')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training settings\n",
    "# Note: Hyper-parameters need to be tuned in order to obtain results reported in the paper.\n",
    "parser = argparse.ArgumentParser(description='PyTorch graph convolutional neural net for whole-graph classification')\n",
    "parser.add_argument('--dataset', type=str, default=\"MUTAG\",help='name of dataset (default: MUTAG)')\n",
    "parser.add_argument('--device', type=int, default=0,help='which gpu to use if any (default: 0)')\n",
    "parser.add_argument('--batch_size', type=int, default=32,help='input batch size for training (default: 32)')\n",
    "parser.add_argument('--iters_per_epoch', type=int, default=50,help='number of iterations per each epoch (default: 50)')\n",
    "parser.add_argument('--epochs', type=int, default=50,help='number of epochs to train (default: 350)')\n",
    "parser.add_argument('--lr', type=float, default=0.01,help='learning rate (default: 0.01)')\n",
    "parser.add_argument('--seed', type=int, default=0,help='random seed for splitting the dataset into 10 (default: 0)')\n",
    "parser.add_argument('--fold_idx', type=int, default=0, help='the index of fold in 10-fold validation. Should be less then 10.')\n",
    "parser.add_argument('--num_layers', type=int, default=5, help='number of layers INCLUDING the input one (default: 5)')\n",
    "parser.add_argument('--num_mlp_layers', type=int, default=2, help='number of layers for MLP EXCLUDING the input one (default: 2). 1 means linear model.')\n",
    "parser.add_argument('--hidden_dim', type=int, default=64, help='number of hidden units (default: 64)')\n",
    "parser.add_argument('--final_dropout', type=float, default=0.5, help='final layer dropout (default: 0.5)')\n",
    "parser.add_argument('--graph_pooling_type', type=str, default=\"sum\", choices=[\"sum\", \"average\"],help='Pooling for over nodes in a graph: sum or average')\n",
    "parser.add_argument('--neighbor_pooling_type', type=str, default=\"sum\", choices=[\"sum\", \"average\", \"max\"],help='Pooling for over neighboring nodes: sum, average or max')\n",
    "parser.add_argument('--learn_eps', action=\"store_true\", help='Whether to learn the epsilon weighting for the center nodes. Does not affect training accuracy though.')\n",
    "parser.add_argument('--degree_as_tag', action=\"store_true\", help='let the input node features be the degree of nodes (heuristics for unlabeled graph)')\n",
    "parser.add_argument('--filename', type = str, default = \"output file\",help='output file')\n",
    "\n",
    "# parser.add_argument('--config', type=str, default='./experiments.conf')   # 获取的一些配置文件\n",
    "#args = parser.parse_args()                                               # pychram 中使用\n",
    "args = parser.parse_args(args=[])\n",
    "args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数，交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 固定随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set up seeds and gpu device\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)    \n",
    "device = torch.device(\"cuda:\" + str(args.device)) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "# classes: 2\n",
      "# maximum node tag: 7\n",
      "# data: 188\n"
     ]
    }
   ],
   "source": [
    "graphs, num_classes = load_data(args.dataset, args.degree_as_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Graph with 23 nodes and 27 edges\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2]\n",
      "[[1, 13], [0, 2], [1, 3, 11], [2, 4], [3, 5], [4, 6, 10], [5, 7, 20], [6, 8], [7, 9], [10, 8, 15], [11, 5, 9], [2, 10, 12], [13, 11, 14], [0, 12], [15, 12, 19], [9, 14, 16], [15, 17], [16, 18], [19, 17], [14, 18], [6, 21, 22], [20], [20]]\n",
      "torch.Size([23, 7])\n",
      "torch.Size([2, 54])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(graphs[0].label)\n",
    "print(graphs[0].g)\n",
    "print(graphs[0].node_tags)\n",
    "print(graphs[0].neighbors)\n",
    "print(graphs[0].node_features.shape)\n",
    "print(graphs[0].edge_mat.shape)\n",
    "print(graphs[0].max_neighbor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 13, 2, 3, 11, 4, 5, 6, 10, 7, 20, 8, 9, 15, 12, 14, 19, 16, 17, 18, 21, 22]\n",
      "[(0, 1), (0, 13), (1, 2), (13, 12), (2, 3), (2, 11), (3, 4), (11, 10), (11, 12), (4, 5), (5, 6), (5, 10), (6, 7), (6, 20), (10, 9), (7, 8), (20, 21), (20, 22), (8, 9), (9, 15), (15, 14), (15, 16), (12, 14), (14, 19), (19, 18), (16, 17), (17, 18)]\n"
     ]
    }
   ],
   "source": [
    "print(graphs[0].g.nodes)\n",
    "print(graphs[0].g.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 拆分数据集，训练集和测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##10-fold cross validation. Conduct an experiment on the fold specified by args.fold_idx.\n",
    "train_graphs, test_graphs = separate_data(graphs, args.seed, args.fold_idx)\n",
    "len(train_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_graphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GIN模型中的 maxpooling 操作测试(可忽略)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_deg = max([graph.max_neighbor for graph in test_graphs]) # 计算批次数据中的最大邻居数量\n",
    "max_deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 13, -1], [0, 2, -1], [1, 3, 11], [2, 4, -1], [3, 5, -1], [4, 6, 10], [5, 7, 20], [6, 8, -1], [7, 9, -1], [10, 8, 15], [11, 5, 9], [2, 10, 12], [13, 11, 14], [0, 12, -1], [15, 12, 19], [9, 14, 16], [15, 17, -1], [16, 18, -1], [19, 17, -1], [14, 18, -1], [6, 21, 22], [20, -1, -1], [20, -1, -1], [24, 36, -1], [23, 25, -1], [24, 26, 34], [25, 27, -1], [26, 28, -1], [27, 29, 33], [28, 30, 48], [29, 31, -1], [30, 32, -1], [33, 31, 41], [34, 28, 32], [25, 33, 35], [36, 34, 40], [23, 35, 37], [36, 38, -1], [37, 39, -1], [40, 38, 44], [41, 35, 39], [32, 40, 42], [41, 43, -1], [44, 42, -1], [39, 43, 45], [44, 46, 47], [45, -1, -1], [45, -1, -1], [29, 49, 50], [48, -1, -1], [48, -1, -1], [52, 60, -1], [51, 53, -1], [52, 54, 58], [53, 55, -1], [54, 56, -1], [55, 57, 63], [58, 56, 62], [53, 57, 59], [60, 58, 61], [51, 59, -1], [62, 59, -1], [57, 61, -1], [56, 64, 65], [63, -1, -1], [63, -1, -1], [67, 71, -1], [66, 68, -1], [67, 69, -1], [68, 70, 78], [71, 69, 72], [66, 70, -1], [70, 73, 77], [72, 74, -1], [73, 75, 88], [74, 76, -1], [77, 75, 81], [78, 72, 76], [69, 77, 79], [78, 80, 85], [81, 79, 82], [76, 80, -1], [80, 83, 84], [82, -1, -1], [82, -1, -1], [79, 86, 87], [85, -1, -1], [85, -1, -1], [74, 89, 90], [88, -1, -1], [88, -1, -1], [92, 100, -1], [91, 93, -1], [92, 94, 98], [93, 95, -1], [94, 96, -1], [95, 97, 110], [98, 96, 106], [93, 97, 99], [100, 98, 104], [91, 99, 101], [100, 102, -1], [101, 103, -1], [104, 102, 107], [99, 103, 105], [106, 104, -1], [97, 105, -1], [103, 108, 109], [107, -1, -1], [107, -1, -1], [96, -1, -1], [112, 116, -1], [111, 113, -1], [112, 114, -1], [113, 115, 124], [116, 114, 117], [111, 115, -1], [115, 118, 122], [117, 119, -1], [118, 120, -1], [119, 121, 134], [122, 120, 130], [117, 121, 123], [124, 122, 128], [114, 123, 125], [124, 126, 131], [125, 127, -1], [128, 126, -1], [123, 127, 129], [130, 128, -1], [121, 129, -1], [125, 132, 133], [131, -1, -1], [131, -1, -1], [120, 135, 136], [134, -1, -1], [134, -1, -1], [138, 145, -1], [137, 139, -1], [138, 140, 146], [139, 141, -1], [145, 140, 142], [141, 143, -1], [142, 144, -1], [145, 143, -1], [137, 141, 144], [139, 147, 148], [146, -1, -1], [146, -1, -1], [150, 162, -1], [149, 151, -1], [150, 152, 160], [151, 153, -1], [152, 154, -1], [153, 155, 159], [154, 156, 174], [155, 157, -1], [156, 158, -1], [159, 157, 167], [160, 154, 158], [151, 159, 161], [162, 160, 166], [149, 161, 163], [162, 164, -1], [163, 165, -1], [166, 164, 170], [167, 161, 165], [158, 166, 168], [167, 169, 171], [170, 168, -1], [165, 169, -1], [168, 172, 173], [171, -1, -1], [171, -1, -1], [155, 175, 176], [174, -1, -1], [174, -1, -1], [178, 182, -1], [177, 179, -1], [178, 180, -1], [179, 181, 189], [182, 180, 183], [177, 181, -1], [181, 184, 188], [183, 185, -1], [184, 186, 191], [185, 187, -1], [188, 186, -1], [189, 183, 187], [180, 188, 190], [189, -1, -1], [185, 192, 193], [191, -1, -1], [191, -1, -1], [195, 211, -1], [194, 196, -1], [195, 197, 213], [196, 198, -1], [197, 199, -1], [198, 200, 214], [199, 201, -1], [200, 202, -1], [201, 203, 215], [202, 204, 218], [203, 205, -1], [204, 206, 216], [205, 207, -1], [206, 208, -1], [207, 209, 217], [208, 210, -1], [211, 209, -1], [194, 210, 212], [211, 213, 217], [196, 212, 214], [213, 199, 215], [214, 202, 216], [215, 205, 217], [216, 208, 212], [203, 219, 220], [218, -1, -1], [218, -1, -1], [222, 226, -1], [221, 223, -1], [222, 224, 232], [223, 225, 230], [226, 224, -1], [221, 225, 227], [226, 228, 229], [227, -1, -1], [227, -1, -1], [224, 231, -1], [232, 230, 236], [223, 231, 233], [232, 234, -1], [233, 235, -1], [236, 234, 237], [231, 235, -1], [235, -1, -1], [239, 247, -1], [238, 240, -1], [239, 241, 245], [240, 242, -1], [241, 243, -1], [242, 244, -1], [245, 243, 249], [240, 244, 246], [247, 245, 248], [238, 246, -1], [249, 246, 253], [244, 248, 250], [249, 251, 254], [250, 252, -1], [253, 251, -1], [248, 252, -1], [250, 255, 256], [254, -1, -1], [254, -1, -1], [258, 266, -1], [257, 259, -1], [258, 260, 264], [259, 261, 279], [260, 262, 276], [261, 263, -1], [264, 262, 272], [259, 263, 265], [266, 264, 270], [257, 265, 267], [266, 268, -1], [267, 269, -1], [270, 268, 273], [265, 269, 271], [272, 270, -1], [263, 271, -1], [269, 274, 275], [273, -1, -1], [273, -1, -1], [261, 277, 281], [276, 278, 280], [279, 277, -1], [260, 278, -1], [277, -1, -1], [276, -1, -1], [283, 287, -1], [282, 284, -1], [283, 285, -1], [284, 286, 300], [287, 285, 288], [282, 286, -1], [286, 289, 293], [288, 290, -1], [289, 291, -1], [290, 292, 297], [293, 291, -1], [288, 292, 294], [293, 295, 296], [294, -1, -1], [294, -1, -1], [291, 298, 299], [297, -1, -1], [297, -1, -1], [285, 301, 302], [300, -1, -1], [300, -1, -1], [304, 308, -1], [303, 305, -1], [304, 306, 313], [305, 307, 312], [308, 306, -1], [303, 307, 309], [308, 310, 311], [309, -1, -1], [309, -1, -1], [306, -1, -1], [305, -1, -1], [315, 326, -1], [314, 316, -1], [315, 317, -1], [316, 318, 325], [317, 319, -1], [318, 320, 324], [319, 321, -1], [320, 322, -1], [321, 323, 327], [324, 322, -1], [325, 319, 323], [326, 317, 324], [314, 325, -1], [322, 328, 329], [327, -1, -1], [327, -1, -1], [331, -1, -1], [330, 332, 339], [331, 333, -1], [332, 334, 338], [333, 335, -1], [334, 336, 340], [335, 337, -1], [338, 336, -1], [339, 333, 337], [331, 338, -1], [335, 341, 342], [340, -1, -1], [340, -1, -1], [344, 348, -1], [343, 345, -1], [344, 346, -1], [345, 347, 358], [348, 346, 349], [343, 347, -1], [347, 350, 354], [349, 351, -1], [350, 352, -1], [351, 353, -1], [354, 352, 355], [349, 353, -1], [353, 356, 357], [355, -1, -1], [355, -1, -1], [346, -1, -1], [360, -1, -1], [359, 361, 371], [360, 362, 366], [361, 363, 370], [362, 364, -1], [363, 365, -1], [366, 364, -1], [361, 365, 367], [366, 368, 369], [367, -1, -1], [367, -1, -1], [371, 362, -1], [360, 370, -1]]\n"
     ]
    }
   ],
   "source": [
    "#create a list of padded neighbor lists\n",
    "padded_neighbor_list = [] # 用于存储填充后的邻居列表。\n",
    "start_idx = [0]  # 用于存储每个图的起始索引。\n",
    "flag = 0\n",
    "#iterate through the graphs in the current minibatch\n",
    "for i, graph in enumerate(test_graphs):\n",
    "    #increment the start index of the current graph\n",
    "    # print(start_idx[i],len(graph.g))\n",
    "    start_idx.append(start_idx[i] + len(graph.g))\n",
    "    # print(start_idx)\n",
    "    #create a list of padded neighbors\n",
    "    padded_neighbors = []\n",
    "    #iterate through the neighbors of the current graph\n",
    "    for j in range(len(graph.neighbors)):\n",
    "        #add off-set values to the neighbor indices\n",
    "        pad = [n + start_idx[i] for n in graph.neighbors[j]]\n",
    "        #padding, dummy data is assumed to be stored in -1\n",
    "        pad.extend([-1]*(max_deg - len(pad)))\n",
    "        # print(pad)\n",
    "        #append the padded neighbor to the list of padded neighbors\n",
    "        padded_neighbors.append(pad)\n",
    "        # print(padded_neighbors)\n",
    "    #append the list of padded neighbors to the list of padded neighbor lists\n",
    "    padded_neighbor_list.extend(padded_neighbors)\n",
    "    # print(padded_neighbor_list)\n",
    "    flag += 1\n",
    "    # if flag == 2:\n",
    "    #     break\n",
    "print(padded_neighbor_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([372, 7])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_concat = torch.cat([graph.node_features for graph in test_graphs], 0)\n",
    "X_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7])\n",
      "torch.Size([373, 7])\n",
      "tensor([1., 0., 0., 0., 0., 0., 0.])\n",
      "torch.Size([372, 3, 7])\n",
      "tensor([[1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]])\n",
      "torch.Size([372, 7])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 1., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "h = X_concat\n",
    "\n",
    "dummy = torch.min(h, dim = 0)[0]\n",
    "print(dummy.shape)\n",
    "\n",
    "h_with_dummy = torch.cat([h, dummy.reshape((1, -1))])\n",
    "print(h_with_dummy.shape)\n",
    "print(h_with_dummy[2])\n",
    "\n",
    "print(h_with_dummy[padded_neighbor_list].shape)\n",
    "print(h_with_dummy[padded_neighbor_list][2])\n",
    "\n",
    "pooled_rep = torch.max(h_with_dummy[padded_neighbor_list], dim = 1)[0]\n",
    "print(pooled_rep.shape)\n",
    "print(pooled_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GIN模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphCNN(args.num_layers, args.num_mlp_layers, train_graphs[0].node_features.shape[1], args.hidden_dim, num_classes, args.final_dropout, args.learn_eps, args.graph_pooling_type, args.neighbor_pooling_type, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_graphs, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    total_iters = args.iters_per_epoch\n",
    "    pbar = tqdm(range(total_iters), unit='batch')\n",
    "\n",
    "    loss_accum = 0\n",
    "    for pos in pbar:\n",
    "        # 从训练数据中随机选择一个子集，用于训练神经网络\n",
    "        # 首先使用np.random.permutation函数对训练数据索引进行随机排序\n",
    "        # 然后使用[:args.batch_size]选择前batch_size个随机索引作为子集\n",
    "        selected_idx = np.random.permutation(len(train_graphs))[:args.batch_size]\n",
    "\n",
    "        batch_graph = [train_graphs[idx] for idx in selected_idx]\n",
    "        output = model(batch_graph)\n",
    "\n",
    "        labels = torch.LongTensor([graph.label for graph in batch_graph]).to(device)\n",
    "\n",
    "        #compute loss\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        #backprop\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()         \n",
    "            optimizer.step()\n",
    "        \n",
    "\n",
    "        loss = loss.detach().cpu().numpy()\n",
    "        loss_accum += loss\n",
    "\n",
    "        #report\n",
    "        pbar.set_description('epoch: %d' % (epoch))\n",
    "\n",
    "    average_loss = loss_accum/total_iters\n",
    "    print(\"loss training: %f\" % (average_loss))\n",
    "    \n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, train_graphs, optimizer, epoch):\n",
    "    model.train()\n",
    "\n",
    "    total_iters = args.iters_per_epoch\n",
    "    pbar = tqdm(range(total_iters), unit='batch')\n",
    "\n",
    "    loss_accum = 0\n",
    "    for pos in pbar:\n",
    "        # \n",
    "        selected_idx = np.random.permutation(len(train_graphs))[:args.batch_size]\n",
    "\n",
    "        batch_graph = [train_graphs[idx] for idx in selected_idx]\n",
    "        output = model(batch_graph)\n",
    "\n",
    "        labels = torch.LongTensor([graph.label for graph in batch_graph]).to(device)\n",
    "\n",
    "        #compute loss\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        #backprop\n",
    "        if optimizer is not None:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()         \n",
    "            optimizer.step()\n",
    "        \n",
    "\n",
    "        loss = loss.detach().cpu().numpy()\n",
    "        loss_accum += loss\n",
    "\n",
    "        #report\n",
    "        pbar.set_description('epoch: %d' % (epoch))\n",
    "\n",
    "    average_loss = loss_accum/total_iters\n",
    "    print(\"loss training: %f\" % (average_loss))\n",
    "    \n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "###pass data to model with minibatch during testing to avoid memory overflow (does not perform backpropagation)\n",
    "def pass_data_iteratively(model, graphs, minibatch_size = 64):\n",
    "    model.eval()\n",
    "    output = []\n",
    "    idx = np.arange(len(graphs))\n",
    "    for i in range(0, len(graphs), minibatch_size):\n",
    "        sampled_idx = idx[i:i+minibatch_size]\n",
    "        if len(sampled_idx) == 0:\n",
    "            continue\n",
    "        output.append(model([graphs[j] for j in sampled_idx]).detach())\n",
    "    return torch.cat(output, 0)\n",
    "\n",
    "def test(args, model, device, train_graphs, test_graphs, epoch):\n",
    "    model.eval()\n",
    "\n",
    "    output = pass_data_iteratively(model, train_graphs)\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    labels = torch.LongTensor([graph.label for graph in train_graphs]).to(device)\n",
    "    correct = pred.eq(labels.view_as(pred)).sum().cpu().item()\n",
    "    acc_train = correct / float(len(train_graphs))\n",
    "\n",
    "    output = pass_data_iteratively(model, test_graphs)\n",
    "    pred = output.max(1, keepdim=True)[1]\n",
    "    labels = torch.LongTensor([graph.label for graph in test_graphs]).to(device)\n",
    "    correct = pred.eq(labels.view_as(pred)).sum().cpu().item()\n",
    "    acc_test = correct / float(len(test_graphs))\n",
    "\n",
    "    print(\"accuracy train: %f test: %f\" % (acc_train, acc_test))\n",
    "\n",
    "    return acc_train, acc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1: 100%|██████████| 50/50 [00:00<00:00, 76.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 2.305406\n",
      "accuracy train: 0.852071 test: 0.789474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 2: 100%|██████████| 50/50 [00:00<00:00, 82.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 1.228823\n",
      "accuracy train: 0.810651 test: 0.894737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 3: 100%|██████████| 50/50 [00:00<00:00, 76.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.847793\n",
      "accuracy train: 0.840237 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 4: 100%|██████████| 50/50 [00:00<00:00, 80.47batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.548788\n",
      "accuracy train: 0.923077 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 5: 100%|██████████| 50/50 [00:00<00:00, 83.00batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.478933\n",
      "accuracy train: 0.905325 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 6: 100%|██████████| 50/50 [00:00<00:00, 78.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.587393\n",
      "accuracy train: 0.893491 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 7: 100%|██████████| 50/50 [00:00<00:00, 79.45batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.484600\n",
      "accuracy train: 0.840237 test: 0.736842\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 8: 100%|██████████| 50/50 [00:00<00:00, 79.33batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.531488\n",
      "accuracy train: 0.923077 test: 0.894737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 9: 100%|██████████| 50/50 [00:00<00:00, 78.83batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.318316\n",
      "accuracy train: 0.940828 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 10: 100%|██████████| 50/50 [00:00<00:00, 72.30batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.203298\n",
      "accuracy train: 0.952663 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 11: 100%|██████████| 50/50 [00:00<00:00, 65.88batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.284641\n",
      "accuracy train: 0.917160 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 12: 100%|██████████| 50/50 [00:00<00:00, 80.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.278371\n",
      "accuracy train: 0.869822 test: 1.000000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 13: 100%|██████████| 50/50 [00:00<00:00, 81.92batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.274377\n",
      "accuracy train: 0.923077 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 14: 100%|██████████| 50/50 [00:00<00:00, 81.25batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.269141\n",
      "accuracy train: 0.964497 test: 0.894737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 15: 100%|██████████| 50/50 [00:00<00:00, 80.86batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.302259\n",
      "accuracy train: 0.946746 test: 0.789474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 16: 100%|██████████| 50/50 [00:00<00:00, 80.09batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.234965\n",
      "accuracy train: 0.946746 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 17: 100%|██████████| 50/50 [00:00<00:00, 79.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.256096\n",
      "accuracy train: 0.905325 test: 0.894737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 18: 100%|██████████| 50/50 [00:00<00:00, 79.96batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.184259\n",
      "accuracy train: 0.964497 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 19: 100%|██████████| 50/50 [00:00<00:00, 81.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.146424\n",
      "accuracy train: 0.952663 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 20: 100%|██████████| 50/50 [00:00<00:00, 78.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.137270\n",
      "accuracy train: 0.964497 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 21: 100%|██████████| 50/50 [00:00<00:00, 80.60batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.222598\n",
      "accuracy train: 0.917160 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 22: 100%|██████████| 50/50 [00:00<00:00, 80.99batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.143980\n",
      "accuracy train: 0.976331 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 23: 100%|██████████| 50/50 [00:00<00:00, 77.57batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.159584\n",
      "accuracy train: 0.964497 test: 0.894737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 24: 100%|██████████| 50/50 [00:00<00:00, 80.04batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.129295\n",
      "accuracy train: 0.964497 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 25: 100%|██████████| 50/50 [00:00<00:00, 79.45batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.197274\n",
      "accuracy train: 0.982249 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 26: 100%|██████████| 50/50 [00:00<00:00, 81.78batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.165769\n",
      "accuracy train: 0.964497 test: 0.894737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 27: 100%|██████████| 50/50 [00:00<00:00, 80.73batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.128073\n",
      "accuracy train: 0.988166 test: 0.894737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 28: 100%|██████████| 50/50 [00:00<00:00, 80.86batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.124632\n",
      "accuracy train: 0.934911 test: 0.894737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 29: 100%|██████████| 50/50 [00:00<00:00, 80.73batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.170659\n",
      "accuracy train: 0.911243 test: 0.789474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 30: 100%|██████████| 50/50 [00:00<00:00, 83.56batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.133205\n",
      "accuracy train: 0.970414 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 31: 100%|██████████| 50/50 [00:00<00:00, 87.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.141866\n",
      "accuracy train: 0.964497 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 32: 100%|██████████| 50/50 [00:00<00:00, 82.19batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.136684\n",
      "accuracy train: 0.923077 test: 0.789474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 33: 100%|██████████| 50/50 [00:00<00:00, 83.85batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.185614\n",
      "accuracy train: 0.852071 test: 0.894737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 34: 100%|██████████| 50/50 [00:00<00:00, 86.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.127170\n",
      "accuracy train: 0.964497 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 35: 100%|██████████| 50/50 [00:00<00:00, 88.58batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.185674\n",
      "accuracy train: 0.946746 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 36: 100%|██████████| 50/50 [00:00<00:00, 83.42batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.117828\n",
      "accuracy train: 0.976331 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 37: 100%|██████████| 50/50 [00:00<00:00, 74.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.101013\n",
      "accuracy train: 0.982249 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 38: 100%|██████████| 50/50 [00:00<00:00, 83.00batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.085040\n",
      "accuracy train: 0.964497 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 39: 100%|██████████| 50/50 [00:00<00:00, 84.83batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.105642\n",
      "accuracy train: 1.000000 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 40: 100%|██████████| 50/50 [00:00<00:00, 83.98batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.080059\n",
      "accuracy train: 0.964497 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 41: 100%|██████████| 50/50 [00:00<00:00, 83.84batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.103368\n",
      "accuracy train: 0.692308 test: 0.684211\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 42: 100%|██████████| 50/50 [00:00<00:00, 84.69batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.361313\n",
      "accuracy train: 0.928994 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 43: 100%|██████████| 50/50 [00:00<00:00, 81.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.278004\n",
      "accuracy train: 0.964497 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 44: 100%|██████████| 50/50 [00:00<00:00, 86.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.155826\n",
      "accuracy train: 0.964497 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 45: 100%|██████████| 50/50 [00:00<00:00, 84.83batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.143157\n",
      "accuracy train: 0.982249 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 46: 100%|██████████| 50/50 [00:00<00:00, 85.55batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.159131\n",
      "accuracy train: 0.976331 test: 0.894737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 47: 100%|██████████| 50/50 [00:00<00:00, 84.12batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.082433\n",
      "accuracy train: 0.976331 test: 0.894737\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 48: 100%|██████████| 50/50 [00:00<00:00, 87.04batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.117535\n",
      "accuracy train: 0.952663 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 49: 100%|██████████| 50/50 [00:00<00:00, 85.70batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.170909\n",
      "accuracy train: 0.875740 test: 0.842105\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 50: 100%|██████████| 50/50 [00:00<00:00, 84.54batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss training: 0.103308\n",
      "accuracy train: 0.982249 test: 0.842105\n",
      "\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0.], requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    scheduler.step()\n",
    "\n",
    "    avg_loss = train(args, model, device, train_graphs, optimizer, epoch)\n",
    "    acc_train, acc_test = test(args, model, device, train_graphs, test_graphs, epoch)\n",
    "\n",
    "    if not args.filename == \"\":\n",
    "        with open(args.filename, 'w') as f:\n",
    "            f.write(\"%f %f %f\" % (avg_loss, acc_train, acc_test))\n",
    "            f.write(\"\\n\")\n",
    "    print(\"\")\n",
    "\n",
    "print(model.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
