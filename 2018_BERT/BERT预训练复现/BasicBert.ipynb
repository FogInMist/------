{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**参考资料：**\n",
    "* [BertWithPretrained](https://github.com/moon-hotel/BertWithPretrained)\n",
    "\n",
    "**模型架构：**\n",
    "\n",
    "![Bert_model](Model.jpg \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一、全局参数管理对象config的创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import copy\n",
    "import six\n",
    "import logging\n",
    "\n",
    "class BertConfig(object):\n",
    "    \"\"\"Configuration for `BertModel`.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size=21128, #词汇字典大小 \n",
    "                 hidden_size=768, #隐藏层大小\n",
    "                 num_hidden_layers=12, # 自注意力块的个数\n",
    "                 num_attention_heads=12,  # 每个自注意力块的多头个数\n",
    "                 intermediate_size=3072,  #中间层大小 \n",
    "                 pad_token_id=0, # 填充字符\n",
    "                 hidden_act=\"gelu\", #激活函数类型\n",
    "                 hidden_dropout_prob=0.1, # dropout率\n",
    "                 attention_probs_dropout_prob=0.1, # dropout率\n",
    "                 max_position_embeddings=512, # 单句最大长度\n",
    "                 type_vocab_size=2, # 句子顺序，1或2\n",
    "                 initializer_range=0.02):\n",
    "        \"\"\"Constructs BertConfig.\n",
    "        Args:\n",
    "          vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n",
    "          hidden_size: Size of the encoder layers and the pooler layer.\n",
    "          num_hidden_layers: Number of hidden layers in the Transformer encoder.\n",
    "          num_attention_heads: Number of attention heads for each attention layer in\n",
    "            the Transformer encoder.\n",
    "          intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n",
    "            layer in the Transformer encoder.\n",
    "          hidden_act: The non-linear activation function (function or string) in the\n",
    "            encoder and pooler.\n",
    "          hidden_dropout_prob: The dropout probability for all fully connected\n",
    "            layers in the embeddings, encoder, and pooler.\n",
    "          attention_probs_dropout_prob: The dropout ratio for the attention\n",
    "            probabilities.\n",
    "          max_position_embeddings: The maximum sequence length that this model might\n",
    "            ever be used with. Typically set this to something large just in case\n",
    "            (e.g., 512 or 1024 or 2048).\n",
    "          type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n",
    "            `BertModel`.\n",
    "          initializer_range: The stdev of the truncated_normal_initializer for\n",
    "            initializing all weight matrices.\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.pad_token_id = pad_token_id\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob\n",
    "        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.type_vocab_size = type_vocab_size\n",
    "        self.initializer_range = initializer_range\n",
    "\n",
    "    @classmethod\n",
    "    def from_dict(cls, json_object): # 从json文件中读取参数配置\n",
    "        \"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"\n",
    "        config = BertConfig(vocab_size=None)\n",
    "        for (key, value) in six.iteritems(json_object):\n",
    "            config.__dict__[key] = value\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_json_file(cls, json_file):\n",
    "        \"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"\n",
    "        \"\"\"从json配置文件读取配置信息\"\"\"\n",
    "        with open(json_file, 'r') as reader:\n",
    "            text = reader.read()\n",
    "        logging.info(f\"成功导入BERT配置文件 {json_file}\")\n",
    "        return cls.from_dict(json.loads(text))\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        return output\n",
    "\n",
    "    def to_json_string(self):\n",
    "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
    "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size = 21128\n",
      "hidden_size = 768\n",
      "num_hidden_layers = 12\n",
      "num_attention_heads = 12\n",
      "hidden_act = gelu\n",
      "intermediate_size = 3072\n",
      "pad_token_id = 0\n",
      "hidden_dropout_prob = 0.1\n",
      "attention_probs_dropout_prob = 0.1\n",
      "max_position_embeddings = 512\n",
      "type_vocab_size = 2\n",
      "initializer_range = 0.02\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig()\n",
    "for i in config.__dict__: # 查看全局变量设置\n",
    "    print(i, '=', config.__dict__[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二、输入编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**模型架构：**\n",
    "\n",
    "![Bert_model](Input.jpg \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.nn.init import normal_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词嵌入 编码\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, pad_token_id=0, initializer_range=0.02):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=pad_token_id) # 直接调用 21128 ——> 768\n",
    "        self._reset_parameters(initializer_range)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        :param input_ids: shape : [input_ids_len, batch_size]\n",
    "        :return: shape: [input_ids_len, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        return self.embedding(input_ids)  # 512x64 ——> 512x64x768\n",
    "\n",
    "    def _reset_parameters(self, initializer_range):\n",
    "        r\"\"\"Initiate parameters.\"\"\"\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                normal_(p, mean=0.0, std=initializer_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单词位置 编码\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    位置编码。\n",
    "      *** 注意： Bert中的位置编码完全不同于Transformer中的位置编码，\n",
    "                前者本质上也是一个普通的Embedding层，而后者是通过公式计算得到，\n",
    "                而这也是为什么Bert只能接受长度为512字符的原因，因为位置编码的最大size为512 ***\n",
    "      # Since the position embedding table is a learned variable, we create it\n",
    "      # using a (long) sequence length `max_position_embeddings`. The actual\n",
    "      # sequence length might be shorter than this, for faster training of\n",
    "      # tasks that do not have long sequences.\n",
    "                                                 ————————  GoogleResearch\n",
    "    https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/modeling.py\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, max_position_embeddings=512, initializer_range=0.02):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # 因为BERT预训练模型的长度为512\n",
    "        self.embedding = nn.Embedding(max_position_embeddings, hidden_size) # 512x768\n",
    "        self._reset_parameters(initializer_range)\n",
    "\n",
    "    def forward(self, position_ids):\n",
    "        \"\"\"\n",
    "        :param position_ids: [1,position_ids_len]\n",
    "        :return: [position_ids_len, 1, hidden_size]\n",
    "        \"\"\"\n",
    "        return self.embedding(position_ids).transpose(0, 1)  # 1x512 ——> 512x1x768\n",
    "\n",
    "    def _reset_parameters(self, initializer_range):\n",
    "        r\"\"\"Initiate parameters.\"\"\"\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                normal_(p, mean=0.0, std=initializer_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句子编码\n",
    "class SegmentEmbedding(nn.Module):\n",
    "    def __init__(self, type_vocab_size, hidden_size, initializer_range=0.02):\n",
    "        super(SegmentEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(type_vocab_size, hidden_size) # 2x768\n",
    "        self._reset_parameters(initializer_range)\n",
    "\n",
    "    def forward(self, token_type_ids):\n",
    "        \"\"\"\n",
    "\n",
    "        :param token_type_ids:  shape: [token_type_ids_len, batch_size]\n",
    "        :return: shape: [token_type_ids_len, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        return self.embedding(token_type_ids) # 512x64 ——> 512x64x768\n",
    "\n",
    "    def _reset_parameters(self, initializer_range):\n",
    "        r\"\"\"Initiate parameters.\"\"\"\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                normal_(p, mean=0.0, std=initializer_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Embedding which is consisted with under features\n",
    "        1. TokenEmbedding : normal embedding matrix\n",
    "        2. PositionalEmbedding : normal embedding matrix\n",
    "        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n",
    "        sum of all these features are output of BERTEmbedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = TokenEmbedding(vocab_size=config.vocab_size,\n",
    "                                              hidden_size=config.hidden_size,\n",
    "                                              pad_token_id=config.pad_token_id,\n",
    "                                              initializer_range=config.initializer_range)\n",
    "        # return shape [src_len,batch_size,hidden_size]\n",
    "\n",
    "        self.position_embeddings = PositionalEmbedding(max_position_embeddings=config.max_position_embeddings,\n",
    "                                                       hidden_size=config.hidden_size,\n",
    "                                                       initializer_range=config.initializer_range)\n",
    "        # return shape [src_len,1,hidden_size]\n",
    "\n",
    "        self.token_type_embeddings = SegmentEmbedding(type_vocab_size=config.type_vocab_size,\n",
    "                                                      hidden_size=config.hidden_size,\n",
    "                                                      initializer_range=config.initializer_range)\n",
    "        # return shape  [src_len,batch_size,hidden_size]\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.register_buffer(\"position_ids\",\n",
    "                             torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        # shape: [1, max_position_embeddings]\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                position_ids=None,\n",
    "                token_type_ids=None):\n",
    "        \"\"\"\n",
    "        :param input_ids:  输入序列的原始token id, shape: [src_len, batch_size]\n",
    "        :param position_ids: 位置序列，本质就是 [0,1,2,3,...,src_len-1], shape: [1,src_len]\n",
    "        :param token_type_ids: 句子分隔token, 例如[0,0,0,0,1,1,1,1]用于区分两个句子 shape:[src_len,batch_size]\n",
    "        :return: [src_len, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        src_len = input_ids.size(0)\n",
    "        token_embedding = self.word_embeddings(input_ids)\n",
    "        # shape:[src_len,batch_size,hidden_size]\n",
    "\n",
    "        if position_ids is None:  # 在实际建模时这个参数其实可以不用传值\n",
    "            position_ids = self.position_ids[:, :src_len]  # [1,src_len]\n",
    "        positional_embedding = self.position_embeddings(position_ids)\n",
    "        # [src_len, 1, hidden_size]\n",
    "\n",
    "        if token_type_ids is None:  # 如果输入模型的只有一个序列，那么这个参数也不用传值\n",
    "            token_type_ids = torch.zeros_like(input_ids,\n",
    "                                              device=self.position_ids.device)  # [src_len, batch_size]\n",
    "            \n",
    "        segment_embedding = self.token_type_embeddings(token_type_ids)\n",
    "        # [src_len,batch_size,hidden_size]\n",
    "\n",
    "        embeddings = token_embedding + positional_embedding + segment_embedding\n",
    "        # [src_len,batch_size,hidden_size] + [src_len,1,hidden_size] + [src_len,batch_size,hidden_size]\n",
    "        embeddings = self.LayerNorm(embeddings)  # [src_len, batch_size, hidden_size]\n",
    "        embeddings = self.dropout(embeddings)\n",
    "\n",
    "        \n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三、训练数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    \"\"\"\n",
    "    根据本地的vocab文件，构造一个词表\n",
    "    vocab = Vocab()\n",
    "    print(vocab.itos)  # 得到一个列表，返回词表中的每一个词；\n",
    "    print(vocab.itos[2])  # 通过索引返回得到词表中对应的词；\n",
    "    print(vocab.stoi)  # 得到一个字典，返回词表中每个词的索引；\n",
    "    print(vocab.stoi['我'])  # 通过单词返回得到词表中对应的索引\n",
    "    print(len(vocab))  # 返回词表长度\n",
    "    \"\"\"\n",
    "    UNK = '[UNK]'\n",
    "\n",
    "    def __init__(self, vocab_path):\n",
    "        self.stoi = {}\n",
    "        self.itos = []\n",
    "        with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "            for i, word in enumerate(f):\n",
    "                w = word.strip('\\n')\n",
    "                self.stoi[w] = i\n",
    "                self.itos.append(w)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return self.stoi.get(token, self.stoi.get(Vocab.UNK))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2769\n",
      "[PAD]\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(vocab_path):\n",
    "    \"\"\"\n",
    "    vocab = Vocab()\n",
    "    print(vocab.itos)  # 得到一个列表，返回词表中的每一个词；\n",
    "    print(vocab.itos[2])  # 通过索引返回得到词表中对应的词；\n",
    "    print(vocab.stoi)  # 得到一个字典，返回词表中每个词的索引；\n",
    "    print(vocab.stoi['我'])  # 通过单词返回得到词表中对应的索引\n",
    "    \"\"\"\n",
    "    return Vocab(vocab_path)\n",
    "\n",
    "vocab = build_vocab('./data/vocab.txt')\n",
    "print(vocab.stoi['我'])\n",
    "print(vocab.itos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['十年生死两茫茫', '不思量，自难忘', '千里孤坟，无处话凄凉', '纵使相逢应不识，尘满面，鬓如霜', '夜来幽梦忽还乡，小轩窗，正梳妆', '相顾无言，惟有泪千行', '料得年年断肠处，明月夜，短松冈']\n",
      "['红酥手，黄縢酒，满城春色宫墙柳', '东风恶，欢情薄', '一怀愁绪，几年离索', '春如旧，人空瘦，泪痕红鲛绡透', '桃花落，闲池阁', '山盟虽在，锦书难托']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "text = ['红酥手，黄縢酒，满城春色宫墙柳。东风恶，欢情薄。一怀愁绪，几年离索。春如旧，人空瘦，泪痕红鲛绡透。桃花落，闲池阁。山盟虽在，锦书难托。',\n",
    "        '十年生死两茫茫。不思量，自难忘。千里孤坟，无处话凄凉。纵使相逢应不识，尘满面，鬓如霜。夜来幽梦忽还乡，小轩窗，正梳妆。相顾无言，惟有泪千行。料得年年断肠处，明月夜，短松冈。']\n",
    "\n",
    "paragraphs = []\n",
    "\n",
    "for line in text:\n",
    "    paragraphs.append([line[0]])\n",
    "    line = line.strip() # 去掉换行符和两边的空格\n",
    "    for w in line[1:]:\n",
    "        if paragraphs[-1][-1][-1] in '。':\n",
    "            paragraphs[-1][-1] = paragraphs[-1][-1][:-1]\n",
    "            paragraphs[-1].append(w)\n",
    "        else:\n",
    "            paragraphs[-1][-1] += w\n",
    "    paragraphs[-1][-1] = paragraphs[-1][-1][:-1]\n",
    "     \n",
    "random.shuffle(paragraphs) # 将所有段落打乱\n",
    "\n",
    "for i in paragraphs:\n",
    "    print(i)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NSP任务\n",
    "def get_next_sentence_sample(sentence, next_sentence, paragraphs):\n",
    "    if random.random() < 0.5: # 产生[0,1)之间的一个随机数\n",
    "        is_next = True\n",
    "    else:\n",
    "        new_next_sentence = next_sentence\n",
    "        while next_sentence == new_next_sentence:\n",
    "            new_next_sentence = random.choice(random.choice(paragraphs))\n",
    "        next_sentence = new_next_sentence\n",
    "        is_next = False\n",
    "    return sentence, next_sentence, is_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ## 当前句文本：红酥手，黄縢酒，满城春色宫墙柳\n",
      " ## 下一句文本：东风恶，欢情薄\n",
      " ## 下一句标签：True\n",
      "[101, 5273, 6989, 2797, 8024, 7942, 100, 6983, 8024, 4007, 1814, 3217, 5682, 2151, 1870, 3394, 102, 691, 7599, 2626, 8024, 3614, 2658, 5946, 102]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "5273\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "model_name = 'bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name).tokenize\n",
    "\n",
    "for paragraph in paragraphs:\n",
    "    for i in range(len(paragraph) - 1):\n",
    "        sentence, next_sentence, is_next = get_next_sentence_sample(paragraph[i], paragraph[i + 1], paragraphs) # 构造 NSP 样本\n",
    "        token_a_ids = [vocab[token] for token in tokenizer(sentence)]\n",
    "        token_b_ids = [vocab[token] for token in tokenizer(next_sentence)]\n",
    "        token_ids = [vocab['[CLS]']] + token_a_ids + [vocab['[SEP]']] + token_b_ids\n",
    "        token_ids += [vocab['[SEP]']]\n",
    "        seg1 = [0] * (len(token_a_ids) + 2)  # 2 表示[CLS]和中间的[SEP]这两个字符\n",
    "        seg2 = [1] * (len(token_b_ids) + 1)\n",
    "        segs = seg1 + seg2\n",
    "        break\n",
    "\n",
    "print(f\" ## 当前句文本：{sentence}\")\n",
    "print(f\" ## 下一句文本：{next_sentence}\")\n",
    "print(f\" ## 下一句标签：{is_next}\")\n",
    "print(token_ids)\n",
    "print(segs)\n",
    "print(vocab.stoi['红'])\n",
    "print(vocab.stoi['[CLS]'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM任务数据处理\n",
    "def replace_masked_tokens(token_ids, candidate_pred_positions, num_mlm_preds):\n",
    "    \"\"\"\n",
    "    本函数的作用是根据给定的token_ids、候选mask位置以及需要mask的数量来返回被mask后的token_ids以及标签信息\n",
    "    :param token_ids:\n",
    "    :param candidate_pred_positions:\n",
    "    :param num_mlm_preds:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    pred_positions = []\n",
    "    mlm_input_tokens_id = [token_id for token_id in token_ids]\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions) >= num_mlm_preds:\n",
    "            break  # 如果已经mask的数量大于等于num_mlm_preds则停止mask\n",
    "        masked_token_id = None\n",
    "        # 80%的时间：将词替换为['MASK']词元，但这里是直接替换为['MASK']对应的id\n",
    "        if random.random() < 0.8:  # 0.8\n",
    "            masked_token_id = vocab['[MASK]']\n",
    "        else:\n",
    "            # 10%的时间：保持词不变\n",
    "            if random.random() < 0.5:  # 0.5\n",
    "                masked_token_id = token_ids[mlm_pred_position]\n",
    "            # 10%的时间：用随机词替换该词\n",
    "            else:\n",
    "                masked_token_id = random.randint(0, len(vocab.stoi) - 1)\n",
    "        mlm_input_tokens_id[mlm_pred_position] = masked_token_id\n",
    "        pred_positions.append(mlm_pred_position)  # 保留被mask位置的索引信息\n",
    "    # 构造mlm任务中需要预测位置对应的正确标签，如果其没出现在pred_positions则表示该位置不是mask位置\n",
    "    # 则在进行损失计算时需要忽略掉这些位置（即为PAD_IDX）；而如果其出现在mask的位置，则其标签为原始token_ids对应的id\n",
    "    mlm_label = [0 if idx not in pred_positions\n",
    "                    else token_ids[idx] for idx in range(len(token_ids))]\n",
    "    return mlm_input_tokens_id, mlm_label\n",
    "\n",
    "def get_masked_sample(token_ids):\n",
    "    \"\"\"\n",
    "    本函数的作用是将传入的 一段token_ids的其中部分进行mask处理\n",
    "    :param token_ids:         e.g. [101, 1031, 4895, 2243, 1033, 10029, 2000, 2624, 1031,....]\n",
    "    :return: mlm_input_tokens_id:  [101, 1031, 103, 2243, 1033, 10029, 2000, 103,  1031, ...]\n",
    "                        mlm_label:  [ 0,   0,   4895,  0,    0,    0,    0,   2624,  0,...]\n",
    "    \"\"\"\n",
    "    candidate_pred_positions = []  # 候选预测位置的索引\n",
    "    for i, ids in enumerate(token_ids):\n",
    "        # 在遮蔽语言模型任务中不会预测特殊词元，所以如果该位置是特殊词元\n",
    "        # 那么该位置就不会成为候选mask位置\n",
    "        if ids in [vocab['[CLS]'], vocab['[SEP]']]:\n",
    "            continue\n",
    "        candidate_pred_positions.append(i)\n",
    "        # 保存候选位置的索引， 例如可能是 [ 2,3,4,5, ....]\n",
    "    random.shuffle(candidate_pred_positions)  # 将所有候选位置打乱，更利于后续随机\n",
    "    # 被掩盖位置的数量，BERT模型中默认将15%的Token进行mask\n",
    "    num_mlm_preds = max(1, round(len(token_ids) * 0.15))\n",
    "    # print(f\" ## Mask数量为: {num_mlm_preds}\")\n",
    "    mlm_input_tokens_id, mlm_label = replace_masked_tokens(\n",
    "        token_ids, candidate_pred_positions, num_mlm_preds)\n",
    "    return mlm_input_tokens_id, mlm_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ## Mask数量为: 4\n",
      "103\n",
      "[101, 5273, 103, 2797, 8024, 7942, 103, 6983, 8024, 4007, 103, 3217, 5682, 2151, 1870, 3394, 102, 691, 7599, 2626, 8024, 103, 2658, 5946, 102]\n",
      "[0, 0, 6989, 0, 0, 0, 100, 0, 0, 0, 1814, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3614, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "mlm_input_tokens_id, mlm_label= get_masked_sample(token_ids)\n",
    "print(vocab['[MASK]'])\n",
    "print(mlm_input_tokens_id)\n",
    "print(mlm_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## 当前句文本：红酥手，黄縢酒，满城春色宫墙柳\n",
      "## 下一句文本：东风恶，欢情薄\n",
      "## 是否为下一句：True\n",
      "==================\n",
      "## Mask之前的词元结果:['[CLS]', '红', '酥', '手', '，', '黄', '[UNK]', '酒', '，', '满', '城', '春', '色', '宫', '墙', '柳', '[SEP]', '东', '风', '恶', '，', '欢', '情', '薄', '[SEP]']\n",
      "## Mask之前的label ids:[101, 5273, 6989, 2797, 8024, 7942, 100, 6983, 8024, 4007, 1814, 3217, 5682, 2151, 1870, 3394, 102, 691, 7599, 2626, 8024, 3614, 2658, 5946, 102]\n",
      "## 两句话位置标记segs:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "==================\n",
      "## Mask之后的词元结果:['[CLS]', '红', '[MASK]', '手', '，', '黄', '[MASK]', '酒', '，', '满', '[MASK]', '春', '色', '宫', '墙', '柳', '[SEP]', '东', '风', '恶', '，', '[MASK]', '情', '薄', '[SEP]']\n",
      "## Mask之前的label ids:[0, 0, 6989, 0, 0, 0, 100, 0, 0, 0, 1814, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3614, 0, 0, 0]\n",
      "## 当前样本构造结束================== \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"## 当前句文本：{sentence}\")\n",
    "print(f\"## 下一句文本：{next_sentence}\")\n",
    "print(f\"## 是否为下一句：{is_next}\")\n",
    "print(f\"==================\")\n",
    "print(f\"## Mask之前的词元结果:{[vocab.itos[t] for t in token_ids]}\")\n",
    "print(f\"## Mask之前的label ids:{token_ids}\")\n",
    "print(f\"## 两句话位置标记segs:{segs}\")\n",
    "print(f\"==================\")\n",
    "print(f\"## Mask之后的词元结果:{[vocab.itos[t] for t in mlm_input_tokens_id]}\")\n",
    "print(f\"## Mask之前的label ids:{mlm_label}\")\n",
    "print(f\"## 当前样本构造结束================== \\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(paragraphs):\n",
    "    \"\"\"\n",
    "    本函数的作用是是根据格式化后的数据制作NSP和MLM两个任务对应的处理完成的数据\n",
    "    :param filepath:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # paragraphs = paragraphs\n",
    "    # 返回的是一个二维列表，每个列表可以看做是一个段落（其中每个元素为一句话）\n",
    "    data = []\n",
    "    max_len = 0 # 这里的max_len用来记录整个数据集中最长序列的长度，在后续可将其作为padding长度的标准\n",
    "\n",
    "    for paragraph in paragraphs:  # 遍历每个段落\n",
    "        for i in range(len(paragraph) - 1):  # 遍历一个段落中的每一句话\n",
    "            sentence, next_sentence, is_next = get_next_sentence_sample(paragraph[i], paragraph[i + 1], paragraphs) # 构造 NSP 样本\n",
    "            token_a_ids = [vocab[token] for token in tokenizer(sentence)]\n",
    "            token_b_ids = [vocab[token] for token in tokenizer(next_sentence)]\n",
    "            token_ids = [vocab['[CLS]']] + token_a_ids + [vocab['[SEP]']] + token_b_ids\n",
    "            token_ids += [vocab['[SEP]']]\n",
    "            seg1 = [0] * (len(token_a_ids) + 2)  # 2 表示[CLS]和中间的[SEP]这两个字符\n",
    "            seg2 = [1] * (len(token_b_ids) + 1)\n",
    "            segs = seg1 + seg2\n",
    "            segs = torch.tensor(segs, dtype=torch.long)\n",
    "            nsp_lable = torch.tensor(int(is_next), dtype=torch.long)\n",
    "            mlm_input_tokens_id, mlm_label = get_masked_sample(token_ids)\n",
    "            token_ids = torch.tensor(mlm_input_tokens_id, dtype=torch.long)\n",
    "            mlm_label = torch.tensor(mlm_label, dtype=torch.long)\n",
    "            max_len = max(max_len, token_ids.size(0))\n",
    "            data.append([token_ids, segs, nsp_lable, mlm_label])\n",
    "            \n",
    "    all_data = {'data': data, 'max_len': max_len}\n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = data_process(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 101, 1282, 2399, 4495, 3647,  697, 5755, 5755,  102,  679,  103,  103,\n",
      "        8024, 5632, 3398, 2563,  102])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "tensor(1)\n",
      "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0, 2590, 7030,\n",
      "           0,    0, 7410,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "for t in all_data['data'][0]:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data['max_len']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequences, batch_first=False, max_len=None, padding_value=0):\n",
    "    \"\"\"\n",
    "    对一个List中的元素进行padding\n",
    "    Pad a list of variable length Tensors with ``padding_value``\n",
    "    a = torch.ones(25)\n",
    "    b = torch.ones(22)\n",
    "    c = torch.ones(15)\n",
    "    pad_sequence([a, b, c],max_len=None).size()\n",
    "    torch.Size([25, 3])\n",
    "        sequences:\n",
    "        batch_first: 是否把batch_size放到第一个维度\n",
    "        padding_value:\n",
    "        max_len :\n",
    "                当max_len = 50时，表示以某个固定长度对样本进行padding，多余的截掉；\n",
    "                当max_len=None是，表示以当前batch中最长样本的长度对其它进行padding；\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if max_len is None:\n",
    "        max_len = max([s.size(0) for s in sequences])\n",
    "    out_tensors = []\n",
    "    for tensor in sequences:\n",
    "        if tensor.size(0) < max_len:\n",
    "            tensor = torch.cat([tensor, torch.tensor([padding_value] * (max_len - tensor.size(0)))], dim=0)\n",
    "        else:\n",
    "            tensor = tensor[:max_len]\n",
    "        out_tensors.append(tensor)\n",
    "    out_tensors = torch.stack(out_tensors, dim=1)\n",
    "    if batch_first:\n",
    "        return out_tensors.transpose(0, 1)\n",
    "    \n",
    "    return out_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(data_batch):\n",
    "    max_len = all_data['max_len']\n",
    "    # print('ok')\n",
    "    b_token_ids, b_segs, b_nsp_label, b_mlm_label = [], [], [], []\n",
    "    for (token_ids, segs, nsp_lable, mlm_label) in data_batch:\n",
    "        # 开始对一个batch中的每一个样本进行处理\n",
    "        b_token_ids.append(token_ids)\n",
    "        b_segs.append(segs)\n",
    "        b_nsp_label.append(nsp_lable)\n",
    "        b_mlm_label.append(mlm_label)\n",
    "        # print('ok')\n",
    "        \n",
    "    b_token_ids = pad_sequence(b_token_ids,  # [batch_size,max_len]\n",
    "                                padding_value=0,\n",
    "                                batch_first=False,\n",
    "                                max_len=max_len)\n",
    "    # b_token_ids:  [src_len,batch_size]\n",
    "\n",
    "    b_segs = pad_sequence(b_segs,  # [batch_size,max_len]\n",
    "                            padding_value=0,\n",
    "                            batch_first=False,\n",
    "                            max_len=max_len)\n",
    "    # b_segs: [src_len,batch_size]\n",
    "\n",
    "    b_mlm_label = pad_sequence(b_mlm_label,  # [batch_size,max_len]\n",
    "                                padding_value=0,\n",
    "                                batch_first=False,\n",
    "                                max_len=max_len)\n",
    "    # b_mlm_label:  [src_len,batch_size]\n",
    "\n",
    "    b_mask = (b_token_ids == 0).transpose(0, 1)\n",
    "    # b_mask: [batch_size,max_len]\n",
    "\n",
    "    b_nsp_label = torch.tensor(b_nsp_label, dtype=torch.long)\n",
    "    # b_nsp_label: [batch_size]\n",
    "    \n",
    "    return b_token_ids, b_segs, b_mask, b_mlm_label, b_nsp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(all_data['data'], batch_size=4,\n",
    "                            shuffle=True, collate_fn = generate_batch)\n",
    "\n",
    "len(train_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids: torch.Size([33, 4])\n",
      "segs: torch.Size([33, 4])\n",
      "mask: torch.Size([4, 33])\n",
      "mlm_label: torch.Size([33, 4])\n",
      "nsp_label: torch.Size([4])\n",
      "mask: tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False, False, False, False,  True,\n",
      "         True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True])\n"
     ]
    }
   ],
   "source": [
    "for b_token_ids, b_segs, b_mask, b_mlm_label, b_nsp_label in train_loader:\n",
    "    print('token_ids:',b_token_ids.shape)\n",
    "    print('segs:',b_segs.shape)\n",
    "    print('mask:',b_mask.shape)\n",
    "    print('mlm_label:',b_mlm_label.shape)\n",
    "    print('nsp_label:',b_nsp_label.shape)\n",
    "    print('mask:',b_mask[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编码处理测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([33, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "bert_embedding = BertEmbeddings(config)\n",
    "for i,(b_token_ids, b_segs, b_mask, b_mlm_label, b_nsp_label) in enumerate(train_loader):\n",
    "    bert_embedding_result = bert_embedding(input_ids = b_token_ids, token_type_ids = b_segs)\n",
    "    print(bert_embedding_result.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT的Encoder实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**模型架构：**\n",
    "\n",
    "![Bert_Encoder](BertEncoder.jpg \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自注意力机制\n",
    "class BertSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    实现多头注意力机制，对应的是GoogleResearch代码中的attention_layer方法\n",
    "    https://github.com/google-research/bert/blob/eedf5716ce1268e56f0a50264a88cafad334ac61/modeling.py#L558\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super(BertSelfAttention, self).__init__()\n",
    "\n",
    "        MultiHeadAttention = nn.MultiheadAttention\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(embed_dim=config.hidden_size, # 编码维度\n",
    "                                                       num_heads=config.num_attention_heads, # 多头\n",
    "                                                       dropout=config.attention_probs_dropout_prob) # 丢弃率\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        :param query: # [tgt_len, batch_size, hidden_size], tgt_len 表示目标序列的长度\n",
    "        :param key:  #  [src_len, batch_size, hidden_size], src_len 表示源序列的长度\n",
    "        :param value: # [src_len, batch_size, hidden_size], src_len 表示源序列的长度\n",
    "        :param attn_mask: # [tgt_len,src_len] or [num_heads*batch_size,tgt_len, src_len]\n",
    "        一般只在解码时使用，为了并行一次喂入所有解码部分的输入，所以要用mask来进行掩盖当前时刻之后的位置信息\n",
    "        在Bert中，attention_mask指代的其实是key_padding_mask，因为Bert主要是基于Transformer Encoder部分构建的，\n",
    "        所有没有Decoder部分，因此也就不需要用mask来进行掩盖当前时刻之后的位置信息\n",
    "        :param key_padding_mask: [batch_size, src_len], src_len 表示源序列的长度\n",
    "        :return:\n",
    "        attn_output: [tgt_len, batch_size, hidden_size]\n",
    "        attn_output_weights: # [batch_size, tgt_len, src_len]\n",
    "        \"\"\"\n",
    "        return self.multi_head_attention(query, key, value, attn_mask=attn_mask, key_padding_mask=key_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自注意机制输出标准化\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        \"\"\"\n",
    "        :param hidden_states: [src_len, batch_size, hidden_size]\n",
    "        :param input_tensor: [src_len, batch_size, hidden_size]\n",
    "        :return: [src_len, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        # hidden_states = self.dense(hidden_states)  # [src_len, batch_size, hidden_size]\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor) # 加上残差连接\n",
    "\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert注意力机制实现\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config)\n",
    "        self.output = BertSelfOutput(config)\n",
    "\n",
    "    def forward(self,\n",
    "                hidden_states,\n",
    "                attention_mask=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param hidden_states: [src_len, batch_size, hidden_size]\n",
    "        :param attention_mask: [batch_size, src_len]\n",
    "        :return: [src_len, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        self_outputs = self.self(hidden_states,\n",
    "                                 hidden_states,\n",
    "                                 hidden_states,\n",
    "                                 attn_mask = None,\n",
    "                                 key_padding_mask = attention_mask)\n",
    "        # self_outputs[0] shape: [src_len, batch_size, hidden_size]\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        \n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函数\n",
    "def get_activation(activation_string):\n",
    "    act = activation_string.lower()\n",
    "    if act == \"linear\":\n",
    "        return None\n",
    "    elif act == \"relu\":\n",
    "        return nn.ReLU()\n",
    "    elif act == \"gelu\":\n",
    "        return nn.GELU()\n",
    "    elif act == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation: %s\" % act)\n",
    "\n",
    "# 全连接层\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = get_activation(config.hidden_act)\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "\n",
    "        :param hidden_states: [src_len, batch_size, hidden_size]\n",
    "        :return: [src_len, batch_size, intermediate_size]\n",
    "        \"\"\"\n",
    "        hidden_states = self.dense(hidden_states)  # [src_len, batch_size, intermediate_size]\n",
    "\n",
    "        if self.intermediate_act_fn is None:\n",
    "            hidden_states = hidden_states\n",
    "        else:\n",
    "            hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "# BERT输出层\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        \"\"\"\n",
    "\n",
    "        :param hidden_states: [src_len, batch_size, intermediate_size]\n",
    "        :param input_tensor: [src_len, batch_size, hidden_size]\n",
    "        :return: [src_len, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        hidden_states = self.dense(hidden_states)  # [src_len, batch_size, hidden_size]\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert_attention = BertAttention(config)\n",
    "        self.bert_intermediate = BertIntermediate(config)\n",
    "        self.bert_output = BertOutput(config)\n",
    "\n",
    "    def forward(self,\n",
    "                hidden_states,\n",
    "                attention_mask=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param hidden_states: [src_len, batch_size, hidden_size]\n",
    "        :param attention_mask: [batch_size, src_len] mask掉padding部分的内容\n",
    "        :return: [src_len, batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        attention_output = self.bert_attention(hidden_states, attention_mask)\n",
    "        # [src_len, batch_size, hidden_size]\n",
    "        intermediate_output = self.bert_intermediate(attention_output)\n",
    "        # [src_len, batch_size, intermediate_size]\n",
    "        layer_output = self.bert_output(intermediate_output, attention_output)\n",
    "        # [src_len, batch_size, hidden_size]\n",
    "        return layer_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.bert_layers = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            hidden_states,\n",
    "            attention_mask=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param hidden_states: [src_len, batch_size, hidden_size]\n",
    "        :param attention_mask: [batch_size, src_len]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        all_encoder_layers = []\n",
    "        layer_output = hidden_states\n",
    "        \n",
    "        for i, layer_module in enumerate(self.bert_layers):\n",
    "            layer_output = layer_module(layer_output,\n",
    "                                        attention_mask)\n",
    "            #  [src_len, batch_size, hidden_size]\n",
    "            all_encoder_layers.append(layer_output)\n",
    "\n",
    "        return all_encoder_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "\n",
    "        :param hidden_states:  [src_len, batch_size, hidden_size]\n",
    "        :return: [batch_size, hidden_size]\n",
    "        \"\"\"\n",
    "        # if 'pooler_type' not in self.config.__dict__:\n",
    "        #     raise ValueError(\"pooler_type must be in ['first_token_transform', 'all_token_average']\"\n",
    "        #                      \"请在配置文件config.json中添加一个pooler_type参数\")\n",
    "        # if self.config.pooler_type == \"first_token_transform\":\n",
    "        #     token_tensor = hidden_states[0, :].reshape(-1, self.config.hidden_size)\n",
    "        # elif self.config.pooler_type == \"all_token_average\":\n",
    "        token_tensor = torch.mean(hidden_states, dim=0)\n",
    "        pooled_output = self.dense(token_tensor)  # [batch_size, hidden_size]\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        \n",
    "        return pooled_output  # [batch_size, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(nn.Module):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert_embeddings = BertEmbeddings(config)\n",
    "        self.bert_encoder = BertEncoder(config)\n",
    "        self.bert_pooler = BertPooler(config)\n",
    "        self.config = config\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids=None,\n",
    "                attention_mask=None,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None):\n",
    "        \"\"\"\n",
    "        ***** 一定要注意，attention_mask中，被mask的Token用1(True)表示，没有mask的用0(false)表示\n",
    "        这一点一定一定要注意\n",
    "        :param input_ids:  [src_len, batch_size]\n",
    "        :param attention_mask: [batch_size, src_len] mask掉padding部分的内容\n",
    "        :param token_type_ids: [src_len, batch_size]  # 如果输入模型的只有一个序列，那么这个参数也不用传值\n",
    "        :param position_ids: [1,src_len] # 在实际建模时这个参数其实可以不用传值\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        embedding_output = self.bert_embeddings(input_ids=input_ids,\n",
    "                                                position_ids=position_ids,\n",
    "                                                token_type_ids=token_type_ids)\n",
    "        # embedding_output: [src_len, batch_size, hidden_size]\n",
    "\n",
    "        all_encoder_outputs = self.bert_encoder(embedding_output,\n",
    "                                                attention_mask=attention_mask)\n",
    "        # all_encoder_outputs 为一个包含有num_hidden_layers个层的输出\n",
    "\n",
    "        sequence_output = all_encoder_outputs[-1]  # 取最后一层\n",
    "        # sequence_output: [src_len, batch_size, hidden_size]\n",
    "\n",
    "        pooled_output = self.bert_pooler(sequence_output)\n",
    "        # 默认是最后一层的first token 即[cls]位置经dense + tanh 后的结果\n",
    "        # pooled_output: [batch_size, hidden_size]\n",
    "        \n",
    "        return pooled_output, all_encoder_outputs\n",
    "    \n",
    "    def _reset_parameters(self):\n",
    "        r\"\"\"Initiate parameters in the transformer model.\"\"\"\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        \"\"\"\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                normal_(p, mean=0.0, std=self.config.initializer_range)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 768])\n",
      "torch.Size([33, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "bert_model = BertModel(config)\n",
    "\n",
    "for i,(b_token_ids, b_segs, b_mask, b_mlm_label, b_nsp_label) in enumerate(train_loader):\n",
    "    bert_model_output, all_encoder_outputs  = bert_model(input_ids = b_token_ids, attention_mask=b_mask, token_type_ids = b_segs)\n",
    "    print(bert_model_output.shape)\n",
    "    print(all_encoder_outputs[-1].shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预训练任务实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLM实现\n",
    "class BertForLMTransformHead(nn.Module):\n",
    "    \"\"\"\n",
    "    用于BertForMaskedLM中的一次变换。 因为在单独的MLM任务中\n",
    "    和最后NSP与MLM的整体任务中均要用到，所以这里单独抽象为一个类便于复用\n",
    "\n",
    "    ref: https://github.com/google-research/bert/blob/master/run_pretraining.py\n",
    "        第248-262行\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, bert_model_embedding_weights=None):\n",
    "        \"\"\"\n",
    "        :param config:\n",
    "        :param bert_model_embedding_weights:\n",
    "        the output-weights are the same as the input embeddings, but there is\n",
    "        an output-only bias for each token. 即TokenEmbedding层中的词表矩阵\n",
    "        \"\"\"\n",
    "        super(BertForLMTransformHead, self).__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = get_activation(config.hidden_act)\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "        # [hidden_size, vocab_size]\n",
    "        \n",
    "        self.decoder.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        \"\"\"\n",
    "        :param hidden_states: [src_len, batch_size, hidden_size] Bert最后一层的输出\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        hidden_states = self.dense(hidden_states)  # [src_len, batch_size, hidden_size]\n",
    "        hidden_states = self.transform_act_fn(hidden_states)  # [src_len, batch_size, hidden_size]\n",
    "        hidden_states = self.LayerNorm(hidden_states)  # [src_len, batch_size, hidden_size]\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        # hidden_states:  [src_len, batch_size, vocab_size]\n",
    "        \n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForPretrainingModel(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT预训练模型，包括MLM和NSP两个任务\n",
    "    \"\"\"\n",
    "    def __init__(self, config, bert_pretrained_model_dir=None):\n",
    "        super(BertForPretrainingModel, self).__init__()\n",
    "        self.bert = BertModel(config)\n",
    "        weights = None\n",
    "        self.mlm_prediction = BertForLMTransformHead(config, weights) # 句子mask预测\n",
    "        self.nsp_prediction = nn.Linear(config.hidden_size, 2) # 句子间预测\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, input_ids,  # [src_len, batch_size]\n",
    "                attention_mask=None,  # [batch_size, src_len] mask掉padding部分的内容\n",
    "                token_type_ids=None,  # [src_len, batch_size]\n",
    "                position_ids=None,\n",
    "                masked_lm_labels=None,  # [src_len,batch_size]\n",
    "                next_sentence_labels=None):  # [batch_size]\n",
    "        # 自注意输出\n",
    "        pooled_output, all_encoder_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask,token_type_ids=token_type_ids,position_ids=position_ids)\n",
    "        \n",
    "        # 取Bert最后一层的输出\n",
    "        sequence_output = all_encoder_outputs[-1]  \n",
    "        # sequence_output: [src_len, batch_size, hidden_size]\n",
    "        \n",
    "        mlm_prediction_logits = self.mlm_prediction(sequence_output)\n",
    "        # mlm_prediction_logits: [src_len, batch_size, vocab_size]\n",
    "\n",
    "        nsp_pred_logits = self.nsp_prediction(pooled_output)\n",
    "        # nsp_pred_logits： [batch_size, 2]\n",
    "        \n",
    "        if masked_lm_labels is not None and next_sentence_labels is not None:\n",
    "            loss_fct_mlm = nn.CrossEntropyLoss(ignore_index=0)\n",
    "            # MLM任务在构造数据集时pandding部分和MASK部分都是用的0来填充，所以ignore_index需要指定为0\n",
    "            loss_fct_nsp = nn.CrossEntropyLoss()\n",
    "            # 由于NSP中的分类标签中含有0，上面MLM中的损失指定了ignore_index=0，所以这里需要重新定义一个CrossEntropyLoss\n",
    "            # 如果MLM任务在padding和MASK中用100之类的来代替，那么两者可以共用一个CrossEntropyLoss\n",
    "            mlm_loss = loss_fct_mlm(mlm_prediction_logits.reshape(-1, self.config.vocab_size),\n",
    "                                    masked_lm_labels.reshape(-1))\n",
    "            nsp_loss = loss_fct_nsp(nsp_pred_logits.reshape(-1, 2),\n",
    "                                    next_sentence_labels.reshape(-1))\n",
    "            total_loss = mlm_loss + nsp_loss\n",
    "            return total_loss, mlm_prediction_logits, nsp_pred_logits\n",
    "        else:\n",
    "            return mlm_prediction_logits, nsp_pred_logits\n",
    "        # [src_len, batch_size, vocab_size], [batch_size, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.8286, grad_fn=<AddBackward0>)\n",
      "torch.Size([33, 4, 21128])\n",
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "Bert_Pre_model = BertForPretrainingModel(config)\n",
    "\n",
    "for i,(b_token_ids, b_segs, b_mask, b_mlm_label, b_nsp_label) in enumerate(train_loader):\n",
    "    total_loss, mlm_prediction_logits, nsp_pred_logits  = Bert_Pre_model(input_ids = b_token_ids, attention_mask=b_mask, token_type_ids = b_segs, next_sentence_labels=b_nsp_label, masked_lm_labels = b_mlm_label)\n",
    "    print(total_loss)\n",
    "    print(mlm_prediction_logits.shape)\n",
    "    print(nsp_pred_logits.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预训练模型整体实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(lr=0.0002, b1=0.5, b2=0.999)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "## 超参数配置\n",
    "parser = argparse.ArgumentParser() # 创建参数解析器的实例\n",
    "\n",
    "# 添加一个选项参数\n",
    "parser.add_argument(\"--lr\", type=float, default=0.0002, help=\"adam: learning rate\")\n",
    "parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "\n",
    "\n",
    "# 访问参数值\n",
    "# opt = parser.parse_args() # 解析命令行中的参数\n",
    "opt = parser.parse_args(args=[])                 ## 在jupyter notebook中运行时，换为此行\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "整体训练误差： Epoch 1 11.202541987101236\n",
      "整体训练误差： Epoch 2 8.934702396392822\n",
      "整体训练误差： Epoch 3 8.143661975860596\n",
      "整体训练误差： Epoch 4 7.198670069376628\n",
      "整体训练误差： Epoch 5 6.7806064287821455\n",
      "整体训练误差： Epoch 6 6.426120758056641\n",
      "整体训练误差： Epoch 7 8.323813279469809\n",
      "整体训练误差： Epoch 8 6.039237817128499\n",
      "整体训练误差： Epoch 9 5.490671475728353\n",
      "整体训练误差： Epoch 10 5.126372655232747\n"
     ]
    }
   ],
   "source": [
    "Bert_Pre_model = BertForPretrainingModel(config)\n",
    "optimizer = torch.optim.Adam(Bert_Pre_model.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "## ----------\n",
    "##  Training\n",
    "## ----------\n",
    "## 进行多个epoch的训练\n",
    "for epoch in range(10):                               ## epoch:50\n",
    "    losses = 0\n",
    "    for i,(b_token_ids, b_segs, b_mask, b_mlm_label, b_nsp_label) in enumerate(train_loader):\n",
    "        loss, mlm_prediction_logits, nsp_pred_logits  = Bert_Pre_model(input_ids = b_token_ids, attention_mask=b_mask, token_type_ids = b_segs, next_sentence_labels=b_nsp_label, masked_lm_labels = b_mlm_label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses += loss.item()\n",
    "\n",
    "    train_loss = losses / len(train_loader)\n",
    "    \n",
    "    print('整体训练误差：','Epoch %d'%(epoch + 1), train_loss)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
