{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DGNN代码复现\n",
    "\n",
    "* [源仓库地址](https://github.com/alge24/DyGNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入python库和相关自定义文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from model_recurrent import DyGNN\n",
    "from datasets import Temporal_Dataset\n",
    "import argparse\n",
    "from scipy.stats import rankdata\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from param_parser import parameter_parser, tab_printer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取全局参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parameter_parser()\n",
    "args.batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|   Parameter    | Value |\n",
      "+================+=======+\n",
      "| Act            | tanh  |\n",
      "+----------------+-------+\n",
      "| Batch size     | 5     |\n",
      "+----------------+-------+\n",
      "| Dataset        | uci   |\n",
      "+----------------+-------+\n",
      "| Decay method   | log   |\n",
      "+----------------+-------+\n",
      "| Drop p         | 0     |\n",
      "+----------------+-------+\n",
      "| If no time     | 0     |\n",
      "+----------------+-------+\n",
      "| If propagation | 1     |\n",
      "+----------------+-------+\n",
      "| If updated     | 0     |\n",
      "+----------------+-------+\n",
      "| Is att         | 1     |\n",
      "+----------------+-------+\n",
      "| Learning rate  | 0.001 |\n",
      "+----------------+-------+\n",
      "| Nor            | 0     |\n",
      "+----------------+-------+\n",
      "| Num negative   | 5     |\n",
      "+----------------+-------+\n",
      "| Reset rep      | 1     |\n",
      "+----------------+-------+\n",
      "| Second order   | 0     |\n",
      "+----------------+-------+\n",
      "| Seed           | 0     |\n",
      "+----------------+-------+\n",
      "| Threhold       | None  |\n",
      "+----------------+-------+\n",
      "| Train ratio    | 0.800 |\n",
      "+----------------+-------+\n",
      "| Transfer       | 1     |\n",
      "+----------------+-------+\n",
      "| Valid ratio    | 0.010 |\n",
      "+----------------+-------+\n",
      "| W              | 2     |\n",
      "+----------------+-------+\n",
      "| Weight decay   | 0.001 |\n",
      "+----------------+-------+\n"
     ]
    }
   ],
   "source": [
    "tab_printer(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型的存储文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型的存储文件夹\n",
    "model_save_dir = 'saved_models/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "data = Temporal_Dataset('Dataset/UCI_email_1899_59835/opsahl-ucsocial/out.opsahl-ucsocial',1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59835"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1899"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.node_num()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on UCI_message dataset\n"
     ]
    }
   ],
   "source": [
    "if args.dataset == 'uci':\n",
    "    num_nodes = data.node_num() # 节点数量 1899\n",
    "    model_save_dir = model_save_dir + 'UCI/'  # 模型存储路径\n",
    "    print('Train on UCI_message dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = args.batch_size\n",
    "learning_rate = args.learning_rate\n",
    "num_negative = args.num_negative\n",
    "act = args.act\n",
    "transfer = args.transfer\n",
    "drop_p = args.drop_p\n",
    "if_propagation = args.if_propagation\n",
    "w = args.w\n",
    "is_att = args.is_att\n",
    "seed = args.seed\n",
    "reset_rep = args.reset_rep\n",
    "decay_method = args.decay_method\n",
    "nor = args.nor\n",
    "if_updated = args.if_updated\n",
    "weight_decay = args.weight_decay\n",
    "if_no_time = args.if_no_time\n",
    "threhold = args.threhold\n",
    "second_order = args.second_order\n",
    "num_iter = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练集、验证集与测试集划分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data length:  59835\n",
      "Train length:  47868\n",
      "Valid length:  598\n",
      "Test length:  11369\n"
     ]
    }
   ],
   "source": [
    "train_ratio = args.train_ratio\n",
    "valid_ratio = args.valid_ratio\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_data = data[0:int(len(data)*train_ratio)]  # 选择训练集\n",
    "validation_data = data[int(len(data)*train_ratio):int(len(data)*(train_ratio+valid_ratio))]  # 验证集\n",
    "test_data = data[int(len(data)*(train_ratio + valid_ratio)):len(data)]  # 测试集\n",
    "print('Data length: ', len(data))\n",
    "print('Train length: ', len(train_data))\n",
    "print('Valid length: ', len(validation_data))\n",
    "print('Test length: ', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47868, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = SequentialSampler(train_data)  # 顺序采样\n",
    "data_loader = DataLoader(train_data, batch_size, sampler = sampler)  # 定义dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_nodes 1899\n"
     ]
    }
   ],
   "source": [
    "all_nodes = set(range(num_nodes))\n",
    "print('num_nodes',len(all_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### src和dst节点对应到的所有节点集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node2candidate(train_data, all_nodes, pri = False):\n",
    "    head_node2candidate = dict()\n",
    "    tail_node2candidate = dict()\n",
    "\n",
    "    pri = True\n",
    "    if pri:\n",
    "        start_time = time.time()\n",
    "        print('Start to build node2candidate')\n",
    "\n",
    "\n",
    "    for i in range(len(train_data)):  # 遍历训练数据;\n",
    "\n",
    "        head, tail, not_in_use = train_data[i]  # src, dst, time\n",
    "        head = int(head)\n",
    "        tail = int(tail)\n",
    "        if head not in head_node2candidate:\n",
    "            head_node2candidate[head] = all_nodes  # src节点对应到的所有节点\n",
    "\n",
    "        if tail not in tail_node2candidate:\n",
    "            tail_node2candidate[tail] = all_nodes  # dst节点\n",
    "\n",
    "\n",
    "\n",
    "    if pri: \n",
    "        end_time = time.time()\n",
    "\n",
    "        print('node2candidate built in' , str(end_time-start_time))\n",
    "    return head_node2candidate, tail_node2candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to build node2candidate\n",
      "node2candidate built in 0.053854942321777344\n"
     ]
    }
   ],
   "source": [
    "head_node2candidate, tail_node2candidate = get_node2candidate(train_data, all_nodes)  # src和dst节点对应到的所有节点集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1217 1899\n"
     ]
    }
   ],
   "source": [
    "print(len(head_node2candidate),len(head_node2candidate[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型存储位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_dir = model_save_dir  + 'nt_' +str(if_no_time)+ '_wd_' + str(weight_decay) + '_up_' + str(if_updated) +'_w_' + str(w) +'_b_' + str(batch_size) + '_l_' + str(learning_rate) + '_tr_' + str(train_ratio) + '_nn_' +str(num_negative)+'_' + act + '_trans_' +str(transfer) + '_dr_p_' + str(drop_p) + '_prop_' + str(if_propagation) + '_att_' +str(is_att) + '_rp_' + str(reset_rep) + '_dcm_' + decay_method + '_nor_' + str(nor)\n",
    "if threhold is not None:\n",
    "    model_save_dir = model_save_dir + '_th_' + str(threhold)\n",
    "if second_order:\n",
    "    model_save_dir = model_save_dir + '_2hop'\n",
    "if not os.path.exists(model_save_dir):\n",
    "    os.makedirs(model_save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only propagate to relevance nodes below time interval:  None\n"
     ]
    }
   ],
   "source": [
    "dyGnn = DyGNN(num_nodes,64,64,device, w,is_att ,transfer,nor,if_no_time, threhold,second_order, if_updated,drop_p, num_negative, act, if_propagation, decay_method )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DyGNN(\n",
       "  (combiner): Combiner(\n",
       "    (h2o): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (l2o): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (act): Tanh()\n",
       "  )\n",
       "  (act): Tanh()\n",
       "  (decayer): Decayer(\n",
       "    (linear): Linear(in_features=1, out_features=1, bias=False)\n",
       "  )\n",
       "  (edge_updater_head): Edge_updater_nn(\n",
       "    (h2o): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (l2o): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (act): Tanh()\n",
       "  )\n",
       "  (edge_updater_tail): Edge_updater_nn(\n",
       "    (h2o): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (l2o): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (act): Tanh()\n",
       "  )\n",
       "  (node_updater_head): TLSTM(\n",
       "    (i2h): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (h2h): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (c2s): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "    )\n",
       "    (sigmoid): Sigmoid()\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       "  (node_updater_tail): TLSTM(\n",
       "    (i2h): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (h2h): Linear(in_features=64, out_features=256, bias=True)\n",
       "    (c2s): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "    )\n",
       "    (sigmoid): Sigmoid()\n",
       "    (tanh): Tanh()\n",
       "  )\n",
       "  (tran_head_edge_head): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (tran_head_edge_tail): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (tran_tail_edge_head): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (tran_tail_edge_tail): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (attention): Attention(\n",
       "    (bilinear): Bilinear(in1_features=64, in2_features=64, out_features=1, bias=True)\n",
       "    (softmax): Softmax(dim=0)\n",
       "  )\n",
       "  (cell_head): Embedding(1899, 64)\n",
       "  (cell_tail): Embedding(1899, 64)\n",
       "  (hidden_head): Embedding(1899, 64)\n",
       "  (hidden_tail): Embedding(1899, 64)\n",
       "  (node_representations): Embedding(1899, 64)\n",
       "  (transfer2head): Linear(in_features=64, out_features=64, bias=False)\n",
       "  (transfer2tail): Linear(in_features=64, out_features=64, bias=False)\n",
       "  (dropout): Dropout(p=0, inplace=False)\n",
       "  (cell_head_copy): Embedding(1899, 64)\n",
       "  (cell_tail_copy): Embedding(1899, 64)\n",
       "  (hidden_head_copy): Embedding(1899, 64)\n",
       "  (hidden_tail_copy): Embedding(1899, 64)\n",
       "  (node_representations_copy): Embedding(1899, 64)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dyGnn.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,dyGnn.parameters()),lr = learning_rate, weight_decay=weight_decay)\n",
    "old_head_rank = num_nodes/2 # 949.5\n",
    "old_tail_rank = num_nodes/2 # 949.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算验证集损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(data, head_reps, tail_reps,device):\n",
    "\n",
    "    head_list = list(data[:,0])\n",
    "    tail_list = list(data[:,1])\n",
    "\n",
    "    head_tensors = head_reps(torch.LongTensor(head_list).to(device))\n",
    "    tail_tensors = tail_reps(torch.LongTensor(tail_list).to(device))\n",
    "    scores = torch.bmm(head_tensors.view(len(head_list),1,head_tensors.size()[1]),tail_tensors.view(len(head_list),head_tensors.size()[1],1)).view(len(head_list))\n",
    "    labels = torch.FloatTensor([1]*len(head_list)).to(device)\n",
    "    bce_with_logits_loss = nn.BCEWithLogitsLoss().to(device)\n",
    "    loss = bce_with_logits_loss(scores,labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 得分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(node, true_candidate, node2candidate, node_reps, candidate_reps, device, pri = False):\n",
    "    node_tensor = node_reps(torch.LongTensor([node]).to(device)).view(-1,1)\n",
    "    candidates = list(node2candidate[node])\n",
    "\n",
    "    candidates.append(true_candidate)\n",
    "\n",
    "    length = len(candidates)\n",
    "\n",
    "    candidate_tensors = candidate_reps(torch.LongTensor(candidates).to(device))\n",
    "\n",
    "    scores = torch.mm(candidate_tensors, node_tensor)\n",
    "    negative_scores_numpy = -scores.view(1,-1).to('cpu').numpy()\n",
    "    rank = rankdata(negative_scores_numpy)[-1]\n",
    "\n",
    "    if pri:\n",
    "        print(node , true_candidate)\n",
    "        print(scores.view(-1))\n",
    "        print(rank, 'out of',length)\n",
    "\n",
    "    return rank, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranks(test_data,head_reps, tail_reps, device, head_node2candidate, tail_node2candidate, pri=False, previous_links = None, bo = False):\n",
    "\n",
    "    head_ranks = []\n",
    "    tail_ranks = []\n",
    "    head_lengths = []\n",
    "    tail_lengths = []\n",
    "\n",
    "    for interactioin in test_data:\n",
    "        head_node, tail_node , time = interactioin\n",
    "        head_node = int(head_node)\n",
    "        tail_node = int(tail_node)\n",
    "        if pri:\n",
    "            print('--------------', head_node, tail_node, '---------------')\n",
    "\n",
    "\n",
    "        if bo:\n",
    "            if previous_links is not None: \n",
    "                if head_node in head_node2candidate and tail_node in tail_node2candidate and tail_node in head_node2candidate and head_node in tail_node2candidate and (head_node, tail_node) not in previous_links:\n",
    "                    head_rank, head_length = rank(head_node, tail_node, head_node2candidate, head_reps, tail_reps, device,pri)\n",
    "                    head_ranks.append(head_rank)\n",
    "                    head_lengths.append(head_length)\n",
    "\n",
    "                    tail_rank, tail_length = rank(tail_node, head_node, tail_node2candidate, tail_reps, head_reps, device)\n",
    "                    tail_ranks.append(tail_rank)\n",
    "                    tail_lengths.append(tail_length)\n",
    "            else:\n",
    "\n",
    "                if head_node in head_node2candidate and tail_node in tail_node2candidate and tail_node in head_node2candidate and head_node in tail_node2candidate:\n",
    "                    head_rank, head_length = rank(head_node, tail_node, head_node2candidate, head_reps, tail_reps, device,pri)\n",
    "                    head_ranks.append(head_rank)\n",
    "                    head_lengths.append(head_length)\n",
    "\n",
    "                    tail_rank, tail_length = rank(tail_node, head_node, tail_node2candidate, tail_reps, head_reps, device)\n",
    "                    tail_ranks.append(tail_rank)\n",
    "                    tail_lengths.append(tail_length)\n",
    "        else:\n",
    "\n",
    "            if previous_links is not None: \n",
    "                if head_node in head_node2candidate and tail_node in tail_node2candidate and (head_node, tail_node) not in previous_links:\n",
    "                    head_rank, head_length = rank(head_node, tail_node, head_node2candidate, head_reps, tail_reps, device,pri)\n",
    "                    head_ranks.append(head_rank)\n",
    "                    head_lengths.append(head_length)\n",
    "\n",
    "                    tail_rank, tail_length = rank(tail_node, head_node, tail_node2candidate, tail_reps, head_reps, device)\n",
    "                    tail_ranks.append(tail_rank)\n",
    "                    tail_lengths.append(tail_length)\n",
    "            else:\n",
    "\n",
    "                if head_node in head_node2candidate and tail_node in tail_node2candidate:\n",
    "                    head_rank, head_length = rank(head_node, tail_node, head_node2candidate, head_reps, tail_reps, device,pri)\n",
    "                    head_ranks.append(head_rank)\n",
    "                    head_lengths.append(head_length)\n",
    "\n",
    "                    tail_rank, tail_length = rank(tail_node, head_node, tail_node2candidate, tail_reps, head_reps, device)\n",
    "                    tail_ranks.append(tail_rank)\n",
    "                    tail_lengths.append(tail_length)\n",
    "\n",
    "    return head_ranks, tail_ranks, head_lengths, tail_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 链接预测任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_prediction(data, reps):\n",
    "    head_list = list(data[:,0])\n",
    "    tail_list = list(data[:,1])\n",
    "    head_reps = reps[head_list,:]\n",
    "    tail_reps = reps[tail_list,:]\n",
    "\n",
    "def get_previous_links(data):\n",
    "    previous_links = set()\n",
    "    for i in range(len(data)):\n",
    "        head, tail, time = data[i]\n",
    "        previous_links.add((int(head), int(tail)))\n",
    "    return previous_links "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "Resetting time...\n",
      "Time reset\n",
      "reps reset\n",
      "0  train_loss:  1.1134543418884277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000  train_loss:  0.2990732789039612\n",
      "2000  train_loss:  0.3084699213504791\n",
      "3000  train_loss:  0.2624087631702423\n",
      "4000  train_loss:  0.3137985169887543\n",
      "5000  train_loss:  0.3106762170791626\n",
      "6000  train_loss:  0.264798641204834\n",
      "7000  train_loss:  0.28286978602409363\n",
      "8000  train_loss:  0.30891525745391846\n",
      "9000  train_loss:  0.29906752705574036\n",
      "head_length mean:  1900.0 ; num_test:  523\n",
      "tail_lengths mean:  1900.0 ; num_test:  523\n",
      "head_rank mean:  927.9311663479923  ;  head_rank var:  263297.83082598186\n",
      "tail_rank mean:  894.8546845124283  ;  tail_rank var:  264886.0046960286\n",
      "reverse head_rank mean:  0.0014650623107585626\n",
      "reverse tail_rank mean:  0.0015926185354674747\n",
      "head_rank HITS 100:  0\n",
      "tail_rank_HITS 100:  0\n",
      "head_rank HITS 50:  0\n",
      "tail_rank_HITS 50:  0\n",
      "head_rank HITS 20:  0\n",
      "tail_rank_HITS 20:  0\n",
      "model saved in:  saved_models/UCI/nt_0_wd_0.001_up_0_w_2_b_5_l_0.001_tr_0.8_nn_5_tanh_trans_1_dr_p_0_prop_1_att_1_rp_1_dcm_log_nor_0/model_after_epoch_0.pt\n",
      "epoch:  1\n",
      "Resetting time...\n",
      "Time reset\n",
      "reps reset\n",
      "0  train_loss:  0.521590530872345\n",
      "1000  train_loss:  0.2810666859149933\n",
      "2000  train_loss:  0.2969825267791748\n",
      "3000  train_loss:  0.2678421437740326\n",
      "4000  train_loss:  0.30873385071754456\n",
      "5000  train_loss:  0.31199726462364197\n",
      "6000  train_loss:  0.29614880681037903\n",
      "7000  train_loss:  0.279989093542099\n",
      "8000  train_loss:  0.29434096813201904\n",
      "9000  train_loss:  0.3448079228401184\n",
      "head_length mean:  1900.0 ; num_test:  523\n",
      "tail_lengths mean:  1900.0 ; num_test:  523\n",
      "head_rank mean:  660.1835564053538  ;  head_rank var:  253427.23877358524\n",
      "tail_rank mean:  738.4502868068834  ;  tail_rank var:  327345.2470505869\n",
      "reverse head_rank mean:  0.002467183888765228\n",
      "reverse tail_rank mean:  0.002388520792007157\n",
      "head_rank HITS 100:  0\n",
      "tail_rank_HITS 100:  0\n",
      "head_rank HITS 50:  0\n",
      "tail_rank_HITS 50:  0\n",
      "head_rank HITS 20:  0\n",
      "tail_rank_HITS 20:  0\n",
      "model saved in:  saved_models/UCI/nt_0_wd_0.001_up_0_w_2_b_5_l_0.001_tr_0.8_nn_5_tanh_trans_1_dr_p_0_prop_1_att_1_rp_1_dcm_log_nor_0/model_after_epoch_1.pt\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_iter):\n",
    "    print('epoch: ', epoch)\n",
    "    print('Resetting time...')\n",
    "    dyGnn.reset_time()  # 定义最近的时间; 交互的时间\n",
    "    print('Time reset')\n",
    "\n",
    "    if reset_rep:\n",
    "        dyGnn.reset_reps()\n",
    "        print('reps reset')\n",
    "\n",
    "    x = int(5000/batch_size)\n",
    "    y = int(10000/batch_size)\n",
    "\n",
    "\n",
    "    for i, interactions in enumerate(data_loader):\n",
    "        # interactions.shape = torch.tensor[5,3] => [batch, info]\n",
    "        # interactions: [src, dst, time_diff] \n",
    "        # Compute and print loss.\n",
    "        loss = dyGnn.loss(interactions)  # 计算loss函数\n",
    "        if i%x==0:\n",
    "            #dyGnn.reset_reps()\n",
    "            print(i,' train_loss: ', loss.item())\n",
    "\n",
    "            if transfer:\n",
    "                head_reps = nn.Embedding.from_pretrained(dyGnn.transfer2head(dyGnn.node_representations.weight))  # 获取embedding\n",
    "                tail_reps = nn.Embedding.from_pretrained(dyGnn.transfer2tail(dyGnn.node_representations.weight))\n",
    "            else:\n",
    "                head_reps = dyGnn.node_representations\n",
    "                tail_reps = dyGnn.node_representations\n",
    "\n",
    "            # normalize\n",
    "            head_reps = nn.Embedding.from_pretrained(nn.functional.normalize(head_reps.weight))  # 节点特征归一化\n",
    "            tail_reps = nn.Embedding.from_pretrained(nn.functional.normalize(tail_reps.weight))\n",
    "\n",
    "\n",
    "        if i%y==-1:\n",
    "            if transfer:\n",
    "                head_reps = nn.Embedding.from_pretrained(dyGnn.transfer2head(dyGnn.node_representations.weight))\n",
    "                tail_reps = nn.Embedding.from_pretrained(dyGnn.transfer2tail(dyGnn.node_representations.weight))\n",
    "            else:\n",
    "                head_reps = dyGnn.node_representations\n",
    "                tail_reps = dyGnn.node_representations\n",
    "\n",
    "            head_reps = nn.Embedding.from_pretrained(nn.functional.normalize(head_reps.weight))\n",
    "            tail_reps = nn.Embedding.from_pretrained(nn.functional.normalize(tail_reps.weight))\n",
    "\n",
    "            head_ranks, tail_ranks, not_in_use, not_in_use2 = get_ranks(validation_data,head_reps, tail_reps, device, head_node2candidate, tail_node2candidate)  # 评价指标\n",
    "            head_ranks_numpy = np.asarray(head_ranks)\n",
    "            tail_ranks_numpy = np.asarray(tail_ranks)\n",
    "            print('head_rank mean: ', np.mean(head_ranks_numpy),' ; ', 'head_rank var: ', np.var(head_ranks_numpy))\n",
    "            print('tail_rank mean: ', np.mean(tail_ranks_numpy),' ; ', 'tail_rank var: ', np.var(tail_ranks_numpy))\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    if transfer:\n",
    "        head_reps = nn.Embedding.from_pretrained(dyGnn.transfer2head(dyGnn.node_representations.weight))\n",
    "        tail_reps = nn.Embedding.from_pretrained(dyGnn.transfer2tail(dyGnn.node_representations.weight))\n",
    "    else:\n",
    "        head_reps = dyGnn.node_representations\n",
    "        tail_reps = dyGnn.node_representations\n",
    "        \n",
    "    head_reps = nn.Embedding.from_pretrained(nn.functional.normalize(head_reps.weight))\n",
    "    tail_reps = nn.Embedding.from_pretrained(nn.functional.normalize(tail_reps.weight))\n",
    "\n",
    "\n",
    "    valid_loss = get_loss(validation_data, head_reps, tail_reps, device)  # 验证集边分类loss\n",
    "    head_ranks, tail_ranks, head_lengths, tail_lengths = get_ranks(validation_data, head_reps, tail_reps, device, head_node2candidate, tail_node2candidate)\n",
    "    head_ranks_numpy = np.asarray(head_ranks)\n",
    "    tail_ranks_numpy = np.asarray(tail_ranks)\n",
    "    head_lengths_numpy = np.asarray(head_lengths)\n",
    "    tail_lengths_numpy = np.asarray(tail_lengths)\n",
    "\n",
    "    mean_head_rank = np.mean(head_ranks_numpy)\n",
    "    mean_tail_rank = np.mean(tail_ranks_numpy)\n",
    "\n",
    "\n",
    "    print('head_length mean: ', np.mean(head_lengths_numpy), ';', 'num_test: ', head_lengths_numpy.shape[0])\n",
    "    print('tail_lengths mean: ', np.mean(tail_lengths_numpy), ';', 'num_test: ', tail_lengths_numpy.shape[0])\n",
    "    print('head_rank mean: ', mean_head_rank,' ; ', 'head_rank var: ', np.var(head_ranks_numpy))\n",
    "    print('tail_rank mean: ', mean_tail_rank,' ; ', 'tail_rank var: ', np.var(tail_ranks_numpy))\n",
    "    print('reverse head_rank mean: ', np.mean(1/head_ranks_numpy))\n",
    "    print('reverse tail_rank mean: ', np.mean(1/tail_ranks_numpy))\n",
    "    print('head_rank HITS 100: ', (head_ranks_numpy<=100).sum())\n",
    "    print('tail_rank_HITS 100: ', (tail_ranks_numpy<=100).sum())\n",
    "    print('head_rank HITS 50: ', (head_ranks_numpy<=50).sum())\n",
    "    print('tail_rank_HITS 50: ', (tail_ranks_numpy<=50).sum())\n",
    "    print('head_rank HITS 20: ', (head_ranks_numpy<=20).sum())\n",
    "    print('tail_rank_HITS 20: ', (tail_ranks_numpy<=20).sum())\n",
    "\n",
    "\n",
    "    if mean_head_rank < old_head_rank or mean_tail_rank < old_tail_rank:\n",
    "        model_save_path = model_save_dir + '/' + 'model_after_epoch_' + str(epoch) + '.pt'\n",
    "        torch.save(dyGnn.state_dict(), model_save_path)\n",
    "        print('model saved in: ', model_save_path)\n",
    "\n",
    "\n",
    "        with open(model_save_dir + '/' + '0valid_results.txt','a') as f:\n",
    "            f.write('epoch: ' + str(epoch) + '\\n')\n",
    "            f.write('head_rank mean: ' + str(mean_head_rank) + ' ; ' +  'head_rank var: ' + str(np.var(head_ranks_numpy)) + '\\n')\n",
    "            f.write('tail_rank mean: ' + str(mean_tail_rank) + ' ; ' +  'tail_rank var: ' + str(np.var(tail_ranks_numpy)) + '\\n')\n",
    "            f.write('head_rank HITS 100: ' + str ( (head_ranks_numpy<=100).sum()) + '\\n')\n",
    "            f.write('tail_rank_HITS 100: ' + str ( (tail_ranks_numpy<=100).sum()) + '\\n')\n",
    "            f.write('head_rank HITS 50: ' + str( (head_ranks_numpy<=50).sum()) + '\\n')\n",
    "            f.write('tail_rank_HITS 50: ' + str( (tail_ranks_numpy<=50).sum()) + '\\n')\n",
    "            f.write('head_rank HITS 20: ' + str( (head_ranks_numpy<=20).sum()) + '\\n')\n",
    "            f.write('tail_rank_HITS 20: ' + str( (tail_ranks_numpy<=20).sum()) + '\\n')\n",
    "            f.write('============================================================================\\n')\n",
    "            \n",
    "        old_head_rank = mean_head_rank + 200\n",
    "        old_tail_rank = mean_tail_rank + 200"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
