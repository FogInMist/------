{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STGCN代码复现\n",
    "* [源仓库地址](https://github.com/hazdzz/STGCN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入python库和文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import random\n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils as utils\n",
    "\n",
    "from script import dataloader, utility, earlystopping\n",
    "from model import models\n",
    "\n",
    "from param_parser import parameter_parser, tab_printer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_env(seed):\n",
    "    # Set available CUDA devices\n",
    "    # This option is crucial for an multi-GPU device\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0, 1'\n",
    "    # os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "    # os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':16:8'\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    os.environ['PYTHONHASHSEED']=str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parameter_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------+\n",
      "|     Parameter     |      Value      |\n",
      "+===================+=================+\n",
      "| Ks                | 3               |\n",
      "+-------------------+-----------------+\n",
      "| Kt                | 3               |\n",
      "+-------------------+-----------------+\n",
      "| Act func          | glu             |\n",
      "+-------------------+-----------------+\n",
      "| Batch size        | 32              |\n",
      "+-------------------+-----------------+\n",
      "| Dataset           | metr-la         |\n",
      "+-------------------+-----------------+\n",
      "| Droprate          | 0.500           |\n",
      "+-------------------+-----------------+\n",
      "| Enable bias       | 1               |\n",
      "+-------------------+-----------------+\n",
      "| Enable cuda       | 1               |\n",
      "+-------------------+-----------------+\n",
      "| Epochs            | 10000           |\n",
      "+-------------------+-----------------+\n",
      "| Gamma             | 0.950           |\n",
      "+-------------------+-----------------+\n",
      "| Graph conv type   | cheb_graph_conv |\n",
      "+-------------------+-----------------+\n",
      "| Gso type          | sym_norm_lap    |\n",
      "+-------------------+-----------------+\n",
      "| Lr                | 0.001           |\n",
      "+-------------------+-----------------+\n",
      "| N his             | 12              |\n",
      "+-------------------+-----------------+\n",
      "| N pred            | 3               |\n",
      "+-------------------+-----------------+\n",
      "| Opt               | adam            |\n",
      "+-------------------+-----------------+\n",
      "| Patience          | 30              |\n",
      "+-------------------+-----------------+\n",
      "| Seed              | 42              |\n",
      "+-------------------+-----------------+\n",
      "| Stblock num       | 2               |\n",
      "+-------------------+-----------------+\n",
      "| Step size         | 10              |\n",
      "+-------------------+-----------------+\n",
      "| Time intvl        | 5               |\n",
      "+-------------------+-----------------+\n",
      "| Weight decay rate | 0.001           |\n",
      "+-------------------+-----------------+\n"
     ]
    }
   ],
   "source": [
    "tab_printer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    " # For stable experiment results\n",
    "set_env(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Running in Nvidia GPU (CUDA) or CPU\n",
    "if args.enable_cuda and torch.cuda.is_available():\n",
    "    # Set available CUDA devices\n",
    "    # This option is crucial for multiple GPUs\n",
    "    # 'cuda' ≡ 'cuda:0'\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [64, 16, 64], [64, 16, 64], [128, 128], [1]]\n"
     ]
    }
   ],
   "source": [
    "Ko = args.n_his - (args.Kt - 1) * 2 * args.stblock_num\n",
    "\n",
    "# blocks: settings of channel size in st_conv_blocks and output layer,\n",
    "# using the bottleneck design in st_conv_blocks\n",
    "blocks = []\n",
    "blocks.append([1])\n",
    "for l in range(args.stblock_num):\n",
    "    blocks.append([64, 16, 64])\n",
    "if Ko == 0:\n",
    "    blocks.append([128])\n",
    "elif Ko > 0:\n",
    "    blocks.append([128, 128])\n",
    "\n",
    "blocks.append([1])\n",
    "\n",
    "print(blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparate(args, device):    \n",
    "    adj, n_vertex = dataloader.load_adj(args.dataset)\n",
    "    gso = utility.calc_gso(adj, args.gso_type)\n",
    "    if args.graph_conv_type == 'cheb_graph_conv':\n",
    "        gso = utility.calc_chebynet_gso(gso)\n",
    "    gso = gso.toarray()\n",
    "    gso = gso.astype(dtype=np.float32)\n",
    "    args.gso = torch.from_numpy(gso).to(device)\n",
    "\n",
    "    dataset_path = './data'\n",
    "    dataset_path = os.path.join(dataset_path, args.dataset)\n",
    "    data_col = pd.read_csv(os.path.join(dataset_path, 'vel.csv')).shape[0]\n",
    "    # recommended dataset split rate as train: val: test = 60: 20: 20, 70: 15: 15 or 80: 10: 10\n",
    "    # using dataset split rate as train: val: test = 70: 15: 15\n",
    "    val_and_test_rate = 0.15\n",
    "\n",
    "    len_val = int(math.floor(data_col * val_and_test_rate))\n",
    "    len_test = int(math.floor(data_col * val_and_test_rate))\n",
    "    len_train = int(data_col - len_val - len_test)\n",
    "\n",
    "    train, val, test = dataloader.load_data(args.dataset, len_train, len_val)\n",
    "    zscore = preprocessing.StandardScaler()\n",
    "    train = zscore.fit_transform(train)\n",
    "    val = zscore.transform(val)\n",
    "    test = zscore.transform(test)\n",
    "\n",
    "    x_train, y_train = dataloader.data_transform(train, args.n_his, args.n_pred, device)\n",
    "    x_val, y_val = dataloader.data_transform(val, args.n_his, args.n_pred, device)\n",
    "    x_test, y_test = dataloader.data_transform(test, args.n_his, args.n_pred, device)\n",
    "\n",
    "    train_data = utils.data.TensorDataset(x_train, y_train)\n",
    "    train_iter = utils.data.DataLoader(dataset=train_data, batch_size=args.batch_size, shuffle=False)\n",
    "    val_data = utils.data.TensorDataset(x_val, y_val)\n",
    "    val_iter = utils.data.DataLoader(dataset=val_data, batch_size=args.batch_size, shuffle=False)\n",
    "    test_data = utils.data.TensorDataset(x_test, y_test)\n",
    "    test_iter = utils.data.DataLoader(dataset=test_data, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    return n_vertex, zscore, train_iter, val_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vertex, zscore, train_iter, val_iter, test_iter = data_preparate(args, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(args, blocks, n_vertex):\n",
    "    loss = nn.MSELoss()\n",
    "    es = earlystopping.EarlyStopping(mode='min', min_delta=0.0, patience=args.patience)\n",
    "\n",
    "    if args.graph_conv_type == 'cheb_graph_conv':\n",
    "        model = models.STGCNChebGraphConv(args, blocks, n_vertex).to(device)\n",
    "    else:\n",
    "        model = models.STGCNGraphConv(args, blocks, n_vertex).to(device)\n",
    "\n",
    "    if args.opt == \"rmsprop\":\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=args.lr, weight_decay=args.weight_decay_rate)\n",
    "    elif args.opt == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay_rate, amsgrad=False)\n",
    "    elif args.opt == \"adamw\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay_rate, amsgrad=False)\n",
    "    else:\n",
    "        raise NotImplementedError(f'ERROR: The optimizer {args.opt} is not implemented.')\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "\n",
    "    return loss, es, model, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, es, model, optimizer, scheduler = prepare_model(args, blocks, n_vertex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val(model, val_iter):\n",
    "    model.eval()\n",
    "    l_sum, n = 0.0, 0\n",
    "    for x, y in val_iter:\n",
    "        y_pred = model(x).view(len(x), -1)\n",
    "        l = loss(y_pred, y)\n",
    "        l_sum += l.item() * y.shape[0]\n",
    "        n += y.shape[0]\n",
    "    return torch.tensor(l_sum / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loss, args, optimizer, scheduler, es, model, train_iter, val_iter):\n",
    "    for epoch in range(args.epochs):\n",
    "        l_sum, n = 0.0, 0  # 'l_sum' is epoch sum loss, 'n' is epoch instance number\n",
    "        model.train()\n",
    "        for x, y in tqdm.tqdm(train_iter):\n",
    "            y_pred = model(x).view(len(x), -1)  # [batch_size, num_nodes]\n",
    "            l = loss(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            n += y.shape[0]\n",
    "        scheduler.step()\n",
    "        val_loss = val(model, val_iter)\n",
    "        # GPU memory usage\n",
    "        gpu_mem_alloc = torch.cuda.max_memory_allocated() / 1000000 if torch.cuda.is_available() else 0\n",
    "        print('Epoch: {:03d} | Lr: {:.20f} |Train loss: {:.6f} | Val loss: {:.6f} | GPU occupy: {:.6f} MiB'.\\\n",
    "            format(epoch+1, optimizer.param_groups[0]['lr'], l_sum / n, val_loss, gpu_mem_alloc))\n",
    "\n",
    "        if es.step(val_loss):\n",
    "            print('Early stopping.')\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [09:53<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 | Lr: 0.00100000000000000002 |Train loss: 0.351105 | Val loss: 0.453681 | GPU occupy: 0.000000 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [09:11<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002 | Lr: 0.00100000000000000002 |Train loss: 0.282300 | Val loss: 0.344895 | GPU occupy: 0.000000 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 750/750 [09:31<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003 | Lr: 0.00100000000000000002 |Train loss: 0.275790 | Val loss: 0.354051 | GPU occupy: 0.000000 MiB\n"
     ]
    }
   ],
   "source": [
    "args.epochs = 3\n",
    "train(loss, args, optimizer, scheduler, es, model, train_iter, val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() \n",
    "def test(zscore, loss, model, test_iter, args):\n",
    "    model.eval()\n",
    "    test_MSE = utility.evaluate_model(model, loss, test_iter)\n",
    "    test_MAE, test_RMSE, test_WMAPE = utility.evaluate_metric(model, test_iter, zscore)\n",
    "    print(f'Dataset {args.dataset:s} | Test loss {test_MSE:.6f} | MAE {test_MAE:.6f} | RMSE {test_RMSE:.6f} | WMAPE {test_WMAPE:.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset metr-la | Test loss 0.408878 | MAE 6.813248 | RMSE 11.466577 | WMAPE 0.13412205\n"
     ]
    }
   ],
   "source": [
    "test(zscore, loss, model, test_iter, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
