{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BiNE代码运行测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入所需库函数和文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser, FileType, ArgumentDefaultsHelpFormatter\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from data_utils import DataUtils\n",
    "from graph_utils import GraphUtils\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score,auc,precision_recall_fscore_support\n",
    "from functools import cmp_to_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 兼容python2的sorted函数修改"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp_for_py3(a,b):\n",
    "    return bool(a>b)-bool(a<b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化节点embedding\n",
    "def init_embedding_vectors(node_u, node_v, node_list_u, node_list_v, args):\n",
    "    \"\"\"\n",
    "    initialize embedding vectors\n",
    "    :param node_u:\n",
    "    :param node_v:\n",
    "    :param node_list_u:\n",
    "    :param node_list_v:\n",
    "    :param args:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # user\n",
    "    for i in node_u:\n",
    "        vectors = np.random.random([1, args.d])\n",
    "        help_vectors = np.random.random([1, args.d])\n",
    "        node_list_u[i] = {}\n",
    "        node_list_u[i]['embedding_vectors'] = preprocessing.normalize(vectors, norm='l2')\n",
    "        node_list_u[i]['context_vectors'] = preprocessing.normalize(help_vectors, norm='l2')\n",
    "    # item\n",
    "    for i in node_v:\n",
    "        vectors = np.random.random([1, args.d])\n",
    "        help_vectors = np.random.random([1, args.d])\n",
    "        node_list_v[i] = {}\n",
    "        node_list_v[i]['embedding_vectors'] = preprocessing.normalize(vectors, norm='l2')\n",
    "        node_list_v[i]['context_vectors'] = preprocessing.normalize(help_vectors, norm='l2')\n",
    "\n",
    "\n",
    "# 生成随机游走\n",
    "def walk_generator(gul,args): \n",
    "    \"\"\"\n",
    "    walk generator\n",
    "    :param gul:\n",
    "    :param args:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    gul.calculate_centrality(args.mode) # 计算图节点中心性\n",
    "\n",
    "    if args.large == 0:\n",
    "        gul.homogeneous_graph_random_walks(percentage=args.p, maxT=args.maxT, minT=args.minT)\n",
    "    elif args.large == 1:\n",
    "        gul.homogeneous_graph_random_walks_for_large_bipartite_graph(percentage=args.p, maxT=args.maxT, minT=args.minT)\n",
    "    elif args.large == 2:\n",
    "        gul.homogeneous_graph_random_walks_for_large_bipartite_graph_without_generating(datafile=args.train_data,percentage=args.p,maxT=args.maxT, minT=args.minT)\n",
    "    \n",
    "    return gul\n",
    "\n",
    "# 构造邻居节点和负采样节点\n",
    "def get_context_and_negative_samples(gul, args):\n",
    "    \"\"\"\n",
    "    get context and negative samples offline\n",
    "    :param gul:\n",
    "    :param args:\n",
    "    :return: context_dict_u, neg_dict_u, context_dict_v, neg_dict_v,gul.node_u,gul.node_v\n",
    "    \"\"\"\n",
    "    if args.large == 0:\n",
    "        neg_dict_u, neg_dict_v = gul.get_negs(args.ns) # 得到负采样结果（字典）\n",
    "        print(\"negative samples is ok.....\")\n",
    "        # 得到上下文节点和负采样节点\n",
    "        context_dict_u, neg_dict_u = gul.get_context_and_negatives(gul.G_u, gul.walks_u, args.ws, args.ns, neg_dict_u) \n",
    "        context_dict_v, neg_dict_v = gul.get_context_and_negatives(gul.G_v, gul.walks_v, args.ws, args.ns, neg_dict_v)\n",
    "    else:\n",
    "        neg_dict_u, neg_dict_v = gul.get_negs(args.ns)\n",
    "        # print len(gul.walks_u),len(gul.walks_u)\n",
    "        print(\"negative samples is ok.....\")\n",
    "        context_dict_u, neg_dict_u = gul.get_context_and_negatives(gul.node_u, gul.walks_u, args.ws, args.ns, neg_dict_u)\n",
    "        context_dict_v, neg_dict_v = gul.get_context_and_negatives(gul.node_v, gul.walks_v, args.ws, args.ns, neg_dict_v)\n",
    "\n",
    "    return context_dict_u, neg_dict_u, context_dict_v, neg_dict_v,gul.node_u,gul.node_v\n",
    "\n",
    "\n",
    "def skip_gram(center, contexts, negs, node_list, lam, pa):\n",
    "    \"\"\"\n",
    "    skip-gram\n",
    "    :param center:\n",
    "    :param contexts:\n",
    "    :param negs:\n",
    "    :param node_list:\n",
    "    :param lam:\n",
    "    :param pa:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    I_z = {center: 1}  # indication function\n",
    "    for node in negs:\n",
    "        I_z[node] = 0\n",
    "    V = np.array(node_list[contexts]['embedding_vectors'])\n",
    "    update = [[0] * V.size]\n",
    "    for u in I_z.keys():\n",
    "        if node_list.get(u) is  None:\n",
    "            pass\n",
    "        Theta = np.array(node_list[u]['context_vectors'])\n",
    "        X = float(V.dot(Theta.T))\n",
    "        sigmod = 1.0 / (1 + (math.exp(-X * 1.0)))\n",
    "        update += pa * lam * (I_z[u] - sigmod) * Theta\n",
    "        node_list[u]['context_vectors'] += pa * lam * (I_z[u] - sigmod) * V\n",
    "        try:\n",
    "            loss += pa * (I_z[u] * math.log(sigmod) + (1 - I_z[u]) * math.log(1 - sigmod))\n",
    "        except:\n",
    "            pass\n",
    "            # print \"skip_gram:\",\n",
    "            # print(V,Theta,sigmod,X,math.exp(-X * 1.0),round(math.exp(-X * 1.0),10))\n",
    "    return update, loss\n",
    "\n",
    "\n",
    "def KL_divergence(edge_dict_u, u, v, node_list_u, node_list_v, lam, gamma):\n",
    "    \"\"\"\n",
    "    KL-divergenceO1\n",
    "    :param edge_dict_u:\n",
    "    :param u:\n",
    "    :param v:\n",
    "    :param node_list_u:\n",
    "    :param node_list_v:\n",
    "    :param lam:\n",
    "    :param gamma:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    loss = 0\n",
    "    e_ij = edge_dict_u[u][v]\n",
    "\n",
    "    update_u = 0\n",
    "    update_v = 0\n",
    "    U = np.array(node_list_u[u]['embedding_vectors'])\n",
    "    V = np.array(node_list_v[v]['embedding_vectors'])\n",
    "    X = float(U.dot(V.T))\n",
    "\n",
    "    sigmod = 1.0 / (1 + (math.exp(-X * 1.0)))\n",
    "\n",
    "    update_u += gamma * lam * ((e_ij * (1 - sigmod)) * 1.0 / math.log(math.e, math.e)) * V\n",
    "    update_v += gamma * lam * ((e_ij * (1 - sigmod)) * 1.0 / math.log(math.e, math.e)) * U\n",
    "\n",
    "    try:\n",
    "        loss += gamma * e_ij * math.log(sigmod)\n",
    "    except:\n",
    "        pass\n",
    "        # print \"KL:\",\n",
    "        # print(U,V,sigmod,X,math.exp(-X * 1.0),round(math.exp(-X * 1.0),10))\n",
    "    return update_u, update_v, loss\n",
    "\n",
    "def top_N(test_u, test_v, test_rate, node_list_u, node_list_v, top_n):\n",
    "    recommend_dict = {}\n",
    "    for u in test_u:\n",
    "        recommend_dict[u] = {}\n",
    "        for v in test_v:\n",
    "            if node_list_u.get(u) is None:\n",
    "                pre = 0\n",
    "            else:\n",
    "                U = np.array(node_list_u[u]['embedding_vectors'])\n",
    "                if node_list_v.get(v) is None:\n",
    "                    pre = 0\n",
    "                else:\n",
    "                    V = np.array(node_list_v[v]['embedding_vectors'])\n",
    "                    pre = U.dot(V.T)[0][0]\n",
    "            recommend_dict[u][v] = float(pre)\n",
    "\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    ap_list = []\n",
    "    ndcg_list = []\n",
    "    rr_list = []\n",
    "\n",
    "    for u in test_u:\n",
    "        tmp_r = sorted(recommend_dict[u].items(), key = cmp_to_key(lambda x, y: cmp_for_py3(x[1], y[1])), reverse=True)[0:min(len(recommend_dict[u]),top_n)]\n",
    "        tmp_t = sorted(test_rate[u].items(), key = cmp_to_key(lambda x, y: cmp_for_py3(x[1], y[1])), reverse=True)[0:min(len(test_rate[u]),top_n)]\n",
    "        tmp_r_list = []\n",
    "        tmp_t_list = []\n",
    "        for (item, rate) in tmp_r:\n",
    "            tmp_r_list.append(item)\n",
    "\n",
    "        for (item, rate) in tmp_t:\n",
    "            tmp_t_list.append(item)\n",
    "        pre, rec = precision_and_recall(tmp_r_list,tmp_t_list)\n",
    "        ap = AP(tmp_r_list,tmp_t_list)\n",
    "        rr = RR(tmp_r_list,tmp_t_list)\n",
    "        ndcg = nDCG(tmp_r_list,tmp_t_list)\n",
    "        precision_list.append(pre)\n",
    "        recall_list.append(rec)\n",
    "        ap_list.append(ap)\n",
    "        rr_list.append(rr)\n",
    "        ndcg_list.append(ndcg)\n",
    "    precison = sum(precision_list) / len(precision_list)\n",
    "    recall = sum(recall_list) / len(recall_list)\n",
    "    #print(precison, recall)\n",
    "    f1 = 2 * precison * recall / (precison + recall)\n",
    "    map = sum(ap_list) / len(ap_list)\n",
    "    mrr = sum(rr_list) / len(rr_list)\n",
    "    mndcg = sum(ndcg_list) / len(ndcg_list)\n",
    "    return f1,map,mrr,mndcg\n",
    "\n",
    "\n",
    "def nDCG(ranked_list, ground_truth):\n",
    "    dcg = 0\n",
    "    idcg = IDCG(len(ground_truth))\n",
    "    for i in range(len(ranked_list)):\n",
    "        id = ranked_list[i]\n",
    "        if id not in ground_truth:\n",
    "            continue\n",
    "        rank = i+1\n",
    "        dcg += 1/ math.log(rank+1, 2)\n",
    "    return dcg / idcg\n",
    "\n",
    "\n",
    "def IDCG(n):\n",
    "    idcg = 0\n",
    "    for i in range(n):\n",
    "        idcg += 1 / math.log(i+2, 2)\n",
    "    return idcg\n",
    "\n",
    "\n",
    "def AP(ranked_list, ground_truth):\n",
    "    hits, sum_precs = 0, 0.0\n",
    "    for i in range(len(ranked_list)):\n",
    "        id = ranked_list[i]\n",
    "        if id in ground_truth:\n",
    "            hits += 1\n",
    "            sum_precs += hits / (i+1.0)\n",
    "    if hits > 0:\n",
    "        return sum_precs / len(ground_truth)\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def RR(ranked_list, ground_list):\n",
    "\n",
    "    for i in range(len(ranked_list)):\n",
    "        id = ranked_list[i]\n",
    "        if id in ground_list:\n",
    "            return 1 / (i + 1.0)\n",
    "    return 0\n",
    "\n",
    "def precision_and_recall(ranked_list,ground_list):\n",
    "    hits = 0\n",
    "    for i in range(len(ranked_list)):\n",
    "        id = ranked_list[i]\n",
    "        if id in ground_list:\n",
    "            hits += 1\n",
    "    pre = hits/(1.0 * len(ranked_list))\n",
    "    rec = hits/(1.0 * len(ground_list))\n",
    "    return pre, rec\n",
    "\n",
    "\n",
    "def generateFeatureFile(filecase,filevector_u,filevector_v,fileout,factors):\n",
    "    vectors_u = {}\n",
    "    vectors_v = {}\n",
    "    with open(filevector_u,'r') as fu:\n",
    "        for line in fu.readlines():\n",
    "            items = line.strip().split(' ')\n",
    "            vectors_u[items[0]] = items[1:]\n",
    "    with open(filevector_v,'r') as fv:\n",
    "        for line in fv.readlines():\n",
    "            items = line.strip().split(' ')\n",
    "            vectors_v[items[0]] = items[1:]\n",
    "    with open(filecase,'r') as fc, open(fileout,'w') as fo:\n",
    "        for line in fc.readlines():\n",
    "            items = line.strip().split('\\t')\n",
    "            if vectors_u.get(items[0]) == None:\n",
    "                vectors_u[items[0]] = ['0'] * factors\n",
    "            if vectors_v.get(items[1]) == None:\n",
    "                vectors_v[items[1]] = ['0'] * factors\n",
    "            if items[-1] == '1':\n",
    "                fo.write('{}\\t{}\\t{}\\n'.format('\\t'.join(vectors_u[items[0]]),'\\t'.join(vectors_v[items[1]]),1))\n",
    "            else:\n",
    "                fo.write('{}\\t{}\\t{}\\n'.format('\\t'.join(vectors_u[items[0]]),'\\t'.join(vectors_v[items[1]]),0))\n",
    "\n",
    "\n",
    "def link_prediction(args):\n",
    "    filecase_a = args.case_train\n",
    "    filecase_e = args.case_test\n",
    "    filevector_u = args.vectors_u\n",
    "    filevector_v = args.vectors_v\n",
    "    filecase_a_c = r'../data/features_train.dat'\n",
    "    filecase_e_c = r'../data/features_test.dat'\n",
    "    generateFeatureFile(filecase_a,filevector_u,filevector_v,filecase_a_c,args.d)\n",
    "    generateFeatureFile(filecase_e,filevector_u,filevector_v,filecase_e_c,args.d)\n",
    "\n",
    "    df_data_train = pd.read_csv(filecase_a_c,header = None,sep='\\t',encoding='utf-8')\n",
    "    X_train = df_data_train.drop(len(df_data_train.keys())-1,axis = 1)\n",
    "    y_train = df_data_train[len(df_data_train.keys())-1]\n",
    "\n",
    "    df_data_test = pd.read_csv(filecase_e_c,header = None,sep='\\t',encoding='utf-8')\n",
    "    X_test = df_data_test.drop(len(df_data_train.keys())-1,axis = 1)\n",
    "    X_test = X_test.fillna(X_test.mean())\n",
    "    y_test = df_data_test[len(df_data_test.keys())-1]\n",
    "    y_test_list = list(y_test)\n",
    "\n",
    "    lg = LogisticRegression(penalty='l2',C=0.001)\n",
    "    lg.fit(X_train,y_train)\n",
    "    lg_y_pred_est = lg.predict_proba(X_test)[:,1]\n",
    "    fpr,tpr,thresholds = metrics.roc_curve(y_test,lg_y_pred_est)\n",
    "    average_precision = average_precision_score(y_test, lg_y_pred_est)\n",
    "    os.remove(filecase_a_c)\n",
    "    os.remove(filecase_e_c)\n",
    "    return metrics.auc(fpr,tpr), average_precision\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    model_path = os.path.join('../', args.model_name)\n",
    "    if os.path.exists(model_path) is False:\n",
    "        os.makedirs(model_path)\n",
    "    alpha, beta, gamma, lam = args.alpha, args.beta, args.gamma, args.lam\n",
    "    print('======== experiment settings =========')\n",
    "    print('alpha : %0.4f, beta : %0.4f, gamma : %0.4f, lam : %0.4f, p : %0.4f, ws : %d, ns : %d, maxT : % d, minT : %d, max_iter : %d, d : %d' % (alpha, beta, gamma, lam, args.p, args.ws, args.ns,args.maxT,args.minT,args.max_iter, args.d))\n",
    "    print('========== processing data ===========')\n",
    "    dul = DataUtils(model_path)\n",
    "    if args.rec:\n",
    "        test_user, test_item, test_rate = dul.read_data(args.test_data)\n",
    "    print(\"constructing graph....\")\n",
    "    gul = GraphUtils(model_path)\n",
    "    gul.construct_training_graph(args.train_data)\n",
    "    edge_dict_u = gul.edge_dict_u\n",
    "    edge_list = gul.edge_list\n",
    "    walk_generator(gul,args)\n",
    "\n",
    "    print(\"getting context and negative samples....\")\n",
    "    context_dict_u, neg_dict_u, context_dict_v, neg_dict_v, node_u, node_v = get_context_and_negative_samples(gul, args)\n",
    "    node_list_u, node_list_v = {}, {}\n",
    "    init_embedding_vectors(node_u, node_v, node_list_u, node_list_v, args)\n",
    "\n",
    "    last_loss, count, epsilon = 0, 0, 1e-3\n",
    "    print(\"============== training ==============\")\n",
    "    for iter in range(0, args.max_iter):\n",
    "        s1 = \"\\r[%s%s]%0.2f%%\"%(\"*\"* iter,\" \"*(args.max_iter-iter),iter*100.0/(args.max_iter-1))\n",
    "        loss = 0\n",
    "        num = 0\n",
    "        visited_u = dict(zip(node_list_u.keys(), [0] * len(node_list_u.keys())))\n",
    "        visited_v = dict(zip(node_list_v.keys(), [0] * len(node_list_v.keys())))\n",
    "\n",
    "        random.shuffle(edge_list)\n",
    "        for (u, v, w) in edge_list:\n",
    "            if visited_u.get(u) == 0 or random.random() > 0.95:\n",
    "                # print(u)\n",
    "                length = len(context_dict_u[u])\n",
    "                index_list = random.sample(list(range(length)), min(length, 1))\n",
    "                for index in index_list:\n",
    "                    context_u = context_dict_u[u][index]\n",
    "                    neg_u = neg_dict_u[u][index]\n",
    "                    # center,context,neg,node_list,eta\n",
    "                    for k, z in enumerate(context_u):\n",
    "                        tmp_z, tmp_loss = skip_gram(u, z, neg_u, node_list_u, lam, alpha)\n",
    "                        node_list_u[z]['embedding_vectors'] += tmp_z\n",
    "                        loss += tmp_loss\n",
    "                visited_u[u] = 1\n",
    "            if visited_v.get(v) == 0 or random.random() > 0.95:\n",
    "                # print(v)\n",
    "                length = len(context_dict_v[v])\n",
    "                index_list = random.sample(list(range(length)), min(length, 1))\n",
    "                for index in index_list:\n",
    "                    context_v = context_dict_v[v][index]\n",
    "                    neg_v = neg_dict_v[v][index]\n",
    "                    # center,context,neg,node_list,eta\n",
    "                    for k,z in enumerate(context_v):\n",
    "                        tmp_z, tmp_loss = skip_gram(v, z, neg_v, node_list_v, lam, beta)\n",
    "                        node_list_v[z]['embedding_vectors'] += tmp_z\n",
    "                        loss += tmp_loss\n",
    "                visited_v[v] = 1\n",
    "            # print(len(edge_dict_u))\n",
    "            update_u, update_v, tmp_loss = KL_divergence(edge_dict_u, u, v, node_list_u, node_list_v, lam, gamma)\n",
    "            loss += tmp_loss\n",
    "            node_list_u[u]['embedding_vectors'] += update_u\n",
    "            node_list_v[v]['embedding_vectors'] += update_v\n",
    "            count = iter\n",
    "            num += 1\n",
    "        delta_loss = abs(loss - last_loss)\n",
    "        if last_loss > loss:\n",
    "            lam *= 1.05\n",
    "        else:\n",
    "            lam *= 0.95\n",
    "        last_loss = loss\n",
    "        if delta_loss < epsilon:\n",
    "            break\n",
    "        sys.stdout.write(s1)\n",
    "        sys.stdout.flush()\n",
    "    save_to_file(node_list_u,node_list_v,model_path,args)\n",
    "    print(\"\")\n",
    "    if args.rec:\n",
    "        print(\"============== testing ===============\")\n",
    "        f1, map, mrr, mndcg = top_N(test_user,test_item,test_rate,node_list_u,node_list_v,args.top_n)\n",
    "        print('recommendation metrics: F1 : %0.4f, MAP : %0.4f, MRR : %0.4f, NDCG : %0.4f' % (round(f1,4), round(map,4), round(mrr,4), round(mndcg,4)))\n",
    "    if args.lip:\n",
    "        print(\"============== testing ===============\")\n",
    "        auc_roc, auc_pr = link_prediction(args)\n",
    "        print('link prediction metrics: AUC_ROC : %0.4f, AUC_PR : %0.4f' % (round(auc_roc,4), round(auc_pr,4)))\n",
    "    \n",
    "\n",
    "\n",
    "def ndarray_tostring(array):\n",
    "    string = \"\"\n",
    "    for item in array[0]:\n",
    "        string += str(item).strip()+\" \"\n",
    "    return string+\"\\n\"\n",
    "\n",
    "\n",
    "def save_to_file(node_list_u,node_list_v,model_path,args):\n",
    "    with open(args.vectors_u,\"w\") as fw_u:\n",
    "        for u in node_list_u.keys():\n",
    "            fw_u.write(u+\" \"+ ndarray_tostring(node_list_u[u]['embedding_vectors']))\n",
    "    with open(args.vectors_v,\"w\") as fw_v:\n",
    "        for v in node_list_v.keys():\n",
    "            fw_v.write(v+\" \"+ndarray_tostring(node_list_v[v]['embedding_vectors']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 全局参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(alpha=0.01, beta=0.01, case_test='../data/wiki/case_test.dat', case_train='../data/wiki/case_train.dat', d=128, gamma=0.1, lam=0.01, large=0, lip=1, maxT=32, max_iter=50, minT=1, mode='hits', model_name='default', ns=4, p=0.15, rec=1, test_data='../data/wiki/rating_test.dat', top_n=10, train_data='../data/wiki/rating_train.dat', vectors_u='../data/wiki/vectors_u.dat', vectors_v='../data/wiki/vectors_v.dat', ws=5)\n"
     ]
    }
   ],
   "source": [
    "parser = ArgumentParser(\"BiNE\",\n",
    "                            formatter_class=ArgumentDefaultsHelpFormatter,\n",
    "                            conflict_handler='resolve')\n",
    "\n",
    "parser.add_argument('--train-data', default=r'../data/wiki/rating_train.dat',\n",
    "                    help='Input graph file.')\n",
    "\n",
    "parser.add_argument('--test-data', default=r'../data/wiki/rating_test.dat')\n",
    "\n",
    "parser.add_argument('--model-name', default='default',\n",
    "                    help='name of model.')\n",
    "\n",
    "parser.add_argument('--vectors-u', default=r'../data/wiki/vectors_u.dat',\n",
    "                    help=\"file of embedding vectors of U\")\n",
    "\n",
    "parser.add_argument('--vectors-v', default=r'../data/wiki/vectors_v.dat',\n",
    "                    help=\"file of embedding vectors of V\")\n",
    "\n",
    "parser.add_argument('--case-train', default=r'../data/wiki/case_train.dat',\n",
    "                    help=\"file of training data for LR\")\n",
    "\n",
    "parser.add_argument('--case-test', default=r'../data/wiki/case_test.dat',\n",
    "                    help=\"file of testing data for LR\")\n",
    "\n",
    "parser.add_argument('--ws', default=5, type=int,\n",
    "                    help='window size.')\n",
    "\n",
    "parser.add_argument('--ns', default=4, type=int,\n",
    "                    help='number of negative samples.')\n",
    "\n",
    "parser.add_argument('--d', default=128, type=int,\n",
    "                    help='embedding size.')\n",
    "\n",
    "parser.add_argument('--maxT', default=32, type=int,\n",
    "                    help='maximal walks per vertex.')\n",
    "\n",
    "parser.add_argument('--minT', default=1, type=int,\n",
    "                    help='minimal walks per vertex.')\n",
    "\n",
    "parser.add_argument('--p', default=0.15, type=float,\n",
    "                    help='walk stopping probability.')\n",
    "\n",
    "parser.add_argument('--alpha', default=0.01, type=float,\n",
    "                    help='trade-off parameter alpha.')\n",
    "\n",
    "parser.add_argument('--beta', default=0.01, type=float,\n",
    "                    help='trade-off parameter beta.')\n",
    "\n",
    "parser.add_argument('--gamma', default=0.1, type=float,\n",
    "                    help='trade-off parameter gamma.')\n",
    "\n",
    "parser.add_argument('--lam', default=0.01, type=float,\n",
    "                    help='learning rate lambda.')\n",
    "parser.add_argument('--max-iter', default=50, type=int,\n",
    "                    help='maximal number of iterations.')\n",
    "\n",
    "parser.add_argument('--top-n', default=10, type=int,\n",
    "                    help='recommend top-n items for each user.')\n",
    "\n",
    "parser.add_argument('--rec', default=1, type=int,\n",
    "                    help='calculate the recommendation metrics.')\n",
    "\n",
    "parser.add_argument('--lip', default=1, type=int,\n",
    "                    help='calculate the link prediction metrics.')\n",
    "\n",
    "parser.add_argument('--large', default=0, type=int,\n",
    "                    help='for large bipartite, 1 do not generate homogeneous graph file; 2 do not generate homogeneous graph')\n",
    "\n",
    "parser.add_argument('--mode', default='hits', type=str,\n",
    "                    help='metrics of centrality')\n",
    "\n",
    "#args = parser.parse_args()                                               # pychram 中使用\n",
    "args = parser.parse_known_args()[0]                                       # jupyter 中使用\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_by_sampling(args):\n",
    "    model_path = os.path.join('../', args.model_name)\n",
    "    if os.path.exists(model_path) is False:\n",
    "        os.makedirs(model_path)\n",
    "    alpha, beta, gamma, lam = args.alpha, args.beta, args.gamma, args.lam # 超参数设置\n",
    "    print('======== experiment settings =========')\n",
    "    print('alpha : %0.4f, beta : %0.4f, gamma : %0.4f, lam : %0.4f, p : %0.4f, ws : %d, ns : %d, maxT : % d, minT : %d, max_iter : %d, d : %d' % (alpha, beta, gamma, lam, args.p, args.ws, args.ns,args.maxT,args.minT,args.max_iter, args.d))\n",
    "    print('========== processing data ===========')\n",
    "    dul = DataUtils(model_path)\n",
    "    if args.rec:\n",
    "        test_user, test_item, test_rate = dul.read_data(args.test_data)\n",
    "    print(\"constructing graph....\")\n",
    "    gul = GraphUtils(model_path)\n",
    "    gul.construct_training_graph(args.train_data)\n",
    "    edge_dict_u = gul.edge_dict_u\n",
    "    edge_list = gul.edge_list\n",
    "    walk_generator(gul,args)\n",
    "    print(\"getting context and negative samples....\")\n",
    "    context_dict_u, neg_dict_u, context_dict_v, neg_dict_v, node_u, node_v = get_context_and_negative_samples(gul, args)\n",
    "    node_list_u, node_list_v = {}, {}\n",
    "    init_embedding_vectors(node_u, node_v, node_list_u, node_list_v, args)\n",
    "    last_loss, count, epsilon = 0, 0, 1e-3\n",
    " \n",
    "    print(\"============== training ==============\")\n",
    "    for iter in range(0, args.max_iter):\n",
    "        s1 = \"\\r[%s%s]%0.2f%%\"%(\"*\"* iter,\" \"*(args.max_iter-iter),iter*100.0/(args.max_iter-1))\n",
    "        loss = 0\n",
    "        visited_u = dict(zip(node_list_u.keys(), [0] * len(node_list_u.keys())))\n",
    "        visited_v = dict(zip(node_list_v.keys(), [0] * len(node_list_v.keys())))\n",
    "        random.shuffle(edge_list)\n",
    "        for i in range(len(edge_list)):\n",
    "            u, v, w = edge_list[i]\n",
    "              \n",
    "            length = len(context_dict_u[u])\n",
    "            random.shuffle(context_dict_u[u])\n",
    "            if visited_u.get(u) < length:\n",
    "                # print(u)\n",
    "                index_list = list(range(visited_u.get(u),min(visited_u.get(u)+1,length)))\n",
    "                for index in index_list:\n",
    "                    context_u = context_dict_u[u][index]\n",
    "                    neg_u = neg_dict_u[u][index]\n",
    "                    # center,context,neg,node_list,eta\n",
    "                    for z in context_u:\n",
    "                        tmp_z, tmp_loss = skip_gram(u, z, neg_u, node_list_u, lam, alpha)\n",
    "                        node_list_u[z]['embedding_vectors'] += tmp_z\n",
    "                        loss += tmp_loss\n",
    "                visited_u[u] = index_list[-1]+3\n",
    "\n",
    "            length = len(context_dict_v[v])\n",
    "            random.shuffle(context_dict_v[v])\n",
    "            if visited_v.get(v) < length:\n",
    "                # print(v)\n",
    "                index_list = list(range(visited_v.get(v),min(visited_v.get(v)+1,length)))\n",
    "                for index in index_list:\n",
    "                    context_v = context_dict_v[v][index]\n",
    "                    neg_v = neg_dict_v[v][index]\n",
    "                    # center,context,neg,node_list,eta\n",
    "                    for z in context_v:\n",
    "                        tmp_z, tmp_loss = skip_gram(v, z, neg_v, node_list_v, lam, beta)\n",
    "                        node_list_v[z]['embedding_vectors'] += tmp_z\n",
    "                        loss += tmp_loss\n",
    "                visited_v[v] = index_list[-1]+3\n",
    "\n",
    "            update_u, update_v, tmp_loss = KL_divergence(edge_dict_u, u, v, node_list_u, node_list_v, lam, gamma)\n",
    "            loss += tmp_loss\n",
    "            node_list_u[u]['embedding_vectors'] += update_u\n",
    "            node_list_v[v]['embedding_vectors'] += update_v\n",
    "\n",
    "        delta_loss = abs(loss - last_loss)\n",
    "        if last_loss > loss:\n",
    "            lam *= 1.05\n",
    "        else:\n",
    "            lam *= 0.95\n",
    "        last_loss = loss\n",
    "        if delta_loss < epsilon:\n",
    "            break\n",
    "        sys.stdout.write(s1)\n",
    "        sys.stdout.flush()\n",
    "    save_to_file(node_list_u,node_list_v,model_path,args)\n",
    "    print(\"\")\n",
    "    if args.rec:\n",
    "        print(\"============== testing ===============\")\n",
    "        f1, map, mrr, mndcg = top_N(test_user,test_item,test_rate,node_list_u,node_list_v,args.top_n)\n",
    "        print('recommendation metrics: F1 : %0.4f, MAP : %0.4f, MRR : %0.4f, NDCG : %0.4f' % (round(f1,4), round(map,4), round(mrr,4), round(mndcg,4)))\n",
    "    if args.lip:\n",
    "        print(\"============== testing ===============\")\n",
    "        auc_roc, auc_pr = link_prediction(args)\n",
    "        print('link prediction metrics: AUC_ROC : %0.4f, AUC_PR : %0.4f' % (round(auc_roc,4), round(auc_pr,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== experiment settings =========\n",
      "alpha : 0.0100, beta : 0.0100, gamma : 0.1000, lam : 0.0100, p : 0.1500, ws : 5, ns : 4, maxT :  32, minT : 1, max_iter : 50, d : 128\n",
      "========== processing data ===========\n",
      "constructing graph....\n",
      "number of nodes: 15000\n",
      "walking...\n",
      "walking...ok\n",
      "number of nodes: 2529\n",
      "walking...\n",
      "walking...ok\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<graph_utils.GraphUtils at 0x1edb2331d48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = os.path.join('../', args.model_name) # 模型路径\n",
    "if os.path.exists(model_path) is False:\n",
    "    os.makedirs(model_path)\n",
    "alpha, beta, gamma, lam = args.alpha, args.beta, args.gamma, args.lam # 超参数设置\n",
    "print('======== experiment settings =========')\n",
    "print('alpha : %0.4f, beta : %0.4f, gamma : %0.4f, lam : %0.4f, p : %0.4f, ws : %d, ns : %d, maxT : % d, minT : %d, max_iter : %d, d : %d' % (alpha, beta, gamma, lam, args.p, args.ws, args.ns,args.maxT,args.minT,args.max_iter, args.d))\n",
    "\n",
    "print('========== processing data ===========')\n",
    "dul = DataUtils(model_path)\n",
    "if args.rec: # 导入测试集数据\n",
    "    test_user, test_item, test_rate = dul.read_data(args.test_data)\n",
    "\n",
    "print(\"constructing graph....\") # 构造二部图\n",
    "gul = GraphUtils(model_path) # 初始化二部图类\n",
    "gul.construct_training_graph(args.train_data) # 构造二部图,导入数据\n",
    "edge_dict_u = gul.edge_dict_u # 二部图的所有u节点\n",
    "edge_list = gul.edge_list # 所有边\n",
    "\n",
    "# 生成有条件的随机游走\n",
    "walk_generator(gul, args) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting context and negative samples....\n",
      "negative samples is ok.....\n",
      "context...\n",
      "context...ok\n",
      "context...\n",
      "context...ok\n"
     ]
    }
   ],
   "source": [
    "print(\"getting context and negative samples....\")\n",
    "context_dict_u, neg_dict_u, context_dict_v, neg_dict_v, node_u, node_v = get_context_and_negative_samples(gul, args) # 构造节点邻居和负采样集合\n",
    "node_list_u, node_list_v = {}, {}\n",
    "# 初始化节点embedding\n",
    "init_embedding_vectors(node_u, node_v, node_list_u, node_list_v, args)\n",
    "last_loss, count, epsilon = 0, 0, 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== training ==============\n",
      "[*                                                 ]2.04%"
     ]
    }
   ],
   "source": [
    "print(\"============== training ==============\")\n",
    "# for iter in range(0, args.max_iter):\n",
    "for iter in range(0, 2):\n",
    "    s1 = \"\\r[%s%s]%0.2f%%\"%(\"*\"* iter,\" \"*(args.max_iter-iter),iter*100.0/(args.max_iter-1))\n",
    "    loss = 0\n",
    "    visited_u = dict(zip(node_list_u.keys(), [0] * len(node_list_u.keys())))\n",
    "    visited_v = dict(zip(node_list_v.keys(), [0] * len(node_list_v.keys())))\n",
    "    random.shuffle(edge_list) # 点边信息打乱\n",
    "    for i in range(len(edge_list)):\n",
    "        u, v, w = edge_list[i]\n",
    "            \n",
    "        length = len(context_dict_u[u]) # 周围邻居的数量\n",
    "        random.shuffle(context_dict_u[u])\n",
    "        if visited_u.get(u) < length:\n",
    "            # print(u)\n",
    "            index_list = list(range(visited_u.get(u),min(visited_u.get(u)+1,length)))\n",
    "            for index in index_list:\n",
    "                context_u = context_dict_u[u][index]\n",
    "                neg_u = neg_dict_u[u][index]\n",
    "                # center,context,neg,node_list,eta\n",
    "                for z in context_u:\n",
    "                    tmp_z, tmp_loss = skip_gram(u, z, neg_u, node_list_u, lam, alpha)\n",
    "                    node_list_u[z]['embedding_vectors'] += tmp_z\n",
    "                    loss += tmp_loss\n",
    "            visited_u[u] = index_list[-1]+3\n",
    "\n",
    "        length = len(context_dict_v[v])\n",
    "        random.shuffle(context_dict_v[v])\n",
    "        if visited_v.get(v) < length:\n",
    "            # print(v)\n",
    "            index_list = list(range(visited_v.get(v),min(visited_v.get(v)+1,length)))\n",
    "            for index in index_list:\n",
    "                context_v = context_dict_v[v][index]\n",
    "                neg_v = neg_dict_v[v][index]\n",
    "                # center,context,neg,node_list,eta\n",
    "                for z in context_v:\n",
    "                    tmp_z, tmp_loss = skip_gram(v, z, neg_v, node_list_v, lam, beta)\n",
    "                    node_list_v[z]['embedding_vectors'] += tmp_z\n",
    "                    loss += tmp_loss\n",
    "            visited_v[v] = index_list[-1]+3\n",
    "\n",
    "        update_u, update_v, tmp_loss = KL_divergence(edge_dict_u, u, v, node_list_u, node_list_v, lam, gamma)\n",
    "        loss += tmp_loss\n",
    "        node_list_u[u]['embedding_vectors'] += update_u\n",
    "        node_list_v[v]['embedding_vectors'] += update_v\n",
    "\n",
    "    delta_loss = abs(loss - last_loss)\n",
    "    if last_loss > loss:\n",
    "        lam *= 1.05\n",
    "    else:\n",
    "        lam *= 0.95\n",
    "    last_loss = loss\n",
    "    if delta_loss < epsilon:\n",
    "        break\n",
    "    sys.stdout.write(s1)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_file(node_list_u,node_list_v,model_path,args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== testing ===============\n",
      "recommendation metrics: F1 : 0.1570, MAP : 0.2786, MRR : 0.3253, NDCG : 0.3599\n"
     ]
    }
   ],
   "source": [
    "if args.rec:\n",
    "    print(\"============== testing ===============\")\n",
    "    f1, map, mrr, mndcg = top_N(test_user,test_item,test_rate,node_list_u,node_list_v,args.top_n)\n",
    "    print('recommendation metrics: F1 : %0.4f, MAP : %0.4f, MRR : %0.4f, NDCG : %0.4f' % (round(f1,4), round(map,4), round(mrr,4), round(mndcg,4)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== testing ===============\n",
      "link prediction metrics: AUC_ROC : 0.7436, AUC_PR : 0.8301\n"
     ]
    }
   ],
   "source": [
    "if args.lip:\n",
    "    print(\"============== testing ===============\")\n",
    "    auc_roc, auc_pr = link_prediction(args)\n",
    "    print('link prediction metrics: AUC_ROC : %0.4f, AUC_PR : %0.4f' % (round(auc_roc,4), round(auc_pr,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_by_sampling(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
