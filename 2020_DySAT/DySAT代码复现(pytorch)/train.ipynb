{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DySAT代码复现\n",
    "\n",
    "* [参考仓库链接](https://github.com/FeiGSSS/DySAT_pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入相关库文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x22ff5736760>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import dill\n",
    "import pickle as pkl\n",
    "import scipy\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.preprocess import load_graphs, get_context_pairs, get_evaluation_data\n",
    "from utils.minibatch import  MyDataset\n",
    "from utils.utilities import to_device\n",
    "from eval.link_prediction import evaluate_classifier\n",
    "from models.model import DySAT\n",
    "\n",
    "import torch\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inductive_graph(graph_former, graph_later):\n",
    "    \"\"\"Create the adj_train so that it includes nodes from (t+1) \n",
    "       but only edges from t: this is for the purpose of inductive testing.\n",
    "\n",
    "    Args:\n",
    "        graph_former ([type]): [description]\n",
    "        graph_later ([type]): [description]\n",
    "    \"\"\"\n",
    "    newG = nx.MultiGraph()\n",
    "    newG.add_nodes_from(graph_later.nodes(data=True))  # 最后一张图中的节点\n",
    "    newG.add_edges_from(graph_former.edges(data=False))  # 前一张图的边; 目的是语料中出现要预测的图中的节点\n",
    "    return newG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置全局参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_parser():\n",
    "    \"\"\"\n",
    "    A method to parse up command line parameters.\n",
    "    The default hyperparameters give a high performance model without grid search.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--time_steps', type=int, nargs='?', default=16,\n",
    "                        help=\"total time steps used for train, eval and test\")\n",
    "    # Experimental settings.\n",
    "    parser.add_argument('--dataset', type=str, nargs='?', default='Enron',\n",
    "                        help='dataset name')\n",
    "    parser.add_argument('--GPU_ID', type=int, nargs='?', default=0,\n",
    "                        help='GPU_ID (0/1 etc.)')\n",
    "    parser.add_argument('--epochs', type=int, nargs='?', default=30,\n",
    "                        help='# epochs')\n",
    "    parser.add_argument('--val_freq', type=int, nargs='?', default=1,\n",
    "                        help='Validation frequency (in epochs)')\n",
    "    parser.add_argument('--test_freq', type=int, nargs='?', default=1,\n",
    "                        help='Testing frequency (in epochs)')\n",
    "    parser.add_argument('--batch_size', type=int, nargs='?', default=512,\n",
    "                        help='Batch size (# nodes)')\n",
    "    parser.add_argument('--featureless', type=bool, nargs='?', default=True,\n",
    "                    help='True if one-hot encoding.')\n",
    "    parser.add_argument(\"--early_stop\", type=int, default=10,\n",
    "                        help=\"patient\")\n",
    "    # 1-hot encoding is input as a sparse matrix - hence no scalability issue for large datasets.\n",
    "    # Tunable hyper-params\n",
    "    # TODO: Implementation has not been verified, performance may not be good.\n",
    "    parser.add_argument('--residual', type=bool, nargs='?', default=True,\n",
    "                        help='Use residual')\n",
    "    # Number of negative samples per positive pair.\n",
    "    parser.add_argument('--neg_sample_size', type=int, nargs='?', default=10,\n",
    "                        help='# negative samples per positive')\n",
    "    # Walk length for random walk sampling.\n",
    "    parser.add_argument('--walk_len', type=int, nargs='?', default=20,\n",
    "                        help='Walk length for random walk sampling')\n",
    "    # Weight for negative samples in the binary cross-entropy loss function.\n",
    "    parser.add_argument('--neg_weight', type=float, nargs='?', default=1.0,\n",
    "                        help='Weightage for negative samples')\n",
    "    parser.add_argument('--learning_rate', type=float, nargs='?', default=0.01,\n",
    "                        help='Initial learning rate for self-attention model.')\n",
    "    parser.add_argument('--spatial_drop', type=float, nargs='?', default=0.1,\n",
    "                        help='Spatial (structural) attention Dropout (1 - keep probability).')\n",
    "    parser.add_argument('--temporal_drop', type=float, nargs='?', default=0.5,\n",
    "                        help='Temporal attention Dropout (1 - keep probability).')\n",
    "    parser.add_argument('--weight_decay', type=float, nargs='?', default=0.0005,\n",
    "                        help='Initial learning rate for self-attention model.')\n",
    "    # Architecture params\n",
    "    parser.add_argument('--structural_head_config', type=str, nargs='?', default='16,8,8',\n",
    "                        help='Encoder layer config: # attention heads in each GAT layer')\n",
    "    parser.add_argument('--structural_layer_config', type=str, nargs='?', default='128',\n",
    "                        help='Encoder layer config: # units in each GAT layer')\n",
    "    parser.add_argument('--temporal_head_config', type=str, nargs='?', default='16',\n",
    "                        help='Encoder layer config: # attention heads in each Temporal layer')\n",
    "    parser.add_argument('--temporal_layer_config', type=str, nargs='?', default='128',\n",
    "                        help='Encoder layer config: # units in each Temporal layer')\n",
    "    parser.add_argument('--position_ffn', type=str, nargs='?', default='True',\n",
    "                        help='Position wise feedforward')\n",
    "    parser.add_argument('--window', type=int, nargs='?', default=-1,\n",
    "                        help='Window for temporal attention (default : -1 => full)')\n",
    "\n",
    "    return parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可视化参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from texttable import Texttable\n",
    "\n",
    "def tab_printer(args):\n",
    "    \"\"\"\n",
    "    Function to print the logs in a nice tabular format.\n",
    "    :param args: Parameters used for the model.\n",
    "    \"\"\"\n",
    "    args = vars(args)\n",
    "    keys = sorted(args.keys())\n",
    "    t = Texttable()\n",
    "    rows = [[\"Parameter\", \"Value\"]]\n",
    "    for i in [[k.replace(\"_\", \" \").capitalize(), args[k]] for k in keys]:\n",
    "        rows.append(i)\n",
    "    # print(rows)\n",
    "    t.add_rows(rows)\n",
    "    print(t.draw())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parameter_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------+\n",
      "|        Parameter        | Value  |\n",
      "+=========================+========+\n",
      "| Gpu id                  | 0      |\n",
      "+-------------------------+--------+\n",
      "| Batch size              | 512    |\n",
      "+-------------------------+--------+\n",
      "| Dataset                 | Enron  |\n",
      "+-------------------------+--------+\n",
      "| Early stop              | 10     |\n",
      "+-------------------------+--------+\n",
      "| Epochs                  | 30     |\n",
      "+-------------------------+--------+\n",
      "| Featureless             | 1      |\n",
      "+-------------------------+--------+\n",
      "| Learning rate           | 0.010  |\n",
      "+-------------------------+--------+\n",
      "| Neg sample size         | 10     |\n",
      "+-------------------------+--------+\n",
      "| Neg weight              | 1      |\n",
      "+-------------------------+--------+\n",
      "| Position ffn            | True   |\n",
      "+-------------------------+--------+\n",
      "| Residual                | 1      |\n",
      "+-------------------------+--------+\n",
      "| Spatial drop            | 0.100  |\n",
      "+-------------------------+--------+\n",
      "| Structural head config  | 16,8,8 |\n",
      "+-------------------------+--------+\n",
      "| Structural layer config | 128    |\n",
      "+-------------------------+--------+\n",
      "| Temporal drop           | 0.500  |\n",
      "+-------------------------+--------+\n",
      "| Temporal head config    | 16     |\n",
      "+-------------------------+--------+\n",
      "| Temporal layer config   | 128    |\n",
      "+-------------------------+--------+\n",
      "| Test freq               | 1      |\n",
      "+-------------------------+--------+\n",
      "| Time steps              | 16     |\n",
      "+-------------------------+--------+\n",
      "| Val freq                | 1      |\n",
      "+-------------------------+--------+\n",
      "| Walk len                | 20     |\n",
      "+-------------------------+--------+\n",
      "| Weight decay            | 0.001  |\n",
      "+-------------------------+--------+\n",
      "| Window                  | -1     |\n",
      "+-------------------------+--------+\n"
     ]
    }
   ],
   "source": [
    "tab_printer(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16 graphs \n"
     ]
    }
   ],
   "source": [
    "#graphs, feats, adjs = load_graphs(args.dataset)\n",
    "graphs, adjs = load_graphs(args.dataset)  # 导入数据\n",
    "if args.featureless == True:  # 创建单位阵  # 最后一个时间点包括的节点数量\n",
    "    feats = [scipy.sparse.identity(adjs[args.time_steps - 1].shape[0]).tocsr()[range(0, x.shape[0]), :] for x in adjs if\n",
    "            x.shape[0] <= adjs[args.time_steps - 1].shape[0]]  # 选择需要的时间点; 构建ont-hot特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(143, 143)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjs[args.time_steps - 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 143)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 对每张图进行随机游走采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing training pairs ...\n",
      "# nodes with random walk samples: 18\n",
      "# sampled pairs: 40854\n",
      "# nodes with random walk samples: 18\n",
      "# sampled pairs: 39386\n",
      "# nodes with random walk samples: 14\n",
      "# sampled pairs: 31918\n",
      "# nodes with random walk samples: 47\n",
      "# sampled pairs: 104792\n",
      "# nodes with random walk samples: 57\n",
      "# sampled pairs: 129236\n",
      "# nodes with random walk samples: 65\n",
      "# sampled pairs: 149300\n",
      "# nodes with random walk samples: 79\n",
      "# sampled pairs: 193622\n",
      "# nodes with random walk samples: 97\n",
      "# sampled pairs: 239452\n",
      "# nodes with random walk samples: 101\n",
      "# sampled pairs: 245690\n",
      "# nodes with random walk samples: 106\n",
      "# sampled pairs: 256484\n",
      "# nodes with random walk samples: 103\n",
      "# sampled pairs: 254148\n",
      "# nodes with random walk samples: 113\n",
      "# sampled pairs: 279936\n",
      "# nodes with random walk samples: 98\n",
      "# sampled pairs: 232548\n",
      "# nodes with random walk samples: 79\n",
      "# sampled pairs: 181768\n",
      "# nodes with random walk samples: 94\n",
      "# sampled pairs: 231172\n",
      "# nodes with random walk samples: 93\n",
      "# sampled pairs: 227870\n"
     ]
    }
   ],
   "source": [
    "assert args.time_steps <= len(adjs), \"Time steps is illegal\"\n",
    "# node2vec的训练语料; 16个garph 和 16个节点特征;\n",
    "context_pairs_train = get_context_pairs(graphs, adjs)  # 16个图，每个图中进行随机游走采样;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(context_pairs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([9, 8, 0, 10, 5, 11, 4, 7, 15, 17, 3, 1, 2, 6, 16, 14, 13, 12])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_pairs_train[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# context_pairs_train[0][0] # 上下文节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating eval data ....\n",
      "No. Train: Pos=46, Neg=46 \n",
      "No. Val: Pos=46, Neg=46 \n",
      "No. Test: Pos=140, Neg=140\n"
     ]
    }
   ],
   "source": [
    " # Load evaluation data for link prediction. 只是对最后一张图中边进行了处理：如果上张图中没有该节点，则不计算这条边的label\n",
    "train_edges_pos, train_edges_neg, val_edges_pos, val_edges_neg, \\\n",
    "    test_edges_pos, test_edges_neg = get_evaluation_data(graphs)\n",
    "# 训练集、验证集、测试集\n",
    "print(\"No. Train: Pos={}, Neg={} \\nNo. Val: Pos={}, Neg={} \\nNo. Test: Pos={}, Neg={}\".format(\n",
    "    len(train_edges_pos), len(train_edges_neg), len(val_edges_pos), len(val_edges_neg),\n",
    "    len(test_edges_pos), len(test_edges_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the adj_train so that it includes nodes from (t+1) but only edges from t: this is for the purpose of; 创建包括下张图的所有节点，但只保留这一层的边\n",
    "# inductive testing.\n",
    "new_G = inductive_graph(graphs[args.time_steps-2], graphs[args.time_steps-1])  # 下一层的所有点，都放到数据集中\n",
    "graphs[args.time_steps-2] = new_G\n",
    "adjs[args.time_steps-2] = nx.adjacency_matrix(new_G)\n",
    "feats[args.time_steps-2] = feats[args.time_steps-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建dataloader和DySAT模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataloader and model\n",
    "device = torch.device('cuda' if torch.cuda.is_available()  else 'cpu')\n",
    "dataset = MyDataset(args, graphs, feats, adjs, context_pairs_train)\n",
    "\n",
    "dataloader = DataLoader(dataset,  # 定义dataloader\n",
    "                        batch_size=args.batch_size, \n",
    "                        shuffle=True, \n",
    "                        # num_workers=10, \n",
    "                        collate_fn=MyDataset.collate_fn)\n",
    "#dataloader = NodeMinibatchIterator(args, graphs, feats, adjs, context_pairs_train, device) \n",
    "model = DySAT(args, feats[0].shape[1], args.time_steps).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, feed_dict in enumerate(dataloader):\n",
    "#     print(idx, feed_dict)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:02<01:14,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  ,  Loss = 39.316, Val AUC 0.624 Test AUC 0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:24<00:49,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 ,  Loss = 18.979, Val AUC 0.869 Test AUC 0.835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:48<00:23,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 ,  Loss = 16.605, Val AUC 0.939 Test AUC 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [00:50<00:21,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 ,  Loss = 16.520, Val AUC 0.940 Test AUC 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [00:53<00:19,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 ,  Loss = 16.443, Val AUC 0.941 Test AUC 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [00:55<00:16,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 ,  Loss = 16.392, Val AUC 0.946 Test AUC 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:57<00:14,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 ,  Loss = 16.324, Val AUC 0.948 Test AUC 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [01:00<00:11,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 ,  Loss = 16.276, Val AUC 0.949 Test AUC 0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [01:02<00:09,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 ,  Loss = 16.177, Val AUC 0.949 Test AUC 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [01:05<00:07,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 ,  Loss = 16.154, Val AUC 0.948 Test AUC 0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [01:07<00:04,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 ,  Loss = 16.076, Val AUC 0.943 Test AUC 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [01:09<00:02,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 ,  Loss = 16.045, Val AUC 0.939 Test AUC 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:12<00:00,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 ,  Loss = 16.000, Val AUC 0.936 Test AUC 0.897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# in training\n",
    "best_epoch_val = 0\n",
    "patient = 0\n",
    "\n",
    "for epoch in tqdm(range(args.epochs)):\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    # print('ok1')\n",
    "    for idx, feed_dict in enumerate(dataloader):\n",
    "        feed_dict = to_device(feed_dict, device)  # 节点信息\n",
    "        opt.zero_grad()\n",
    "        loss = model.get_loss(feed_dict)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        epoch_loss.append(loss.item())\n",
    "\n",
    "    # print('ok2')\n",
    "    model.eval()\n",
    "    emb = model(feed_dict[\"graphs\"])[:, -2, :].detach().cpu().numpy()  # 倒数第二层节点的信息当作特征，来训练最后一层的边的连接情况;\n",
    "    val_results, test_results, _, _ = evaluate_classifier(train_edges_pos,  # 训练时无监督训练(上下文)训练节点embedding; 测试时根据<e0,e1>或者再接一个classifier\n",
    "                                                        train_edges_neg,\n",
    "                                                        val_edges_pos, \n",
    "                                                        val_edges_neg, \n",
    "                                                        test_edges_pos,\n",
    "                                                        test_edges_neg, \n",
    "                                                        emb, \n",
    "                                                        emb)\n",
    "    epoch_auc_val = val_results[\"HAD\"][1]\n",
    "    epoch_auc_test = test_results[\"HAD\"][1]\n",
    "\n",
    "    if epoch_auc_val > best_epoch_val:\n",
    "        best_epoch_val = epoch_auc_val\n",
    "        torch.save(model.state_dict(), \"./model_checkpoints/model.pt\")\n",
    "        patient = 0\n",
    "    else:\n",
    "        patient += 1\n",
    "        if patient > args.early_stop:\n",
    "            break\n",
    "\n",
    "    if (epoch+1) % 10 == 0 or epoch==0 or (epoch+1) > 20:\n",
    "        print(\"Epoch {:<3},  Loss = {:.3f}, Val AUC {:.3f} Test AUC {:.3f}\".format(epoch+1, \n",
    "                                                                np.mean(epoch_loss),\n",
    "                                                                epoch_auc_val, \n",
    "                                                                epoch_auc_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载模型,并输出最佳测试结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Test AUC = 0.895\n"
     ]
    }
   ],
   "source": [
    "# Test Best Model\n",
    "model.load_state_dict(torch.load(\"./model_checkpoints/model.pt\"))\n",
    "model.eval()\n",
    "emb = model(feed_dict[\"graphs\"])[:, -2, :].detach().cpu().numpy()\n",
    "val_results, test_results, _, _ = evaluate_classifier(train_edges_pos,\n",
    "                                                    train_edges_neg,\n",
    "                                                    val_edges_pos, \n",
    "                                                    val_edges_neg, \n",
    "                                                    test_edges_pos,\n",
    "                                                    test_edges_neg, \n",
    "                                                    emb, \n",
    "                                                    emb)\n",
    "auc_val = val_results[\"HAD\"][1]\n",
    "auc_test = test_results[\"HAD\"][1]\n",
    "print(\"Best Test AUC = {:.3f}\".format(auc_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
